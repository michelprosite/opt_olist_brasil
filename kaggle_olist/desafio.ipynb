{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michel Souza Santana\n",
    "## Projeto Desafio Aceleras\n",
    "## Trilha 1\n",
    "* Start: 15/05/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1 - Transformação do ER proposto em um BI, realizando o ETL usando uma ferrmenta local (Talend, Apache Hop, Nifi, Airflow, SSIS, Pentaho,…)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ETL (Extração, Transformação e Carga) é um processo utilizado para integrar dados de diferentes fontes, transformá-los em um formato que possa ser analisado e carregá-los em um novo destino, geralmente um data warehouse ou um data lake. Seguem abaixo os passos para realizar uma ETL:\n",
    "\n",
    "* Identificar as fontes de dados: \n",
    "\n",
    "Identifique todas as fontes de dados que serão usadas na ETL, como bancos de dados, planilhas, arquivos CSV, feeds RSS, etc.\n",
    "\n",
    "* Extrair os dados: \n",
    "\n",
    "Extrair os dados das fontes identificadas. Isso pode ser feito usando uma variedade de ferramentas, dependendo da fonte de dados. Por exemplo, bancos de dados podem ser extraídos usando SQL, enquanto planilhas podem ser extraídas usando bibliotecas Python como Pandas.\n",
    "\n",
    "* Validar e limpar os dados: \n",
    "\n",
    "Após extrair os dados, é importante validá-los e limpá-los para garantir que os dados estejam completos, precisos e consistentes. Isso envolve remover dados duplicados ou inválidos e preencher quaisquer valores ausentes ou incorretos.\n",
    "\n",
    "* Transformar os dados: \n",
    "\n",
    "Transforme os dados em um formato que possa ser carregado no destino. Isso pode envolver a conversão de tipos de dados, a criação de novas colunas ou a agregação de dados para criar novas visões.\n",
    "\n",
    "* Analisar dados: \n",
    "\n",
    "Uma vez que os dados são limpos e transformados, é importante realizar uma análise exploratória para entender melhor os dados. Isso pode incluir a criação de gráficos, a identificação de padrões e a aplicação de outras técnicas de análise.\n",
    "\n",
    "* Selecionar recursos: \n",
    "\n",
    "Em muitos casos, nem todos os dados são relevantes para o problema que se está tentando resolver. É importante selecionar os recursos mais importantes para o modelo que se está tentando construir.\n",
    "\n",
    "* Dividir os dados: \n",
    "\n",
    "Antes de construir o modelo final, é importante dividir os dados em um conjunto de treinamento e um conjunto de teste. O conjunto de treinamento é usado para construir o modelo, enquanto o conjunto de teste é usado para avaliar a precisão do modelo.\n",
    "\n",
    "* Verificar qualidade: \n",
    "\n",
    "Após dividir os dados, é importante verificar a qualidade dos dados para garantir que eles estejam prontos para serem usados no modelo final. Isso pode incluir a identificação de valores ausentes ou inconsistentes, a verificação de distribuições de variáveis e outras técnicas de validação de dados.\n",
    "\n",
    "* Preparar dados para o modelo: \n",
    "\n",
    "Finalmente, é importante preparar os dados para serem usados no modelo final. Isso pode incluir a codificação de variáveis categóricas, a normalização de dados e outras técnicas de preparação de dados específicas para o tipo de modelo que você está construindo.\n",
    "\n",
    "* Carregar os dados: \n",
    "\n",
    "Finalmente, carregue os dados transformados no destino, como um data warehouse ou um data lake. Isso pode ser feito usando ferramentas ETL dedicadas, como o Apache Airflow, Pentaho ou o Apache Nifi.\n",
    "\n",
    "* Monitorar e manter: \n",
    "\n",
    "Após carregar os dados, monitore e mantenha o processo ETL para garantir que os dados sejam atualizados regularmente e que quaisquer problemas sejam resolvidos imediatamente. Isso pode envolver o uso de ferramentas de monitoramento e alerta para identificar problemas rapidamente e tomar medidas corretivas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ponto de Vista do projeto\n",
    "\n",
    "### 1 - Definir os requisitos\n",
    "* 5w2h\n",
    "* Prototipação\n",
    "\n",
    "### 2 - Verifica a viabilidade nas fontes de dados\n",
    "* Responsável Técnico pelo dados\n",
    "    * TI\n",
    "\n",
    "### 3 - Desenha o DW\n",
    "\n",
    "### 4 - Desenha os processos ETL\n",
    "\n",
    "### 5 - Desenha os Cubos\n",
    "\n",
    "### 6 - Desenha os Dashboards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Tabela 5W2H atualizada para o projeto de construção do DW para a empresa Olist em português:\n",
    "\n",
    "| Item | Descrição\n",
    "| --- | --- \n",
    "| O que | Construção de um Data Warehouse (DW) para a empresa Olist\n",
    "| Por que | Permitir que a empresa Olist obtenha insights de negócios mais precisos e relevantes a partir de seus dados, permitindo que ela tome decisões mais informadas e estratégicas.\n",
    "| Quem | Equipe de TI da empresa Olist\n",
    "| Onde | No ambiente de nuvem da empresa Olist\n",
    "| Quando | Início do projeto: 1º de julho de 2023<br>Data de conclusão: 31 de dezembro de 2023\n",
    "| Como | <ul><li>Levantamento dos requisitos de negócios: Julho de 2023</li><li>Definição da arquitetura do DW: Agosto de 2023</li><li>Escolha da tecnologia de DW: Setembro de 2023</li><li>Instalação e configuração da tecnologia do DW: Outubro de 2023</li><li>Design e desenvolvimento de ETL para alimentar o DW: Novembro de 2023</li><li>Testes de desempenho do DW: Dezembro de 2023</li></ul>\n",
    "| Quanto | O orçamento previsto para a construção do DW é de R$ 500.000,00, que inclui os custos de licenças de software, hardware, serviços de consultoria e treinamento da equipe\n",
    "| Riscos | <ul><li>Atrasos na implementação devido a mudanças nos requisitos de negócios</li><li>Problemas de desempenho devido a volumes de dados maiores do que o previsto</li><li>Problemas de compatibilidade de dados com diferentes fontes</li></ul>\n",
    "| Oportunidades |<ul><li>Novas descobertas de negócios a partir dos dados do DW que podem levar a oportunidades de crescimento e expansão da empresa</li><li>Melhoria da eficiência operacional e redução de custos a partir de insights de negócios mais precisos e relevantes</li></ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para elaborar uma abordagem dimensional, como o modelo estrela, para os dados do conjunto de dados \"Brazilian E-Commerce\" disponível no Kaggle (https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce), podemos considerar os seguintes elementos principais:\n",
    "\n",
    "* Tabela de Fatos (Fact Table):\n",
    "A tabela de fatos é o centro do modelo dimensional e contém as métricas quantitativas ou eventos que desejamos analisar. No caso do conjunto de dados \"Brazilian E-Commerce\", uma possível tabela de fatos pode ser a \"Orders\" (Pedidos), que registra informações sobre os pedidos realizados. Alguns atributos possíveis para essa tabela podem ser: ID do pedido, data do pedido, valor total do pedido, ID do cliente, ID do produto, ID do vendedor, etc.\n",
    "\n",
    "* Tabelas de Dimensões (Dimension Tables):\n",
    "As tabelas de dimensões fornecem o contexto para as métricas na tabela de fatos e contêm atributos descritivos que ajudam a filtrar e agrupar os dados. No contexto do conjunto de dados \"Brazilian E-Commerce\", podemos ter as seguintes tabelas de dimensões:\n",
    "\n",
    "* Tabela de Dimensão \"Clientes\": contendo informações sobre os clientes, como ID do cliente, nome, endereço, cidade, estado, etc.\n",
    "* Tabela de Dimensão \"Produtos\": contendo informações sobre os produtos, como ID do produto, nome do produto, categoria, preço, etc.\n",
    "* Tabela de Dimensão \"Vendedores\": contendo informações sobre os vendedores, como ID do vendedor, nome do vendedor, região, classificação, etc.\n",
    "* Tabela de Dimensão \"Datas\": contendo informações sobre as datas, como data, dia da semana, mês, trimestre, ano, feriados, etc.\n",
    "Cada tabela de dimensão teria uma chave primária única que se relaciona com a tabela de fatos através de chaves estrangeiras.\n",
    "\n",
    "* Tabelas de Dimensões Adicionais (Optional Dimension Tables):\n",
    "Dependendo dos requisitos de análise específicos do conjunto de dados \"Brazilian E-Commerce\", outras tabelas de dimensões adicionais podem ser incluídas para fornecer mais contexto aos dados. Por exemplo, uma tabela de dimensão \"Categorias\" poderia conter informações sobre as categorias dos produtos vendidos.\n",
    "\n",
    "* Tabelas de Bridge (Bridge Tables):\n",
    "Em alguns casos, pode ser necessário representar relacionamentos muitos-para-muitos entre dimensões. Nesses casos, tabelas de bridge podem ser usadas para criar associações entre várias dimensões. Por exemplo, uma tabela de bridge \"Pedidos_Produtos\" pode ser usada para representar quais produtos foram incluídos em cada pedido."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução\n",
    "\n",
    "Site oficial da Olist: https://www.olist.com\n",
    "\n",
    "A Olist é uma plataforma de comércio eletrônico que conecta vendedores e compradores em um mercado online. A empresa opera em um modelo de negócios de marketplace, onde os vendedores podem listar seus produtos na plataforma da Olist e alcançar um grande público de compradores.\n",
    "\n",
    "Para começar a vender na Olist, um vendedor precisa se inscrever na plataforma e passar por um processo de verificação para garantir que seus produtos atendam aos padrões de qualidade da empresa. Uma vez que o vendedor tenha sido aprovado, ele pode listar seus produtos no marketplace da Olist.\n",
    "\n",
    "A Olist oferece aos vendedores uma variedade de serviços, incluindo logística, gerenciamento de pagamentos e atendimento ao cliente. A empresa também oferece ferramentas de marketing e publicidade para ajudar os vendedores a promover seus produtos na plataforma.\n",
    "\n",
    "Para os compradores, a Olist oferece uma ampla variedade de produtos de diferentes categorias, desde eletrônicos e eletrodomésticos até moda e acessórios. Os compradores podem navegar pelos produtos na plataforma, comparar preços e avaliações, e fazer compras com segurança usando as opções de pagamento disponíveis na plataforma.\n",
    "\n",
    "Olist funciona como uma plataforma de comércio eletrônico que conecta vendedores e compradores em um mercado online, fornecendo serviços de logística, gerenciamento de pagamentos e atendimento ao cliente para os vendedores e uma ampla variedade de produtos para os compradores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights\n",
    "\n",
    "O conjunto de dados disponibilizados no endereço eletrõnico https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce, contém informações sobre as vendas do comércio eletrônico no Brasil, incluindo dados sobre os pedidos, produtos, clientes e avaliações dos usuários. Com base nesses dados, podemos tentar responder perguntas como:\n",
    "\n",
    "1. Qual é o perfil dos clientes do comércio eletrônico no Brasil?\n",
    "2. Quais são os produtos mais vendidos no comércio eletrônico no Brasil?\n",
    "3. Qual é o comportamento de compra dos usuários?\n",
    "4. Como as avaliações dos usuários afetam as vendas e a reputação das empresas?\n",
    "5. Explorar os dados por meio de visualizações e gráficos para ajudar a comunicar as descobertas de forma clara e concisa.\n",
    "\n",
    "A análise do negócio do ponto de vista do usuário pode ajudar as empresas a entender melhor seus clientes e a tomar decisões mais informadas sobre como melhorar a experiência do usuário, aumentar as vendas e melhorar sua reputação no mercado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PipeLine\n",
    "\n",
    "### 1 - Copia os dados das fontes de dados\n",
    "* Fazer o download dos dados do Kaggle através do link fornecido.\n",
    "\n",
    "### 2 - Coloca esses dados na staging area\n",
    "* Tabelão burro\n",
    "* Sem relacionamento\n",
    "* Sem limpeza nenhuma\n",
    "* Só insere na tabela\n",
    "\n",
    "### 3 - Limpar, juntar, unificar, formatar, etc\n",
    "* Eliminar dados duplicados, lidar com valores ausentes e inconsistentes, transformar e formatar os dados de maneira que eles possam ser utilizados adequadamente.\n",
    "\n",
    "### 4 - Análise exploratória dos dados \n",
    "* Explorar os dados para entender sua distribuição, encontrar padrões e tendências relevantes e identificar possíveis relações entre as variáveis.\n",
    "\n",
    "### 5 - Transformação e engenharia de features \n",
    "* Criar novas variáveis a partir das já existentes, selecionar e codificar as variáveis relevantes para os modelos que serão desenvolvidos.\n",
    "\n",
    "### 6 - Treinamento e avaliação de modelos \n",
    "* Construir modelos de aprendizado de máquina para prever comportamentos, identificar fatores que influenciam o sucesso das vendas, etc. Avaliar a eficácia dos modelos através de métricas relevantes e realizar ajustes necessários.\n",
    "\n",
    "### 7 - Copia os dados e joga no Star shema\n",
    "* DW\n",
    "* Modelo dimensional\n",
    "    * Carrega as dimensões\n",
    "    * Carrega a \n",
    "        * Métricas base / bruta\n",
    "\n",
    "### 8 - Conecta o cubo no DW e cria as métricas calculadas e KPIs\n",
    "\n",
    "### 9 - Cria as análises, gráficos, visualizações e apresenta eles num relatório ou dashboard\n",
    "* Apresentar os resultados através de gráficos e relatórios que possam ser facilmente compreendidos por usuários não técnicos, a fim de orientar a tomada de decisões.\n",
    "\n",
    "### 10 - Manutenção e atualização dos modelos \n",
    "* Monitorar regularmente o desempenho dos modelos e atualizá-los quando necessário, para garantir que eles permaneçam relevantes e precisos ao longo do tempo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instalando bibliotecas a serem utilizadas no projeto\n",
    "#!pip install pyarrow\n",
    "#!pip install kaggle --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Processo de Transient Data (Dados Transitórios):\n",
    "Os dados transitórios são dados temporários ou de curta duração usados para processamento e transformação intermediários. Esses dados podem ser gerados durante o processamento em tempo real, pipelines de processamento de dados em lote ou durante a integração de diferentes fontes de dados. Os dados transitórios são normalmente armazenados em sistemas de processamento de fluxo de dados ou bancos de dados temporários, e seu objetivo é suportar operações intermediárias antes que os dados sejam refinados e confiáveis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os diretórios estruturais do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diretório engineer já existe.\n",
      "O diretório raw já existe.\n",
      "O diretório refined já existe.\n",
      "O diretório transient já existe.\n",
      "O diretório trusted já existe.\n"
     ]
    }
   ],
   "source": [
    "path_folder = !pwd # Variavel responsável pos capiturar o path local do projeto\n",
    "f = pd.read_csv(str(path_folder[0]) + '/folders.csv') # O 'path_folder' é um arquivo csv criado manualmente para determinar as pastas que devem ser criadas para o projeto\n",
    "lista_folders = f['Folders'].to_list()\n",
    "\n",
    "# Laço for percorre a lista das partas cridas e inicia as pastas do projeto\n",
    "for i in lista_folders:\n",
    "    diretorio = str(path_folder[0]) \n",
    "\n",
    "    # Verifica se as pastas já foram criadas, se não, as cria.\n",
    "    if not os.path.exists(diretorio + '/' + i):\n",
    "        os.makedirs(diretorio + '/' + i)\n",
    "        print(f\"Diretório {i} criado com sucesso!\")\n",
    "    else:\n",
    "        print(f\"O diretório {i} já existe.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copia os dados do site do kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixando os arquivos csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixar & descompactar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/michel/.kaggle/kaggle.json'\n",
      "Downloading brazilian-ecommerce.zip to /home/michel/Documentos/opt/kaggle_olist/transient\n",
      "100%|██████████████████████████████████████| 42.6M/42.6M [00:10<00:00, 3.45MB/s]\n",
      "100%|██████████████████████████████████████| 42.6M/42.6M [00:10<00:00, 4.07MB/s]\n",
      "Archive:  brazilian-ecommerce.zip\n",
      "  inflating: olist_customers_dataset.csv  \n",
      "  inflating: olist_geolocation_dataset.csv  \n",
      "  inflating: olist_order_items_dataset.csv  \n",
      "  inflating: olist_order_payments_dataset.csv  \n",
      "  inflating: olist_order_reviews_dataset.csv  \n",
      "  inflating: olist_orders_dataset.csv  \n",
      "  inflating: olist_products_dataset.csv  \n",
      "  inflating: olist_sellers_dataset.csv  \n",
      "  inflating: product_category_name_translation.csv  \n",
      "Arquivos carregados e descompactado. Total de 0 arquivos.\n"
     ]
    }
   ],
   "source": [
    "pasta = str(path_folder[0]) + '/transient'\n",
    "cont = 0\n",
    "\n",
    "# Verificando a existência dos arquivos, se não existe, baixa e descompacta.\n",
    "verifica_arquivos = [os.path.splitext(arquivo)[0] for arquivo in os.listdir(pasta) if os.path.isfile(os.path.join(pasta, arquivo))]\n",
    "for i in verifica_arquivos: cont += 1\n",
    "\n",
    "# Baixando, descompactando arquivos\n",
    "if cont <= 0:\n",
    "\n",
    "    # Setando o caminho para download dos arquivos\n",
    "    r = !pwd\n",
    "    PATH_FOLDER = r[0] + '/transient'\n",
    "    os.environ['PATH_FOLDER'] = PATH_FOLDER\n",
    "\n",
    "    !cd $PATH_FOLDER && kaggle datasets download -d olistbr/brazilian-ecommerce\n",
    "    !cd $PATH_FOLDER && unzip brazilian-ecommerce.zip\n",
    "    !cd $PATH_FOLDER && rm -r brazilian-ecommerce.zip\n",
    "    print(f'Arquivos carregados e descompactado. Total de {cont} arquivos.')\n",
    "\n",
    "else:\n",
    "    print(f'Arquivos já existem, são no total de {cont} arquivos.')\n",
    "    print('Caso queira atualiza-los se faz necessário executar a carga da Raw para zerar os arquivos.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o arquivo 'controller.csv' na pasta enginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta = str(path_folder[0]) + '/transient'  # Substitua pelo caminho da pasta desejada\n",
    "\n",
    "# Capturar nomes dos arquivos e remover a extensão\n",
    "arquivos = [os.path.splitext(arquivo)[0] for arquivo in os.listdir(pasta) if os.path.isfile(os.path.join(pasta, arquivo))]\n",
    "# Eliminar as palavras indesejadas\n",
    "palavras_indesejadas = ['_dataset.csv', 'olist_', 'order_', '_dataset', 'product_', '_name', '_translation']\n",
    "nomes_limpos = [arquivo for arquivo in arquivos]\n",
    "for palavra in palavras_indesejadas:\n",
    "   nomes_limpos = [nome.replace(palavra, '') for nome in nomes_limpos]\n",
    "\n",
    "# Caminho do arquivo CSV a ser salvo\n",
    "caminho_arquivo = str(path_folder[0]) + '/' + 'controller.csv'\n",
    "\n",
    "# Definir os dados a serem escritos no arquivo CSV\n",
    "dados = [\n",
    "    [\"path_transient\", \"path_raw\", \"path_trusted\", \"table_transient\", \"table_raw\", \"table_trusted\", \"table_name\", \"table_name_temp\"]\n",
    "] + [\n",
    "    [str(path_folder[0]) + '/' + \"transient\", str(path_folder[0]) + '/' + \"raw\", str(path_folder[0]) + '/' + \"trusted\", nome, nome, nome, nome_limp, 'temp_' + nome_limp]\n",
    "    for nome, nome_limp in zip(arquivos, nomes_limpos)\n",
    "]\n",
    "\n",
    "# Salvar o arquivo CSV\n",
    "pd.DataFrame(dados).to_csv(caminho_arquivo, index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando tabela com os path necessários para manipulação dos arquivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando a formação dos path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_transient</th>\n",
       "      <th>path_raw</th>\n",
       "      <th>path_trusted</th>\n",
       "      <th>table_transient</th>\n",
       "      <th>table_raw</th>\n",
       "      <th>table_trusted</th>\n",
       "      <th>table_name</th>\n",
       "      <th>table_name_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>items</td>\n",
       "      <td>temp_items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>payments</td>\n",
       "      <td>temp_payments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>category</td>\n",
       "      <td>temp_category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>products</td>\n",
       "      <td>temp_products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>customers</td>\n",
       "      <td>temp_customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>sellers</td>\n",
       "      <td>temp_sellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>reviews</td>\n",
       "      <td>temp_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>orders</td>\n",
       "      <td>temp_orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>geolocation</td>\n",
       "      <td>temp_geolocation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      path_transient   \n",
       "0  /home/michel/Documentos/opt/kaggle_olist/trans...  \\\n",
       "1  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "2  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "3  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "4  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "5  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "6  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "7  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "8  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "\n",
       "                                       path_raw   \n",
       "0  /home/michel/Documentos/opt/kaggle_olist/raw  \\\n",
       "1  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "2  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "3  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "4  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "5  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "6  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "7  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "8  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "\n",
       "                                       path_trusted   \n",
       "0  /home/michel/Documentos/opt/kaggle_olist/trusted  \\\n",
       "1  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "2  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "3  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "4  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "5  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "6  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "7  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "8  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "\n",
       "                     table_transient                          table_raw   \n",
       "0          olist_order_items_dataset          olist_order_items_dataset  \\\n",
       "1       olist_order_payments_dataset       olist_order_payments_dataset   \n",
       "2  product_category_name_translation  product_category_name_translation   \n",
       "3             olist_products_dataset             olist_products_dataset   \n",
       "4            olist_customers_dataset            olist_customers_dataset   \n",
       "5              olist_sellers_dataset              olist_sellers_dataset   \n",
       "6        olist_order_reviews_dataset        olist_order_reviews_dataset   \n",
       "7               olist_orders_dataset               olist_orders_dataset   \n",
       "8          olist_geolocation_dataset          olist_geolocation_dataset   \n",
       "\n",
       "                       table_trusted   table_name   table_name_temp  \n",
       "0          olist_order_items_dataset        items        temp_items  \n",
       "1       olist_order_payments_dataset     payments     temp_payments  \n",
       "2  product_category_name_translation     category     temp_category  \n",
       "3             olist_products_dataset     products     temp_products  \n",
       "4            olist_customers_dataset    customers    temp_customers  \n",
       "5              olist_sellers_dataset      sellers      temp_sellers  \n",
       "6        olist_order_reviews_dataset      reviews      temp_reviews  \n",
       "7               olist_orders_dataset       orders       temp_orders  \n",
       "8          olist_geolocation_dataset  geolocation  temp_geolocation  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = pd.read_csv(str(path_folder[0]) + '/' + 'controller.csv')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michel/Documentos/opt/kaggle_olist/transient/olist_order_items_dataset.csv\n",
      "/home/michel/Documentos/opt/kaggle_olist/raw/olist_order_items_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "print(path['path_transient'][0] + '/' + path['table_transient'][0] + '.csv')\n",
    "print(path['path_raw'][0] + '/' + path['table_raw'][0] + '.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Processo de carga da Raw Data (Dados Brutos):\n",
    "Nesta fase inicial, os dados brutos são coletados de várias fontes, como bancos de dados, arquivos CSV, APIs, feeds de streaming, entre outros. Esses dados podem estar em formatos diversos e podem conter ruído, inconsistências e falta de estrutura. O objetivo do processo de Dados Brutos é extrair e armazenar esses dados em seu estado bruto original."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertendo or arquivos CSV em parquet e carregando a Raw pela primeira vez .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregando_raw():\n",
    "    for i in range(len(path['path_transient'])):\n",
    "\n",
    "        # Criando o caminho dos arquivos e diretórios\n",
    "        diretorio_transient = path['path_transient'][i]\n",
    "        nome_arquivo_transient = path['table_transient'][i] + '.csv'\n",
    "        diretorio_raw = path['path_raw'][i]\n",
    "        nome_arquivo_raw = path['table_raw'][i] + '.parquet'\n",
    "\n",
    "        caminho_arquivo_transient = os.path.join(diretorio_transient, nome_arquivo_transient)\n",
    "        caminho_arquivo_raw = os.path.join(diretorio_raw, nome_arquivo_raw)\n",
    "\n",
    "        # Verificando a existência dos arquivos em Transient e carrega na Raw pela primeira vêz\n",
    "        if os.path.exists(caminho_arquivo_transient) and not os.path.exists(caminho_arquivo_raw):\n",
    "\n",
    "            # carregando arquivos csv\n",
    "            df = pd.read_csv(caminho_arquivo_transient)\n",
    "\n",
    "            # dropando os duplicados preservando o último atualizado\n",
    "            df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
    "\n",
    "            #Verifica se o DF está vazio\n",
    "            if not df.empty:\n",
    "\n",
    "                # Salva na Raw convertendo para o formato parquet\n",
    "                df.to_parquet(caminho_arquivo_raw)\n",
    "                print(f\"Arquivo Parquet {nome_arquivo_raw} criado: {caminho_arquivo_raw}\")\n",
    "            else:\n",
    "                print(f\"O DataFrame está vazio: {nome_arquivo_raw}\")\n",
    "            carregando_raw()\n",
    "\n",
    "        # Caso a Raw já tenha sido carregada pela primeira vêz esse código é executado para atualizar os arquivos verificando se algum arquivo foi deletado.\n",
    "        elif os.path.exists(caminho_arquivo_transient) and os.path.exists(caminho_arquivo_raw):\n",
    "\n",
    "            # Confere a existência do arquivo, caso deletado, cria novamente\n",
    "            if not os.path.exists(caminho_arquivo_raw):\n",
    "                df = pd.read_csv(caminho_arquivo_transient)\n",
    "                df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
    "\n",
    "                if not df.empty:\n",
    "                    df.to_parquet(caminho_arquivo_raw)\n",
    "                    print(f\"Arquivo Parquet {nome_arquivo_raw} criado: {caminho_arquivo_raw}\")\n",
    "\n",
    "            # Carregando os DF's para concatenar os novos dados\n",
    "            df1 = pd.read_csv(caminho_arquivo_transient)\n",
    "            df2 = pd.read_parquet(caminho_arquivo_raw)\n",
    "\n",
    "            # Concatenando os dados\n",
    "            df_concatenado = pd.concat([df2, df1], axis=0)\n",
    "\n",
    "            # Eliminando as duplicadas pelo id mantendo sempre a última ocorrcia\n",
    "            df_concatenado = df_concatenado.drop_duplicates(subset=df_concatenado.columns[0], keep='last')\n",
    "            df_concatenado.to_parquet(caminho_arquivo_raw)\n",
    "        else:\n",
    "            print(f'Arquivo Raw {nome_arquivo_raw} já existe! Para atualizá-lo, carregue a transient correspondente.')\n",
    "\n",
    "    # Após a criação e atualização dos dados os arquivos Transient são eliminados deichando o diretório livre para receber novos dados\n",
    "    if os.path.exists(caminho_arquivo_transient):\n",
    "        r = !pwd\n",
    "        PATH_FOLDER = r[0] + '/transient'\n",
    "        os.environ['PATH_FOLDER'] = PATH_FOLDER\n",
    "        !cd $PATH_FOLDER && rm -r *\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "carregando_raw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Processo de Trusted Data (Dados Confiáveis):\n",
    "Nesta fase, os dados são considerados confiáveis, prontos para uso e adequados para análise e tomada de decisão. Isso envolve garantir a qualidade dos dados, validar a precisão, consistência e integridade dos dados, aplicar regras de negócios e conformidade, bem como implementar mecanismos de controle de qualidade. O objetivo é fornecer dados confiáveis que possam ser usados com confiança para análise, geração de relatórios e outras tarefas de negócios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo atualizado: olist_order_items_dataset.parquet\n",
      "Arquivo atualizado: olist_order_payments_dataset.parquet\n",
      "Arquivo atualizado: product_category_name_translation.parquet\n",
      "Arquivo atualizado: olist_products_dataset.parquet\n",
      "Arquivo atualizado: olist_customers_dataset.parquet\n",
      "Arquivo atualizado: olist_sellers_dataset.parquet\n",
      "Arquivo atualizado: olist_order_reviews_dataset.parquet\n",
      "Arquivo atualizado: olist_orders_dataset.parquet\n",
      "Arquivo atualizado: olist_geolocation_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "def carregando_trusted():\n",
    "    for i in range(len(path['path_trusted'])):\n",
    "        diretorio_raw = path['path_raw'][i]\n",
    "        nome_arquivo_raw = path['table_raw'][i] + '.parquet'\n",
    "        diretorio_trusted = path['path_trusted'][i]\n",
    "        nome_arquivo_trusted = path['table_trusted'][i] + '.parquet'\n",
    "\n",
    "        caminho_arquivo_raw = os.path.join(diretorio_raw, nome_arquivo_raw)\n",
    "        caminho_arquivo_trusted = os.path.join(diretorio_trusted, nome_arquivo_trusted)\n",
    "\n",
    "        if os.path.exists(caminho_arquivo_raw) and not os.path.exists(caminho_arquivo_trusted):\n",
    "            df = pd.read_parquet(caminho_arquivo_raw)\n",
    "            df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
    "\n",
    "            if not df.empty:\n",
    "                df.to_parquet(caminho_arquivo_trusted)\n",
    "                print(f\"Arquivo Parquet {nome_arquivo_trusted} criado: {caminho_arquivo_trusted}\")\n",
    "            else:\n",
    "                print(f\"O DataFrame está vazio: {nome_arquivo_trusted}\")\n",
    "\n",
    "            carregando_trusted()\n",
    "\n",
    "        elif os.path.exists(caminho_arquivo_raw) and os.path.exists(caminho_arquivo_trusted):\n",
    "            for i in range(len(path['path_trusted'])):\n",
    "                df_csv = pd.read_parquet(caminho_arquivo_raw)\n",
    "                df_parquet = pd.read_parquet(caminho_arquivo_trusted)\n",
    "\n",
    "                if not df_csv.iloc[:, 0].equals(df_parquet.iloc[:, 0]):\n",
    "                    dados_diferentes = df_csv[~df_csv.iloc[:, 0].isin(df_parquet.iloc[:, 0])]\n",
    "\n",
    "                    if not dados_diferentes.empty:\n",
    "                        df_parquet = pd.concat([dados_diferentes, df_parquet], axis=0, ignore_index=True, sort=False)\n",
    "                        df_parquet.drop_duplicates(subset=df_parquet.columns[0], keep='last', inplace=True)\n",
    "                        df_parquet.to_parquet(caminho_arquivo_trusted)\n",
    "                        print(f\"Foram encontradas diferenças na primeira coluna entre as tabelas {nome_arquivo_trusted}. O arquivo foi atualizado.\")\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "            print(f'Arquivo atualizado: {nome_arquivo_trusted}')\n",
    "        else:\n",
    "            print(f'Arquivo trusted {nome_arquivo_trusted} já existe! Para atualizá-lo, carregue a raw correspondente.')\n",
    "\n",
    "carregando_trusted()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo os arquivos e criando o banco de dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testando os caminhos e nomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nome da tabela: items\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_order_items_dataset.parquet\n",
      "\n",
      "Nome da tabela: payments\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_order_payments_dataset.parquet\n",
      "\n",
      "Nome da tabela: category\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedproduct_category_name_translation.parquet\n",
      "\n",
      "Nome da tabela: products\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_products_dataset.parquet\n",
      "\n",
      "Nome da tabela: customers\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_customers_dataset.parquet\n",
      "\n",
      "Nome da tabela: sellers\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_sellers_dataset.parquet\n",
      "\n",
      "Nome da tabela: reviews\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_order_reviews_dataset.parquet\n",
      "\n",
      "Nome da tabela: orders\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_orders_dataset.parquet\n",
      "\n",
      "Nome da tabela: geolocation\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_geolocation_dataset.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(path['table_name'])):\n",
    "    print(f\"Nome da tabela: {path['table_name'][i]}\")\n",
    "    print(f\"caminho dos dados: {path['path_trusted'][i] + path['table_trusted'][i] + '.parquet'}\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o Banco e as tabelas com base nos arquivos parquet existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Configurações de conexão com o banco de dados\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = '*Mss140920@'\n",
    "\n",
    "for i in range(len(path['table_name'])):\n",
    "    # Conectando ao servidor MySQL\n",
    "    connection = mysql.connector.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password\n",
    "    )\n",
    "\n",
    "    # Criando um cursor para executar comandos SQL\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Verificar se o banco 'olistdb' existe, se não, criá-lo\n",
    "    cursor.execute(\"SHOW DATABASES LIKE 'olistdb'\")\n",
    "    result = cursor.fetchone()\n",
    "    if not result:\n",
    "        cursor.execute(\"CREATE DATABASE olistdb\")\n",
    "\n",
    "    # Fechar o cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # Conectar ao banco de dados 'olistdb'\n",
    "    connection = mysql.connector.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        database='olistdb'\n",
    "    )\n",
    "\n",
    "    # Capturar os nomes das futuras tabelas em df['table_name']\n",
    "    df = pd.read_parquet(path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet')\n",
    "    table_name = path['table_name'][i]\n",
    "\n",
    "    # Criar um novo cursor\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Verificar se as tabelas existem, se não, criar as tabelas no banco de dados\n",
    "    cursor.execute(f\"SHOW TABLES LIKE '{table_name}'\")\n",
    "    result = cursor.fetchone()\n",
    "    if not result:\n",
    "        # Obter as colunas e seus tipos a partir do arquivo Parquet\n",
    "        table_path = path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet'\n",
    "        table_data = pd.read_parquet(table_path)\n",
    "        columns = table_data.columns\n",
    "        column_types = table_data.dtypes\n",
    "        table_index = table_data.index\n",
    "\n",
    "        # Criar a tabela com as colunas e tipos correspondentes\n",
    "        create_table_query = f\"CREATE TABLE {table_name} (\"\n",
    "        for col, col_type in zip(columns, column_types):\n",
    "            if col_type == 'int64':\n",
    "                col_type = 'INT'\n",
    "            elif col_type == 'float64':\n",
    "                col_type = 'FLOAT'\n",
    "            elif col_type == 'bool':\n",
    "                col_type = 'BOOLEAN'\n",
    "            else:\n",
    "                col_type = 'VARCHAR(255)'  # Tipo padrão se não for numérico\n",
    "\n",
    "            if col == columns[0]:\n",
    "                create_table_query += f\"{col} {col_type} PRIMARY KEY, \"\n",
    "            else:\n",
    "                create_table_query += f\"{col} {col_type}, \"\n",
    "\n",
    "        create_table_query = create_table_query.rstrip(', ')  # Remover a última vírgula\n",
    "        create_table_query += \")\"\n",
    "        cursor.execute(create_table_query)\n",
    "\n",
    "    # Fechar o cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # Fechar a conexão com o banco de dados\n",
    "    connection.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alimenta as tabelas recém criadas com o conteúdo dos arquivos parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tabela items já possui dados e não será carregada.\n",
      "A tabela payments já possui dados e não será carregada.\n",
      "A tabela category já possui dados e não será carregada.\n",
      "A tabela products já possui dados e não será carregada.\n",
      "A tabela customers já possui dados e não será carregada.\n",
      "A tabela sellers já possui dados e não será carregada.\n",
      "A tabela reviews já possui dados e não será carregada.\n",
      "A tabela orders já possui dados e não será carregada.\n",
      "A tabela geolocation já possui dados e não será carregada.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configurações de conexão com o banco de dados MySQL\n",
    "config = {\n",
    "    'user': 'root',\n",
    "    'password': '*Mss140920@',\n",
    "    'host': 'localhost',\n",
    "    'database': 'olistdb',\n",
    "    'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "# Função para verificar se uma tabela está vazia\n",
    "def tabela_vazia(cursor, tabela):\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {tabela}\")\n",
    "    result = cursor.fetchone()\n",
    "    return result[0] == 0\n",
    "\n",
    "# Função para carregar dados de um arquivo Parquet para uma tabela\n",
    "def carregar_dados_parquet(cursor, tabela, caminho):\n",
    "    dataframe = pd.read_parquet(caminho)\n",
    "    dataframe = dataframe.fillna(0)  # Substituir valores NaN por 0\n",
    "    valores = [tuple(row) for row in dataframe.values]\n",
    "    placeholders = ','.join(['%s'] * len(dataframe.columns))\n",
    "    cursor.executemany(f\"INSERT INTO {tabela} VALUES ({placeholders})\", valores)\n",
    "\n",
    "# Conectando ao banco de dados\n",
    "try:\n",
    "    cnx = mysql.connector.connect(**config)\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    # Verificando e carregando tabelas\n",
    "    for i in range(len(path['table_name'])):\n",
    "        tabela = path['table_name'][i]\n",
    "        if tabela_vazia(cursor, tabela):\n",
    "            caminho = path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet'\n",
    "            carregar_dados_parquet(cursor, tabela, caminho)\n",
    "            cnx.commit()\n",
    "            print(f\"Dados carregados na tabela {tabela} com sucesso!\")\n",
    "        else:\n",
    "            print(f\"A tabela {tabela} já possui dados e não será carregada.\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
    "\n",
    "finally:\n",
    "    if 'cursor' in locals():\n",
    "        cursor.close()\n",
    "    if 'cnx' in locals() and cnx.is_connected():\n",
    "        cnx.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando as tabelas temporárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Configurações de conexão com o banco de dados\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = '*Mss140920@'\n",
    "\n",
    "for i in range(len(path['table_name_temp'])):\n",
    "    # Conectando ao servidor MySQL\n",
    "    connection = mysql.connector.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password\n",
    "    )\n",
    "\n",
    "    # Criando um cursor para executar comandos SQL\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Verificar se o banco 'olistdb' existe, se não, criá-lo\n",
    "    cursor.execute(\"SHOW DATABASES LIKE 'olistdb'\")\n",
    "    result = cursor.fetchone()\n",
    "    if not result:\n",
    "        cursor.execute(\"CREATE DATABASE olistdb\")\n",
    "\n",
    "    # Fechar o cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # Conectar ao banco de dados 'olistdb'\n",
    "    connection = mysql.connector.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        database='olistdb'\n",
    "    )\n",
    "\n",
    "    # Capturar os nomes das futuras tabelas em df['table_name']\n",
    "    df = pd.read_parquet(path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet')\n",
    "    table_name = path['table_name_temp'][i]\n",
    "\n",
    "    # Criar um novo cursor\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Verificar se as tabelas existem, se não, criar as tabelas no banco de dados\n",
    "    cursor.execute(f\"SHOW TABLES LIKE '{table_name}'\")\n",
    "    result = cursor.fetchone()\n",
    "    if not result:\n",
    "        # Obter as colunas e seus tipos a partir do arquivo Parquet\n",
    "        table_path = path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet'\n",
    "        table_data = pd.read_parquet(table_path)\n",
    "        columns = table_data.columns\n",
    "        column_types = table_data.dtypes\n",
    "        table_index = table_data.index\n",
    "\n",
    "        # Criar a tabela com as colunas e tipos correspondentes\n",
    "        create_table_query = f\"CREATE TABLE {table_name} (\"\n",
    "        for col, col_type in zip(columns, column_types):\n",
    "            if col_type == 'int64':\n",
    "                col_type = 'INT'\n",
    "            elif col_type == 'float64':\n",
    "                col_type = 'FLOAT'\n",
    "            elif col_type == 'bool':\n",
    "                col_type = 'BOOLEAN'\n",
    "            else:\n",
    "                col_type = 'VARCHAR(255)'  # Tipo padrão se não for numérico\n",
    "\n",
    "            if col == columns[0]:\n",
    "                create_table_query += f\"{col} {col_type} PRIMARY KEY, \"\n",
    "            else:\n",
    "                create_table_query += f\"{col} {col_type}, \"\n",
    "\n",
    "        create_table_query = create_table_query.rstrip(', ')  # Remover a última vírgula\n",
    "        create_table_query += \")\"\n",
    "        cursor.execute(create_table_query)\n",
    "\n",
    "    # Fechar o cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # Fechar a conexão com o banco de dados\n",
    "    connection.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando as tabelas temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao conectar ao banco de dados: 1146 (42S02): Table 'olistdb.tmp' doesn't exist\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Configurações de conexão com o banco de dados MySQL\n",
    "config = {\n",
    "    'user': 'root',\n",
    "    'password': '*Mss140920@',\n",
    "    'host': 'localhost',\n",
    "    'database': 'olistdb',\n",
    "    'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "# Função para limpar uma tabela\n",
    "def limpar_tabela(cursor, tabela):\n",
    "    cursor.execute(f\"DELETE FROM {tabela}\")\n",
    "\n",
    "# Função para carregar dados de um arquivo Parquet para uma tabela\n",
    "def carregar_dados_parquet(cursor, tabela_temp, caminho):\n",
    "    dataframe = pd.read_parquet(caminho)\n",
    "    dataframe = dataframe.fillna(0)  # Substituir valores NaN por 0\n",
    "    colunas = dataframe.columns.tolist()\n",
    "    placeholders = ','.join(['%s'] * len(colunas))\n",
    "    \n",
    "    # Utilizar INSERT INTO ... VALUES (...) AS alias e substituir VALUES(col) pelo alias.col na cláusula ON DUPLICATE KEY UPDATE\n",
    "    insert_query = f\"INSERT INTO {tabela_temp} ({', '.join(colunas)}) SELECT {placeholders} FROM tmp \" \\\n",
    "                  f\"ON DUPLICATE KEY UPDATE \" \\\n",
    "                  f\"{', '.join([f'{column}=tmp.{column}' for column in colunas])}\"\n",
    "    \n",
    "    valores = [tuple(row) for row in dataframe.values]\n",
    "    cursor.executemany(insert_query, valores)\n",
    "\n",
    "# Conectando ao banco de dados\n",
    "try:\n",
    "    cnx = mysql.connector.connect(**config)\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    # Verificando e carregando tabelas\n",
    "    for i in range(len(path['table_name'])):\n",
    "        tabela_temp = path['table_name'][i]\n",
    "        caminho = path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet'\n",
    "        \n",
    "        # Limpar tabela antes de carregar os dados\n",
    "        limpar_tabela(cursor, tabela_temp)\n",
    "        \n",
    "        carregar_dados_parquet(cursor, tabela_temp, caminho)\n",
    "        cnx.commit()\n",
    "        print(f\"Dados carregados na {tabela_temp} com sucesso!\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
    "\n",
    "finally:\n",
    "    if 'cursor' in locals():\n",
    "        cursor.close()\n",
    "    if 'cnx' in locals() and cnx.is_connected():\n",
    "        cnx.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados na items com sucesso!\n",
      "Dados carregados na payments com sucesso!\n",
      "Dados carregados na category com sucesso!\n",
      "Dados carregados na products com sucesso!\n",
      "Dados carregados na customers com sucesso!\n",
      "Dados carregados na sellers com sucesso!\n",
      "Dados carregados na reviews com sucesso!\n",
      "Dados carregados na orders com sucesso!\n",
      "Dados carregados na geolocation com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Configurações de conexão com o banco de dados MySQL\n",
    "config = {\n",
    "    'user': 'root',\n",
    "    'password': '*Mss140920@',\n",
    "    'host': 'localhost',\n",
    "    'database': 'olistdb',\n",
    "    'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "# Função para limpar uma tabela\n",
    "def limpar_tabela(cursor, tabela):\n",
    "    cursor.execute(f\"DELETE FROM {tabela_temp}\")\n",
    "\n",
    "# Função para carregar dados de um arquivo Parquet para uma tabela\n",
    "def carregar_dados_parquet(cursor, tabela_temp, caminho):\n",
    "    dataframe = pd.read_parquet(caminho)\n",
    "    dataframe = dataframe.fillna(0)  # Substituir valores NaN por 0\n",
    "    colunas = dataframe.columns.tolist()\n",
    "    placeholders = ','.join(['%s'] * len(colunas))\n",
    "    \n",
    "    # Utilizar INSERT INTO ... VALUES (...) AS alias e substituir VALUES(col) pelo alias.col na cláusula ON DUPLICATE KEY UPDATE\n",
    "    insert_query = f\"INSERT INTO {tabela_temp} ({', '.join(colunas)}) VALUES ({placeholders}) AS tmp \" \\\n",
    "                  f\"ON DUPLICATE KEY UPDATE \" \\\n",
    "                  f\"{', '.join([f'{column}=tmp.{column}' for column in colunas])}\"\n",
    "    \n",
    "    valores = [tuple(row) for row in dataframe.values]\n",
    "    cursor.executemany(insert_query, valores)\n",
    "\n",
    "# Conectando ao banco de dados\n",
    "try:\n",
    "    cnx = mysql.connector.connect(**config)\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    # Verificando e carregando tabelas\n",
    "    for i in range(len(path['table_name'])):\n",
    "        tabela_temp = path['table_name'][i]\n",
    "        caminho = path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet'\n",
    "        \n",
    "        # Limpar tabela antes de carregar os dados\n",
    "        limpar_tabela(cursor, tabela_temp)\n",
    "        \n",
    "        carregar_dados_parquet(cursor, tabela_temp, caminho)\n",
    "        cnx.commit()\n",
    "        print(f\"Dados carregados na {tabela_temp} com sucesso!\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
    "\n",
    "finally:\n",
    "    if 'cursor' in locals():\n",
    "        cursor.close()\n",
    "    if 'cnx' in locals() and cnx.is_connected():\n",
    "        cnx.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparando as temp cpm as tabelas originais, realizando updater, insert e delet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE executado com sucesso!\n",
      "INSERT executado com sucesso!\n",
      "DELETE executado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Configurações de conexão com o banco de dados MySQL\n",
    "config = {\n",
    "    'user': 'root',\n",
    "    'password': '*Mss140920@',\n",
    "    'host': 'localhost',\n",
    "    'database': 'olistdb',\n",
    "    'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "# Código SQL\n",
    "sql_update = \"\"\"\n",
    "UPDATE `category` AS destino\n",
    "JOIN `temp_category` AS origem ON destino.`product_category_name` = origem.`product_category_name`\n",
    "SET destino.`product_category_name` = origem.`product_category_name`,\n",
    "    destino.`product_category_name` = origem.`product_category_name`\n",
    "\"\"\"\n",
    "\n",
    "sql_insert = \"\"\"\n",
    "INSERT INTO `category` (`product_category_name`)\n",
    "SELECT origem.`product_category_name`\n",
    "FROM `temp_category` AS origem\n",
    "LEFT JOIN `category` AS destino ON origem.`product_category_name` = destino.`product_category_name`\n",
    "WHERE destino.`product_category_name` IS NULL\n",
    "\"\"\"\n",
    "\n",
    "sql_delete = \"\"\"\n",
    "DELETE destino\n",
    "FROM `category` AS destino\n",
    "LEFT JOIN `temp_category` AS origem ON destino.`product_category_name` = origem.`product_category_name`\n",
    "WHERE origem.`product_category_name` IS NULL\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Conectando ao banco de dados\n",
    "    cnx = mysql.connector.connect(**config)\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    # Executando as consultas SQL\n",
    "    cursor.execute(sql_update)\n",
    "    print(\"UPDATE executado com sucesso!\")\n",
    "\n",
    "    cursor.execute(sql_insert)\n",
    "    print(\"INSERT executado com sucesso!\")\n",
    "\n",
    "    cursor.execute(sql_delete)\n",
    "    print(\"DELETE executado com sucesso!\")\n",
    "\n",
    "    cnx.commit()\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
    "\n",
    "finally:\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if cnx.is_connected():\n",
    "        cnx.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>product_category_name_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>fraldas_higiene_adulto</td>\n",
       "      <td>diapers_and_hygiene_adulto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>fashion_roupa_infanto_juvenil_&amp;_infantil</td>\n",
       "      <td>fashion_childrens_clothes_&amp;_infantil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>seguros_e_servicos_prestacao</td>\n",
       "      <td>security_and_services_prestacao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>testes_projeto_trilha</td>\n",
       "      <td>testando_projeto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>outro_teste_para_garantir</td>\n",
       "      <td>garantindo_o_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>flores</td>\n",
       "      <td>flowers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>artes_e_artesanato</td>\n",
       "      <td>arts_and_craftmanship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>fraldas_higiene</td>\n",
       "      <td>diapers_and_hygiene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>fashion_roupa_infanto_juvenil</td>\n",
       "      <td>fashion_childrens_clothes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>seguros_e_servicos</td>\n",
       "      <td>security_and_services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       product_category_name   \n",
       "71                    fraldas_higiene_adulto  \\\n",
       "72  fashion_roupa_infanto_juvenil_&_infantil   \n",
       "73              seguros_e_servicos_prestacao   \n",
       "74                     testes_projeto_trilha   \n",
       "75                 outro_teste_para_garantir   \n",
       "..                                       ...   \n",
       "66                                    flores   \n",
       "67                        artes_e_artesanato   \n",
       "68                           fraldas_higiene   \n",
       "69             fashion_roupa_infanto_juvenil   \n",
       "70                        seguros_e_servicos   \n",
       "\n",
       "           product_category_name_english  \n",
       "71            diapers_and_hygiene_adulto  \n",
       "72  fashion_childrens_clothes_&_infantil  \n",
       "73       security_and_services_prestacao  \n",
       "74                      testando_projeto  \n",
       "75                     garantindo_o_test  \n",
       "..                                   ...  \n",
       "66                               flowers  \n",
       "67                 arts_and_craftmanship  \n",
       "68                   diapers_and_hygiene  \n",
       "69             fashion_childrens_clothes  \n",
       "70                 security_and_services  \n",
       "\n",
       "[76 rows x 2 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raww = pd.read_parquet('/home/michel/Documentos/opt/kaggle_olist/raw/product_category_name_translation.parquet')\n",
    "df_raww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>product_category_name_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>testes_projeto_trilha</td>\n",
       "      <td>testando_projeto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>outro_teste_para_garantir</td>\n",
       "      <td>garantindo_o_teste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fraldas_higiene_adulto</td>\n",
       "      <td>diapers_and_hygiene_adulto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fashion_roupa_infanto_juvenil_&amp;_infantil</td>\n",
       "      <td>fashion_childrens_clothes_&amp;_infantil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seguros_e_servicos_prestacao</td>\n",
       "      <td>security_and_services_prestacao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>flores</td>\n",
       "      <td>flowers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>artes_e_artesanato</td>\n",
       "      <td>arts_and_craftmanship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>fraldas_higiene</td>\n",
       "      <td>diapers_and_hygiene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>fashion_roupa_infanto_juvenil</td>\n",
       "      <td>fashion_childrens_clothes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>seguros_e_servicos</td>\n",
       "      <td>security_and_services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       product_category_name   \n",
       "0                      testes_projeto_trilha  \\\n",
       "1                  outro_teste_para_garantir   \n",
       "2                     fraldas_higiene_adulto   \n",
       "3   fashion_roupa_infanto_juvenil_&_infantil   \n",
       "4               seguros_e_servicos_prestacao   \n",
       "..                                       ...   \n",
       "71                                    flores   \n",
       "72                        artes_e_artesanato   \n",
       "73                           fraldas_higiene   \n",
       "74             fashion_roupa_infanto_juvenil   \n",
       "75                        seguros_e_servicos   \n",
       "\n",
       "           product_category_name_english  \n",
       "0                       testando_projeto  \n",
       "1                     garantindo_o_teste  \n",
       "2             diapers_and_hygiene_adulto  \n",
       "3   fashion_childrens_clothes_&_infantil  \n",
       "4        security_and_services_prestacao  \n",
       "..                                   ...  \n",
       "71                               flowers  \n",
       "72                 arts_and_craftmanship  \n",
       "73                   diapers_and_hygiene  \n",
       "74             fashion_childrens_clothes  \n",
       "75                 security_and_services  \n",
       "\n",
       "[76 rows x 2 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trusted = pd.read_parquet('/home/michel/Documentos/opt/kaggle_olist/trusted/product_category_name_translation.parquet')\n",
    "df_trusted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
