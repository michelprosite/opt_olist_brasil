{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michel Souza Santana\n",
    "## Projeto Desafio Aceleras\n",
    "## Trilha 1\n",
    "> Start: 15/05/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1 - Transformação do ER proposto em um BI, realizando o ETL usando uma ferrmenta local (Talend, Apache Hop, Nifi, Airflow, SSIS, Pentaho,…)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entenda o modelo ER: Familiarize-se com o modelo ER existente, incluindo as tabelas, relacionamentos e atributos. Isso ajudará você a mapear corretamente os dados durante a transformação.\n",
    "\n",
    "* Identifique os requisitos de BI: Compreenda as necessidades e requisitos do seu projeto de BI. Identifique as informações que você precisa extrair e apresentar no ambiente de BI.\n",
    "\n",
    "* Escolha uma ferramenta ETL: Pesquise e selecione uma ferramenta ETL adequada para sua transformação de dados. Existem várias opções disponíveis, como Pentaho Data Integration, Talend, Microsoft SQL Server Integration Services (SSIS), entre outras.\n",
    "\n",
    "* Instale a ferramenta ETL: Faça o download e instale a ferramenta ETL selecionada no seu ambiente local.\n",
    "\n",
    "* Conecte-se ao banco de dados: Configure a conexão da ferramenta ETL com o banco de dados que contém os dados do modelo ER. Forneça as credenciais de acesso necessárias para estabelecer a conexão.\n",
    "\n",
    "* Extração de dados: Utilizando a ferramenta ETL, extraia os dados do banco de dados conforme necessário para o seu projeto de BI. Isso pode envolver a seleção de tabelas específicas, filtragem de dados ou até mesmo a união de várias tabelas para obter as informações desejadas.\n",
    "\n",
    "* Limpeza e transformação de dados: Aplique as transformações necessárias nos dados extraídos para adequá-los às necessidades do ambiente de BI. Isso pode incluir a remoção de dados duplicados, preenchimento de valores ausentes, conversão de formatos de data, entre outros processos de limpeza e transformação.\n",
    "\n",
    "* Mapeamento para o modelo dimensional: Se você estiver construindo um data warehouse ou uma solução de BI baseada em modelo dimensional, mapeie os dados extraídos para as dimensões e fatos do seu modelo dimensional. Isso envolve a definição de hierarquias, chaves e relacionamentos.\n",
    "\n",
    "* Desenvolva fluxos de trabalho ETL: Utilizando a ferramenta ETL, crie fluxos de trabalho que automatizem a transformação de dados. Isso pode envolver a criação de transformações, tarefas agendadas e outras operações para garantir a integridade e atualização dos dados.\n",
    "\n",
    "* Carregamento dos dados: Carregue os dados transformados no ambiente de BI, que pode incluir um data warehouse, um banco de dados ou outra solução de armazenamento de dados.\n",
    "\n",
    "* Desenvolva visualizações e relatórios: Com os dados carregados no ambiente de BI, desenvolva visualizações e relatórios interativos para fornecer insights acionáveis aos usuários finais. Isso pode ser feito usando ferramentas de visualização de dados como Tableau, Power BI, QlikView, entre outras.\n",
    "\n",
    "* Teste e valide: Realize testes para garantir a precisão e a integridade dos dados transformados. Verifique se as visualiza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os processos de engenharia de dados (data engineering) são etapas sequenciais usadas para transformar e gerenciar dados em um pipeline de processamento. Esses processos são realizados em várias etapas, começando pelos dados brutos e terminando com os dados confiáveis e prontos para análise. Aqui está uma explicação de cada processo em ordem de execução:\n",
    "\n",
    "* Processo de Transient Data (Dados Transitórios):\n",
    "Os dados transitórios são dados temporários ou de curta duração usados para processamento e transformação intermediários. Esses dados podem ser gerados durante o processamento em tempo real, pipelines de processamento de dados em lote ou durante a integração de diferentes fontes de dados. Os dados transitórios são normalmente armazenados em sistemas de processamento de fluxo de dados ou bancos de dados temporários, e seu objetivo é suportar operações intermediárias antes que os dados sejam refinados e confiáveis.\n",
    "\n",
    "* Processo de Raw Data (Dados Brutos):\n",
    "Nesta fase inicial, os dados brutos são coletados de várias fontes, como bancos de dados, arquivos CSV, APIs, feeds de streaming, entre outros. Esses dados podem estar em formatos diversos e podem conter ruído, inconsistências e falta de estrutura. O objetivo do processo de Dados Brutos é extrair e armazenar esses dados em seu estado bruto original.\n",
    "\n",
    "* Processo de Trusted Data (Dados Confiáveis):\n",
    "Nesta fase final, os dados são considerados confiáveis, prontos para uso e adequados para análise e tomada de decisão. Isso envolve garantir a qualidade dos dados, validar a precisão, consistência e integridade dos dados, aplicar regras de negócios e conformidade, bem como implementar mecanismos de controle de qualidade. O objetivo é fornecer dados confiáveis que possam ser usados com confiança para análise, geração de relatórios e outras tarefas de negócios.\n",
    "\n",
    "* Processo de Refined Data (Dados Refinados):\n",
    "Nesta fase, os dados processados são refinados ainda mais para atender a requisitos específicos de negócios e análise. Isso pode incluir agregação de dados, cálculos adicionais, enriquecimento com informações adicionais, como dados geográficos ou dados de terceiros, e transformações personalizadas para atender às necessidades específicas dos usuários finais. O objetivo é fornecer dados refinados e mais valiosos para análise e tomada de decisão.\n",
    "\n",
    "* Processo de Data Engineering (Engenharia de Dados):\n",
    "Nesta etapa, os dados brutos são processados e transformados em um formato adequado para análise e uso posterior. Isso envolve atividades como limpeza de dados, padronização de formatos, filtragem de dados inválidos ou incompletos, remoção de duplicatas e criação de estruturas de dados otimizadas para consultas e processamento eficiente. O objetivo é obter dados estruturados e refinados que possam ser usados em análises e outros processos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A transformação de um modelo Entidade-Relacionamento (ER) em uma solução de Business Intelligence (BI) envolve várias etapas. Aqui está um passo a passo geral que pode ajudá-lo nesse processo:\n",
    "\n",
    "* Entendimento dos requisitos do negócio: Comece entendendo os requisitos do negócio e as necessidades de análise. Identifique quais informações são importantes, quais métricas e indicadores devem ser acompanhados e quais perguntas de negócio devem ser respondidas.\n",
    "\n",
    "* Modelagem do data warehouse: Com base nos requisitos do negócio, projete um modelo de data warehouse que atenda às necessidades de análise. O modelo de data warehouse é uma estrutura otimizada para consultas e análises de dados. Você pode usar uma abordagem dimensional, como o modelo estrela ou floco de neve, para organizar os dados em fatos e dimensões.\n",
    "\n",
    "* Extração, Transformação e Carga (ETL): Nesta etapa, extraia os dados relevantes do modelo ER proposto e transforme-os para se adequar ao modelo de data warehouse. Isso envolve limpeza de dados, padronização, agregação, cálculos e outras transformações necessárias. Em seguida, carregue os dados transformados no data warehouse.\n",
    "\n",
    "* Projeto e desenvolvimento de cubos OLAP: Cubos OLAP são estruturas multidimensionais que permitem uma análise rápida e flexível dos dados. Projete e desenvolva cubos OLAP com base nos requisitos do negócio e no modelo de data warehouse. Defina hierarquias, dimensões, medidas e agregações adequadas aos requisitos de análise.\n",
    "\n",
    "* Desenvolvimento de relatórios e painéis: Crie relatórios e painéis interativos que apresentem as informações relevantes de forma clara e visualmente atraente. Use ferramentas de BI, como o Pentaho, para projetar e desenvolver relatórios personalizados, gráficos e visualizações que atendam às necessidades dos usuários finais.\n",
    "\n",
    "* Implantação e manutenção: Implante a solução de BI em um ambiente de produção e teste-a para garantir que esteja funcionando corretamente. Monitore e mantenha regularmente o sistema de BI, atualizando os dados, ajustando as transformações ETL e fazendo melhorias contínuas conforme necessário.\n",
    "\n",
    "* Treinamento e adoção: Treine os usuários finais para que possam aproveitar ao máximo a solução de BI. Forneça suporte contínuo e promova a adoção da solução, demonstrando os benefícios e as capacidades de análise.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para elaborar uma abordagem dimensional, como o modelo estrela, para os dados do conjunto de dados \"Brazilian E-Commerce\" disponível no Kaggle (https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce), podemos considerar os seguintes elementos principais:\n",
    "\n",
    "* Tabela de Fatos (Fact Table):\n",
    "A tabela de fatos é o centro do modelo dimensional e contém as métricas quantitativas ou eventos que desejamos analisar. No caso do conjunto de dados \"Brazilian E-Commerce\", uma possível tabela de fatos pode ser a \"Orders\" (Pedidos), que registra informações sobre os pedidos realizados. Alguns atributos possíveis para essa tabela podem ser: ID do pedido, data do pedido, valor total do pedido, ID do cliente, ID do produto, ID do vendedor, etc.\n",
    "\n",
    "* Tabelas de Dimensões (Dimension Tables):\n",
    "As tabelas de dimensões fornecem o contexto para as métricas na tabela de fatos e contêm atributos descritivos que ajudam a filtrar e agrupar os dados. No contexto do conjunto de dados \"Brazilian E-Commerce\", podemos ter as seguintes tabelas de dimensões:\n",
    "\n",
    "* Tabela de Dimensão \"Clientes\": contendo informações sobre os clientes, como ID do cliente, nome, endereço, cidade, estado, etc.\n",
    "* Tabela de Dimensão \"Produtos\": contendo informações sobre os produtos, como ID do produto, nome do produto, categoria, preço, etc.\n",
    "* Tabela de Dimensão \"Vendedores\": contendo informações sobre os vendedores, como ID do vendedor, nome do vendedor, região, classificação, etc.\n",
    "* Tabela de Dimensão \"Datas\": contendo informações sobre as datas, como data, dia da semana, mês, trimestre, ano, feriados, etc.\n",
    "Cada tabela de dimensão teria uma chave primária única que se relaciona com a tabela de fatos através de chaves estrangeiras.\n",
    "\n",
    "* Tabelas de Dimensões Adicionais (Optional Dimension Tables):\n",
    "Dependendo dos requisitos de análise específicos do conjunto de dados \"Brazilian E-Commerce\", outras tabelas de dimensões adicionais podem ser incluídas para fornecer mais contexto aos dados. Por exemplo, uma tabela de dimensão \"Categorias\" poderia conter informações sobre as categorias dos produtos vendidos.\n",
    "\n",
    "* Tabelas de Bridge (Bridge Tables):\n",
    "Em alguns casos, pode ser necessário representar relacionamentos muitos-para-muitos entre dimensões. Nesses casos, tabelas de bridge podem ser usadas para criar associações entre várias dimensões. Por exemplo, uma tabela de bridge \"Pedidos_Produtos\" pode ser usada para representar quais produtos foram incluídos em cada pedido."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando o pyarrow para conversão dos arquivos csv em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Processo de Transient Data (Dados Transitórios):\n",
    "Os dados transitórios são dados temporários ou de curta duração usados para processamento e transformação intermediários. Esses dados podem ser gerados durante o processamento em tempo real, pipelines de processamento de dados em lote ou durante a integração de diferentes fontes de dados. Os dados transitórios são normalmente armazenados em sistemas de processamento de fluxo de dados ou bancos de dados temporários, e seu objetivo é suportar operações intermediárias antes que os dados sejam refinados e confiáveis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os diretórios estruturais do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = !pwd\n",
    "f = pd.read_csv(str(path_folder[0]) + '/folders.csv')\n",
    "lista_folders = f['Folders'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diretório engineer já existe.\n",
      "O diretório raw já existe.\n",
      "O diretório refined já existe.\n",
      "O diretório transient já existe.\n",
      "O diretório trusted já existe.\n"
     ]
    }
   ],
   "source": [
    "for i in lista_folders:\n",
    "    diretorio = str(path_folder[0]) \n",
    "\n",
    "    if not os.path.exists(diretorio + '/' + i):\n",
    "        os.makedirs(diretorio + '/' + i)\n",
    "        print(f\"Diretório {i} criado com sucesso!\")\n",
    "    else:\n",
    "        print(f\"O diretório {i} já existe.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificar as fontes de dados: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "\n",
    "A URL em questão se refere a um conjunto de dados disponibilizado no site Kaggle, que contém informações sobre o comércio eletrônico no Brasil. O conjunto de dados é intitulado \"Brazilian E-Commerce Public Dataset by Olist\" e foi criado pela empresa Olist, que é uma plataforma de vendas on-line que conecta pequenos e médios varejistas a marketplaces.\n",
    "\n",
    "O conjunto de dados contém informações de mais de 100 mil pedidos de clientes, com dados que incluem informações do produto, preços, prazos de entrega, avaliações de clientes e informações sobre o vendedor. Além disso, o conjunto de dados contém informações sobre geolocalização dos clientes, categoria de produtos e informações sobre a própria loja virtual.\n",
    "\n",
    "Este conjunto de dados pode ser extremamente útil para análises sobre comércio eletrônico no Brasil, permitindo a análise de tendências de consumo, comportamento dos clientes, performance de vendas e muito mais. A disponibilização de dados desse tipo é importante para o desenvolvimento de modelos de negócios mais eficientes e para a tomada de decisões mais informadas no setor de e-commerce brasileiro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copia os dados do site do kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando o Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixando os arquivos csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixar & diszipar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Arquivos já existem.\n"
     ]
    }
   ],
   "source": [
    "pasta = str(path_folder[0]) + '/transient'\n",
    "cont = 0\n",
    "verifica_arquivos = [os.path.splitext(arquivo)[0] for arquivo in os.listdir(pasta) if os.path.isfile(os.path.join(pasta, arquivo))]\n",
    "for i in verifica_arquivos: cont += 1\n",
    "print(cont)\n",
    "\n",
    "if cont <= 0:  \n",
    "    r = !pwd\n",
    "    PATH_FOLDER = r[0] + '/transient'\n",
    "    os.environ['PATH_FOLDER'] = PATH_FOLDER\n",
    "\n",
    "    !cd $PATH_FOLDER && kaggle datasets download -d olistbr/brazilian-ecommerce\n",
    "    !cd $PATH_FOLDER && unzip brazilian-ecommerce.zip\n",
    "    !cd $PATH_FOLDER && rm -r brazilian-ecommerce.zip\n",
    "    print('Arquivo carregado e descompactado.')\n",
    "else:\n",
    "    print('Arquivos já existem.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o arquivo 'controller.csv' na pasta enginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta = str(path_folder[0]) + '/transient'  # Substitua pelo caminho da pasta desejada\n",
    "\n",
    "# Capturar nomes dos arquivos e remover a extensão\n",
    "arquivos = [os.path.splitext(arquivo)[0] for arquivo in os.listdir(pasta) if os.path.isfile(os.path.join(pasta, arquivo))]\n",
    "# Eliminar as palavras indesejadas\n",
    "palavras_indesejadas = ['_dataset.csv', 'olist_', 'order_', '_dataset', 'product_', '_name', '_translation']\n",
    "nomes_limpos = [arquivo for arquivo in arquivos]\n",
    "for palavra in palavras_indesejadas:\n",
    "   nomes_limpos = [nome.replace(palavra, '') for nome in nomes_limpos]\n",
    "\n",
    "# Caminho do arquivo CSV a ser salvo\n",
    "caminho_arquivo = str(path_folder[0]) + '/' + 'controller.csv'\n",
    "\n",
    "# Definir os dados a serem escritos no arquivo CSV\n",
    "dados = [\n",
    "    [\"path_transient\", \"path_raw\", \"path_trusted\", \"table_transient\", \"table_raw\", \"table_trusted\", \"table_name\", \"table_name_temp\"]\n",
    "] + [\n",
    "    [str(path_folder[0]) + '/' + \"transient\", str(path_folder[0]) + '/' + \"raw\", str(path_folder[0]) + '/' + \"trusted\", nome, nome, nome, nome_limp, 'temp_' + nome_limp]\n",
    "    for nome, nome_limp in zip(arquivos, nomes_limpos)\n",
    "]\n",
    "\n",
    "# Salvar o arquivo CSV\n",
    "pd.DataFrame(dados).to_csv(caminho_arquivo, index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando tabela com os path necessários para manipulação dos arquivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando a formação dos path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_transient</th>\n",
       "      <th>path_raw</th>\n",
       "      <th>path_trusted</th>\n",
       "      <th>table_transient</th>\n",
       "      <th>table_raw</th>\n",
       "      <th>table_trusted</th>\n",
       "      <th>table_name</th>\n",
       "      <th>table_name_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>items</td>\n",
       "      <td>temp_items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>payments</td>\n",
       "      <td>temp_payments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>category</td>\n",
       "      <td>temp_category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>products</td>\n",
       "      <td>temp_products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>customers</td>\n",
       "      <td>temp_customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>sellers</td>\n",
       "      <td>temp_sellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>reviews</td>\n",
       "      <td>temp_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>orders</td>\n",
       "      <td>temp_orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>geolocation</td>\n",
       "      <td>temp_geolocation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      path_transient   \n",
       "0  /home/michel/Documentos/opt/kaggle_olist/trans...  \\\n",
       "1  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "2  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "3  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "4  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "5  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "6  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "7  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "8  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "\n",
       "                                       path_raw   \n",
       "0  /home/michel/Documentos/opt/kaggle_olist/raw  \\\n",
       "1  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "2  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "3  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "4  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "5  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "6  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "7  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "8  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "\n",
       "                                       path_trusted   \n",
       "0  /home/michel/Documentos/opt/kaggle_olist/trusted  \\\n",
       "1  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "2  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "3  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "4  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "5  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "6  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "7  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "8  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "\n",
       "                     table_transient                          table_raw   \n",
       "0          olist_order_items_dataset          olist_order_items_dataset  \\\n",
       "1       olist_order_payments_dataset       olist_order_payments_dataset   \n",
       "2  product_category_name_translation  product_category_name_translation   \n",
       "3             olist_products_dataset             olist_products_dataset   \n",
       "4            olist_customers_dataset            olist_customers_dataset   \n",
       "5              olist_sellers_dataset              olist_sellers_dataset   \n",
       "6        olist_order_reviews_dataset        olist_order_reviews_dataset   \n",
       "7               olist_orders_dataset               olist_orders_dataset   \n",
       "8          olist_geolocation_dataset          olist_geolocation_dataset   \n",
       "\n",
       "                       table_trusted   table_name   table_name_temp  \n",
       "0          olist_order_items_dataset        items        temp_items  \n",
       "1       olist_order_payments_dataset     payments     temp_payments  \n",
       "2  product_category_name_translation     category     temp_category  \n",
       "3             olist_products_dataset     products     temp_products  \n",
       "4            olist_customers_dataset    customers    temp_customers  \n",
       "5              olist_sellers_dataset      sellers      temp_sellers  \n",
       "6        olist_order_reviews_dataset      reviews      temp_reviews  \n",
       "7               olist_orders_dataset       orders       temp_orders  \n",
       "8          olist_geolocation_dataset  geolocation  temp_geolocation  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = pd.read_csv(str(path_folder[0]) + '/' + 'controller.csv')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michel/Documentos/opt/kaggle_olist/transient/olist_order_items_dataset.csv\n",
      "/home/michel/Documentos/opt/kaggle_olist/raw/olist_order_items_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "print(path['path_transient'][0] + '/' + path['table_transient'][0] + '.csv')\n",
    "print(path['path_raw'][0] + '/' + path['table_raw'][0] + '.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Processo de Raw Data (Dados Brutos):\n",
    "Nesta fase inicial, os dados brutos são coletados de várias fontes, como bancos de dados, arquivos CSV, APIs, feeds de streaming, entre outros. Esses dados podem estar em formatos diversos e podem conter ruído, inconsistências e falta de estrutura. O objetivo do processo de Dados Brutos é extrair e armazenar esses dados em seu estado bruto original."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertendo or arquivos CSV em parquet pela primeira vez .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O arquivo Parquet olist_order_items_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_order_items_dataset.parquet\n",
      "O arquivo Parquet olist_order_payments_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_order_payments_dataset.parquet\n",
      "O arquivo Parquet product_category_name_translation.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/product_category_name_translation.parquet\n",
      "O arquivo Parquet olist_products_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_products_dataset.parquet\n",
      "O arquivo Parquet olist_customers_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_customers_dataset.parquet\n",
      "O arquivo Parquet olist_sellers_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_sellers_dataset.parquet\n",
      "O arquivo Parquet olist_order_reviews_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_order_reviews_dataset.parquet\n",
      "O arquivo Parquet olist_orders_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_orders_dataset.parquet\n",
      "O arquivo Parquet olist_geolocation_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_geolocation_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(len(path['path_transient'])):\n",
    "    diretorio = str(path['path_raw'][i])  # Diretório onde o arquivo Parquet será salvo\n",
    "    nome_arquivo = str(path['table_raw'][i]) + '.parquet'  # Nome do arquivo Parquet\n",
    "\n",
    "    caminho_arquivo = os.path.join(diretorio, nome_arquivo)  # Caminho completo do arquivo Parquet\n",
    "\n",
    "    # Verifica a existência do arquivo que irá criar\n",
    "    if not os.path.exists(caminho_arquivo):\n",
    "        df = pd.read_csv(path['path_transient'][i] + '/' + path['table_transient'][i] + '.csv')\n",
    "        df.drop_duplicates(subset=df.columns[0], inplace=True)\n",
    "\n",
    "        if not df.empty:\n",
    "            df.to_parquet(caminho_arquivo)\n",
    "            print(f\"Arquivo Parquet {nome_arquivo} criado: {caminho_arquivo}\")\n",
    "        else:\n",
    "            print(f\"O DataFrame está vazio: {nome_arquivo}\")\n",
    "    else:\n",
    "        print(f\"O arquivo Parquet {nome_arquivo} já existe: {caminho_arquivo}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concatena_csv_parquet(arquivo_csv, arquivo_parquet):\n",
    "    # Verificar se as tabelas existem\n",
    "    for i in range(len(path['path_transient'])):\n",
    "\n",
    "        if not tabela_existe(arquivo_parquet):\n",
    "            continue\n",
    "\n",
    "        # Abrir o arquivo CSV\n",
    "        df1 = pd.read_csv(arquivo_csv)\n",
    "\n",
    "        # Abrir o arquivo Parquet\n",
    "        df2 = pd.read_parquet(arquivo_parquet)\n",
    "\n",
    "        # Concatenar os DataFrames verticalmente\n",
    "        df_concatenado = pd.concat([df2, df1], axis=0)\n",
    "\n",
    "        # Eliminando as duplicadas com referência na primeira coluna\n",
    "        df_concatenado = df_concatenado.drop_duplicates(subset=df_concatenado.columns[0], inplace=True)\n",
    "\n",
    "        # Retornar o DataFrame concatenado\n",
    "        return df_concatenado.to_parquet(str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet')\n",
    "\n",
    "    return None\n",
    "\n",
    "def tabela_existe(arquivo_parquet):\n",
    "    # Verificar se a tabela existe\n",
    "    # Implemente a lógica de verificação da existência da tabela\n",
    "    # Retorne True se a tabela existir, False caso contrário\n",
    "    return False  # Modifique para retornar o resultado correto\n",
    "\n",
    "# Definir os caminhos dos arquivos CSV e Parquet\n",
    "arquivo_csv = str(path['path_transient'][i]) + '/' + str(path['table_transient'][i]) + '.csv'\n",
    "arquivo_parquet = str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet'\n",
    "\n",
    "# Chamar a função concatena_csv_parquet()\n",
    "resultado = concatena_csv_parquet(arquivo_csv, arquivo_parquet)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Processo de Trusted Data (Dados Confiáveis):\n",
    "Nesta fase final, os dados são considerados confiáveis, prontos para uso e adequados para análise e tomada de decisão. Isso envolve garantir a qualidade dos dados, validar a precisão, consistência e integridade dos dados, aplicar regras de negócios e conformidade, bem como implementar mecanismos de controle de qualidade. O objetivo é fornecer dados confiáveis que possam ser usados com confiança para análise, geração de relatórios e outras tarefas de negócios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O arquivo Parquet olist_order_items_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/trusted/olist_order_items_dataset.parquet\n",
      "O arquivo Parquet olist_order_payments_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/trusted/olist_order_payments_dataset.parquet\n",
      "O arquivo Parquet product_category_name_translation.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/trusted/product_category_name_translation.parquet\n",
      "O arquivo Parquet olist_products_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/trusted/olist_products_dataset.parquet\n",
      "O arquivo Parquet olist_customers_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/trusted/olist_customers_dataset.parquet\n",
      "O arquivo Parquet olist_sellers_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/trusted/olist_sellers_dataset.parquet\n",
      "O arquivo Parquet olist_order_reviews_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/trusted/olist_order_reviews_dataset.parquet\n",
      "O arquivo Parquet olist_orders_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/trusted/olist_orders_dataset.parquet\n",
      "O arquivo Parquet olist_geolocation_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/trusted/olist_geolocation_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(len(path['table_name'])):\n",
    "\n",
    "    diretorio = str(path['path_trusted'][i])  # Diretório onde o arquivo Parquet será salvo\n",
    "    nome_arquivo = str(path['table_trusted'][i]) + '.parquet'  # Nome do arquivo Parquet\n",
    "\n",
    "    caminho_arquivo = os.path.join(diretorio, nome_arquivo)  # Caminho completo do arquivo Parquet\n",
    "\n",
    "    # Verifica se o arquivo Parquet já existe\n",
    "    if not os.path.exists(caminho_arquivo):\n",
    "        # O arquivo não existe, então você pode criar\n",
    "        df = pd.read_parquet(path['path_raw'][i] + '/' + path['table_raw'][i] + '.parquet')\n",
    "        df.to_parquet(str(path['path_trusted'][i]) + '/' + str(path['table_raw'][i]) + '.parquet')\n",
    "        print(f\"Arquivo Parquet {nome_arquivo} criado: {caminho_arquivo}\")\n",
    "    else:\n",
    "        # O arquivo já existe\n",
    "        print(f\"O arquivo Parquet {nome_arquivo} já existe: {caminho_arquivo}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Em uma segunda vez, tendo os arquivos parquet sido criados sempre será realizada uma verificação da fonte com a raw, se houver alterações, estas serão acrescentadas aos seus respectivos arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_csv_parquet(raw_path, trusted_path):\n",
    "\n",
    "    for i in range(len(path['table_name'])):\n",
    "        # Lê o arquivo CSV e o transforma em um DataFrame\n",
    "        df_csv = pd.read_csv(raw_path)\n",
    "        df_csv = df_csv.astype(str)\n",
    "\n",
    "        # Lê o arquivo Parquet e o transforma em um DataFrame\n",
    "        df_parquet = pd.read_parquet(trusted_path)\n",
    "\n",
    "        # Verifica se há diferenças na primeira coluna\n",
    "        if not df_csv.iloc[:, 0].equals(df_parquet.iloc[:, 0]):\n",
    "            # Encontra os dados diferentes na primeira coluna do CSV\n",
    "            dados_diferentes = df_csv[~df_csv.iloc[:, 0].isin(df_parquet.iloc[:, 0])]\n",
    "\n",
    "            if not dados_diferentes.empty:\n",
    "                # Concatena os dados diferentes no topo do DataFrame Parquet\n",
    "                df_parquet = pd.concat([dados_diferentes, df_parquet], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "                # Elimina as duplicadas se ainda houver\n",
    "                df_parquet = df_parquet.drop_duplicates(subset=df_parquet.columns[0])\n",
    "\n",
    "                # Salva o DataFrame atualizado no arquivo Parquet\n",
    "                df_parquet.to_parquet(trusted_path, index=False)\n",
    "                print(\"Foram encontradas diferenças na primeira coluna do CSV em relação ao Parquet. O arquivo Parquet foi atualizado.\")\n",
    "            else:\n",
    "                print(\"Não foram encontradas diferenças na primeira coluna do CSV em relação ao Parquet.\")\n",
    "        else:\n",
    "            print(\"Não foram encontradas diferenças na primeira coluna do CSV em relação ao Parquet.\")\n",
    "\n",
    "    # Exemplo de uso\n",
    "    raw_path = str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet'\n",
    "    trusted_path = str(path['path_trusted'][i] + path['table_trusted'][i] + '.parquet')\n",
    "    comparar_csv_parquet(raw_path, trusted_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo os arquivos e criando o banco de dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testando os caminhos e nomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nome da tabela: items\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_order_items_dataset.parquet\n",
      "\n",
      "Nome da tabela: payments\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_order_payments_dataset.parquet\n",
      "\n",
      "Nome da tabela: category\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedproduct_category_name_translation.parquet\n",
      "\n",
      "Nome da tabela: products\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_products_dataset.parquet\n",
      "\n",
      "Nome da tabela: customers\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_customers_dataset.parquet\n",
      "\n",
      "Nome da tabela: sellers\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_sellers_dataset.parquet\n",
      "\n",
      "Nome da tabela: reviews\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_order_reviews_dataset.parquet\n",
      "\n",
      "Nome da tabela: orders\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_orders_dataset.parquet\n",
      "\n",
      "Nome da tabela: geolocation\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_geolocation_dataset.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(path['table_name'])):\n",
    "    print(f\"Nome da tabela: {path['table_name'][i]}\")\n",
    "    print(f\"caminho dos dados: {path['path_trusted'][i] + path['table_trusted'][i] + '.parquet'}\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o Banco e as tabelas com base nos arquivos parquet existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Configurações de conexão com o banco de dados\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = '*Mss140920@'\n",
    "\n",
    "for i in range(len(path['table_name'])):\n",
    "    # Conectando ao servidor MySQL\n",
    "    connection = mysql.connector.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password\n",
    "    )\n",
    "\n",
    "    # Criando um cursor para executar comandos SQL\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Verificar se o banco 'olistdb' existe, se não, criá-lo\n",
    "    cursor.execute(\"SHOW DATABASES LIKE 'olistdb'\")\n",
    "    result = cursor.fetchone()\n",
    "    if not result:\n",
    "        cursor.execute(\"CREATE DATABASE olistdb\")\n",
    "\n",
    "    # Fechar o cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # Conectar ao banco de dados 'olistdb'\n",
    "    connection = mysql.connector.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        database='olistdb'\n",
    "    )\n",
    "\n",
    "    # Capturar os nomes das futuras tabelas em df['table_name']\n",
    "    df = pd.read_parquet(path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet')\n",
    "    table_name = path['table_name'][i]\n",
    "\n",
    "    # Criar um novo cursor\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Verificar se as tabelas existem, se não, criar as tabelas no banco de dados\n",
    "    cursor.execute(f\"SHOW TABLES LIKE '{table_name}'\")\n",
    "    result = cursor.fetchone()\n",
    "    if not result:\n",
    "        # Obter as colunas e seus tipos a partir do arquivo Parquet\n",
    "        table_path = path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet'\n",
    "        table_data = pd.read_parquet(table_path)\n",
    "        columns = table_data.columns\n",
    "        column_types = table_data.dtypes\n",
    "        table_index = table_data.index\n",
    "\n",
    "        # Criar a tabela com as colunas e tipos correspondentes\n",
    "        create_table_query = f\"CREATE TABLE {table_name} (\"\n",
    "        for col, col_type in zip(columns, column_types):\n",
    "            if col_type == 'int64':\n",
    "                col_type = 'INT'\n",
    "            elif col_type == 'float64':\n",
    "                col_type = 'FLOAT'\n",
    "            elif col_type == 'bool':\n",
    "                col_type = 'BOOLEAN'\n",
    "            else:\n",
    "                col_type = 'VARCHAR(255)'  # Tipo padrão se não for numérico\n",
    "\n",
    "            if col == columns[0]:\n",
    "                create_table_query += f\"{col} {col_type} PRIMARY KEY, \"\n",
    "            else:\n",
    "                create_table_query += f\"{col} {col_type}, \"\n",
    "\n",
    "        create_table_query = create_table_query.rstrip(', ')  # Remover a última vírgula\n",
    "        create_table_query += \")\"\n",
    "        cursor.execute(create_table_query)\n",
    "\n",
    "    # Fechar o cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # Fechar a conexão com o banco de dados\n",
    "    connection.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alimenta as tabelas recém criadas com o conteúdo dos arquivos parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tabela items já possui dados e não será carregada.\n",
      "A tabela payments já possui dados e não será carregada.\n",
      "A tabela category já possui dados e não será carregada.\n",
      "A tabela products já possui dados e não será carregada.\n",
      "A tabela customers já possui dados e não será carregada.\n",
      "A tabela sellers já possui dados e não será carregada.\n",
      "A tabela reviews já possui dados e não será carregada.\n",
      "A tabela orders já possui dados e não será carregada.\n",
      "A tabela geolocation já possui dados e não será carregada.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configurações de conexão com o banco de dados MySQL\n",
    "config = {\n",
    "    'user': 'root',\n",
    "    'password': '*Mss140920@',\n",
    "    'host': 'localhost',\n",
    "    'database': 'olistdb',\n",
    "    'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "# Função para verificar se uma tabela está vazia\n",
    "def tabela_vazia(cursor, tabela):\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {tabela}\")\n",
    "    result = cursor.fetchone()\n",
    "    return result[0] == 0\n",
    "\n",
    "# Função para carregar dados de um arquivo Parquet para uma tabela\n",
    "def carregar_dados_parquet(cursor, tabela, caminho):\n",
    "    dataframe = pd.read_parquet(caminho)\n",
    "    dataframe = dataframe.fillna(0)  # Substituir valores NaN por 0\n",
    "    valores = [tuple(row) for row in dataframe.values]\n",
    "    placeholders = ','.join(['%s'] * len(dataframe.columns))\n",
    "    cursor.executemany(f\"INSERT INTO {tabela} VALUES ({placeholders})\", valores)\n",
    "\n",
    "# Conectando ao banco de dados\n",
    "try:\n",
    "    cnx = mysql.connector.connect(**config)\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    # Verificando e carregando tabelas\n",
    "    for i in range(len(path['table_name'])):\n",
    "        tabela = path['table_name'][i]\n",
    "        if tabela_vazia(cursor, tabela):\n",
    "            caminho = path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet'\n",
    "            carregar_dados_parquet(cursor, tabela, caminho)\n",
    "            cnx.commit()\n",
    "            print(f\"Dados carregados na tabela {tabela} com sucesso!\")\n",
    "        else:\n",
    "            print(f\"A tabela {tabela} já possui dados e não será carregada.\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
    "\n",
    "finally:\n",
    "    if 'cursor' in locals():\n",
    "        cursor.close()\n",
    "    if 'cnx' in locals() and cnx.is_connected():\n",
    "        cnx.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando as tabelas temporárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Configurações de conexão com o banco de dados\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = '*Mss140920@'\n",
    "\n",
    "for i in range(len(path['table_name_temp'])):\n",
    "    # Conectando ao servidor MySQL\n",
    "    connection = mysql.connector.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password\n",
    "    )\n",
    "\n",
    "    # Criando um cursor para executar comandos SQL\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Verificar se o banco 'olistdb' existe, se não, criá-lo\n",
    "    cursor.execute(\"SHOW DATABASES LIKE 'olistdb'\")\n",
    "    result = cursor.fetchone()\n",
    "    if not result:\n",
    "        cursor.execute(\"CREATE DATABASE olistdb\")\n",
    "\n",
    "    # Fechar o cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # Conectar ao banco de dados 'olistdb'\n",
    "    connection = mysql.connector.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        database='olistdb'\n",
    "    )\n",
    "\n",
    "    # Capturar os nomes das futuras tabelas em df['table_name']\n",
    "    df = pd.read_parquet(path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet')\n",
    "    table_name = path['table_name_temp'][i]\n",
    "\n",
    "    # Criar um novo cursor\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Verificar se as tabelas existem, se não, criar as tabelas no banco de dados\n",
    "    cursor.execute(f\"SHOW TABLES LIKE '{table_name}'\")\n",
    "    result = cursor.fetchone()\n",
    "    if not result:\n",
    "        # Obter as colunas e seus tipos a partir do arquivo Parquet\n",
    "        table_path = path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet'\n",
    "        table_data = pd.read_parquet(table_path)\n",
    "        columns = table_data.columns\n",
    "        column_types = table_data.dtypes\n",
    "        table_index = table_data.index\n",
    "\n",
    "        # Criar a tabela com as colunas e tipos correspondentes\n",
    "        create_table_query = f\"CREATE TABLE {table_name} (\"\n",
    "        for col, col_type in zip(columns, column_types):\n",
    "            if col_type == 'int64':\n",
    "                col_type = 'INT'\n",
    "            elif col_type == 'float64':\n",
    "                col_type = 'FLOAT'\n",
    "            elif col_type == 'bool':\n",
    "                col_type = 'BOOLEAN'\n",
    "            else:\n",
    "                col_type = 'VARCHAR(255)'  # Tipo padrão se não for numérico\n",
    "\n",
    "            if col == columns[0]:\n",
    "                create_table_query += f\"{col} {col_type} PRIMARY KEY, \"\n",
    "            else:\n",
    "                create_table_query += f\"{col} {col_type}, \"\n",
    "\n",
    "        create_table_query = create_table_query.rstrip(', ')  # Remover a última vírgula\n",
    "        create_table_query += \")\"\n",
    "        cursor.execute(create_table_query)\n",
    "\n",
    "    # Fechar o cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # Fechar a conexão com o banco de dados\n",
    "    connection.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando as tabelas temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados na temp_items com sucesso!\n",
      "Dados carregados na temp_payments com sucesso!\n",
      "Dados carregados na temp_category com sucesso!\n",
      "Dados carregados na temp_products com sucesso!\n",
      "Dados carregados na temp_customers com sucesso!\n",
      "Dados carregados na temp_sellers com sucesso!\n",
      "Dados carregados na temp_reviews com sucesso!\n",
      "Dados carregados na temp_orders com sucesso!\n",
      "Dados carregados na temp_geolocation com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Configurações de conexão com o banco de dados MySQL\n",
    "config = {\n",
    "    'user': 'root',\n",
    "    'password': '*Mss140920@',\n",
    "    'host': 'localhost',\n",
    "    'database': 'olistdb',\n",
    "    'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "# Função para limpar uma tabela\n",
    "def limpar_tabela(cursor, tabela):\n",
    "    cursor.execute(f\"DELETE FROM {tabela_temp}\")\n",
    "\n",
    "# Função para carregar dados de um arquivo Parquet para uma tabela\n",
    "def carregar_dados_parquet(cursor, tabela_temp, caminho):\n",
    "    dataframe = pd.read_parquet(caminho)\n",
    "    dataframe = dataframe.fillna(0)  # Substituir valores NaN por 0\n",
    "    colunas = dataframe.columns.tolist()\n",
    "    placeholders = ','.join(['%s'] * len(colunas))\n",
    "    \n",
    "    # Utilizar INSERT INTO ... VALUES (...) AS alias e substituir VALUES(col) pelo alias.col na cláusula ON DUPLICATE KEY UPDATE\n",
    "    insert_query = f\"INSERT INTO {tabela_temp} ({', '.join(colunas)}) VALUES ({placeholders}) AS tmp \" \\\n",
    "                  f\"ON DUPLICATE KEY UPDATE \" \\\n",
    "                  f\"{', '.join([f'{column}=tmp.{column}' for column in colunas])}\"\n",
    "    \n",
    "    valores = [tuple(row) for row in dataframe.values]\n",
    "    cursor.executemany(insert_query, valores)\n",
    "\n",
    "# Conectando ao banco de dados\n",
    "try:\n",
    "    cnx = mysql.connector.connect(**config)\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    # Verificando e carregando tabelas\n",
    "    for i in range(len(path['table_name_temp'])):\n",
    "        tabela_temp = path['table_name_temp'][i]\n",
    "        caminho = path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet'\n",
    "        \n",
    "        # Limpar tabela antes de carregar os dados\n",
    "        limpar_tabela(cursor, tabela_temp)\n",
    "        \n",
    "        carregar_dados_parquet(cursor, tabela_temp, caminho)\n",
    "        cnx.commit()\n",
    "        print(f\"Dados carregados na {tabela_temp} com sucesso!\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
    "\n",
    "finally:\n",
    "    if 'cursor' in locals():\n",
    "        cursor.close()\n",
    "    if 'cnx' in locals() and cnx.is_connected():\n",
    "        cnx.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE executado com sucesso!\n",
      "INSERT executado com sucesso!\n",
      "DELETE executado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Configurações de conexão com o banco de dados MySQL\n",
    "config = {\n",
    "    'user': 'root',\n",
    "    'password': '*Mss140920@',\n",
    "    'host': 'localhost',\n",
    "    'database': 'olistdb',\n",
    "    'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "# Código SQL\n",
    "sql_update = \"\"\"\n",
    "UPDATE `category` AS destino\n",
    "JOIN `temp_category` AS origem ON destino.`product_category_name` = origem.`product_category_name`\n",
    "SET destino.`product_category_name` = origem.`product_category_name`,\n",
    "    destino.`product_category_name` = origem.`product_category_name`\n",
    "\"\"\"\n",
    "\n",
    "sql_insert = \"\"\"\n",
    "INSERT INTO `category` (`product_category_name`)\n",
    "SELECT origem.`product_category_name`\n",
    "FROM `temp_category` AS origem\n",
    "LEFT JOIN `category` AS destino ON origem.`product_category_name` = destino.`product_category_name`\n",
    "WHERE destino.`product_category_name` IS NULL\n",
    "\"\"\"\n",
    "\n",
    "sql_delete = \"\"\"\n",
    "DELETE destino\n",
    "FROM `category` AS destino\n",
    "LEFT JOIN `temp_category` AS origem ON destino.`product_category_name` = origem.`product_category_name`\n",
    "WHERE origem.`product_category_name` IS NULL\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Conectando ao banco de dados\n",
    "    cnx = mysql.connector.connect(**config)\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    # Executando as consultas SQL\n",
    "    cursor.execute(sql_update)\n",
    "    print(\"UPDATE executado com sucesso!\")\n",
    "\n",
    "    cursor.execute(sql_insert)\n",
    "    print(\"INSERT executado com sucesso!\")\n",
    "\n",
    "    cursor.execute(sql_delete)\n",
    "    print(\"DELETE executado com sucesso!\")\n",
    "\n",
    "    cnx.commit()\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
    "\n",
    "finally:\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if cnx.is_connected():\n",
    "        cnx.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
