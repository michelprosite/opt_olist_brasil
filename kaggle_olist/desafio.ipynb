{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michel Souza Santana\n",
    "## Projeto Desafio Aceleras\n",
    "## Trilha 1\n",
    "> Start: 15/05/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1 - Transformação do ER proposto em um BI, realizando o ETL usando uma ferrmenta local (Talend, Apache Hop, Nifi, Airflow, SSIS, Pentaho,…)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entenda o modelo ER: Familiarize-se com o modelo ER existente, incluindo as tabelas, relacionamentos e atributos. Isso ajudará você a mapear corretamente os dados durante a transformação.\n",
    "\n",
    "* Identifique os requisitos de BI: Compreenda as necessidades e requisitos do seu projeto de BI. Identifique as informações que você precisa extrair e apresentar no ambiente de BI.\n",
    "\n",
    "* Escolha uma ferramenta ETL: Pesquise e selecione uma ferramenta ETL adequada para sua transformação de dados. Existem várias opções disponíveis, como Pentaho Data Integration, Talend, Microsoft SQL Server Integration Services (SSIS), entre outras.\n",
    "\n",
    "* Instale a ferramenta ETL: Faça o download e instale a ferramenta ETL selecionada no seu ambiente local.\n",
    "\n",
    "* Conecte-se ao banco de dados: Configure a conexão da ferramenta ETL com o banco de dados que contém os dados do modelo ER. Forneça as credenciais de acesso necessárias para estabelecer a conexão.\n",
    "\n",
    "* Extração de dados: Utilizando a ferramenta ETL, extraia os dados do banco de dados conforme necessário para o seu projeto de BI. Isso pode envolver a seleção de tabelas específicas, filtragem de dados ou até mesmo a união de várias tabelas para obter as informações desejadas.\n",
    "\n",
    "* Limpeza e transformação de dados: Aplique as transformações necessárias nos dados extraídos para adequá-los às necessidades do ambiente de BI. Isso pode incluir a remoção de dados duplicados, preenchimento de valores ausentes, conversão de formatos de data, entre outros processos de limpeza e transformação.\n",
    "\n",
    "* Mapeamento para o modelo dimensional: Se você estiver construindo um data warehouse ou uma solução de BI baseada em modelo dimensional, mapeie os dados extraídos para as dimensões e fatos do seu modelo dimensional. Isso envolve a definição de hierarquias, chaves e relacionamentos.\n",
    "\n",
    "* Desenvolva fluxos de trabalho ETL: Utilizando a ferramenta ETL, crie fluxos de trabalho que automatizem a transformação de dados. Isso pode envolver a criação de transformações, tarefas agendadas e outras operações para garantir a integridade e atualização dos dados.\n",
    "\n",
    "* Carregamento dos dados: Carregue os dados transformados no ambiente de BI, que pode incluir um data warehouse, um banco de dados ou outra solução de armazenamento de dados.\n",
    "\n",
    "* Desenvolva visualizações e relatórios: Com os dados carregados no ambiente de BI, desenvolva visualizações e relatórios interativos para fornecer insights acionáveis aos usuários finais. Isso pode ser feito usando ferramentas de visualização de dados como Tableau, Power BI, QlikView, entre outras.\n",
    "\n",
    "* Teste e valide: Realize testes para garantir a precisão e a integridade dos dados transformados. Verifique se as visualiza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os processos de engenharia de dados (data engineering) são etapas sequenciais usadas para transformar e gerenciar dados em um pipeline de processamento. Esses processos são realizados em várias etapas, começando pelos dados brutos e terminando com os dados confiáveis e prontos para análise. Aqui está uma explicação de cada processo em ordem de execução:\n",
    "\n",
    "* Processo de Transient Data (Dados Transitórios):\n",
    "Os dados transitórios são dados temporários ou de curta duração usados para processamento e transformação intermediários. Esses dados podem ser gerados durante o processamento em tempo real, pipelines de processamento de dados em lote ou durante a integração de diferentes fontes de dados. Os dados transitórios são normalmente armazenados em sistemas de processamento de fluxo de dados ou bancos de dados temporários, e seu objetivo é suportar operações intermediárias antes que os dados sejam refinados e confiáveis.\n",
    "\n",
    "* Processo de Raw Data (Dados Brutos):\n",
    "Nesta fase inicial, os dados brutos são coletados de várias fontes, como bancos de dados, arquivos CSV, APIs, feeds de streaming, entre outros. Esses dados podem estar em formatos diversos e podem conter ruído, inconsistências e falta de estrutura. O objetivo do processo de Dados Brutos é extrair e armazenar esses dados em seu estado bruto original.\n",
    "\n",
    "* Processo de Trusted Data (Dados Confiáveis):\n",
    "Nesta fase final, os dados são considerados confiáveis, prontos para uso e adequados para análise e tomada de decisão. Isso envolve garantir a qualidade dos dados, validar a precisão, consistência e integridade dos dados, aplicar regras de negócios e conformidade, bem como implementar mecanismos de controle de qualidade. O objetivo é fornecer dados confiáveis que possam ser usados com confiança para análise, geração de relatórios e outras tarefas de negócios.\n",
    "\n",
    "* Processo de Refined Data (Dados Refinados):\n",
    "Nesta fase, os dados processados são refinados ainda mais para atender a requisitos específicos de negócios e análise. Isso pode incluir agregação de dados, cálculos adicionais, enriquecimento com informações adicionais, como dados geográficos ou dados de terceiros, e transformações personalizadas para atender às necessidades específicas dos usuários finais. O objetivo é fornecer dados refinados e mais valiosos para análise e tomada de decisão.\n",
    "\n",
    "* Processo de Data Engineering (Engenharia de Dados):\n",
    "Nesta etapa, os dados brutos são processados e transformados em um formato adequado para análise e uso posterior. Isso envolve atividades como limpeza de dados, padronização de formatos, filtragem de dados inválidos ou incompletos, remoção de duplicatas e criação de estruturas de dados otimizadas para consultas e processamento eficiente. O objetivo é obter dados estruturados e refinados que possam ser usados em análises e outros processos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A transformação de um modelo Entidade-Relacionamento (ER) em uma solução de Business Intelligence (BI) envolve várias etapas. Aqui está um passo a passo geral que pode ajudá-lo nesse processo:\n",
    "\n",
    "* Entendimento dos requisitos do negócio: Comece entendendo os requisitos do negócio e as necessidades de análise. Identifique quais informações são importantes, quais métricas e indicadores devem ser acompanhados e quais perguntas de negócio devem ser respondidas.\n",
    "\n",
    "* Modelagem do data warehouse: Com base nos requisitos do negócio, projete um modelo de data warehouse que atenda às necessidades de análise. O modelo de data warehouse é uma estrutura otimizada para consultas e análises de dados. Você pode usar uma abordagem dimensional, como o modelo estrela ou floco de neve, para organizar os dados em fatos e dimensões.\n",
    "\n",
    "* Extração, Transformação e Carga (ETL): Nesta etapa, extraia os dados relevantes do modelo ER proposto e transforme-os para se adequar ao modelo de data warehouse. Isso envolve limpeza de dados, padronização, agregação, cálculos e outras transformações necessárias. Em seguida, carregue os dados transformados no data warehouse.\n",
    "\n",
    "* Projeto e desenvolvimento de cubos OLAP: Cubos OLAP são estruturas multidimensionais que permitem uma análise rápida e flexível dos dados. Projete e desenvolva cubos OLAP com base nos requisitos do negócio e no modelo de data warehouse. Defina hierarquias, dimensões, medidas e agregações adequadas aos requisitos de análise.\n",
    "\n",
    "* Desenvolvimento de relatórios e painéis: Crie relatórios e painéis interativos que apresentem as informações relevantes de forma clara e visualmente atraente. Use ferramentas de BI, como o Pentaho, para projetar e desenvolver relatórios personalizados, gráficos e visualizações que atendam às necessidades dos usuários finais.\n",
    "\n",
    "* Implantação e manutenção: Implante a solução de BI em um ambiente de produção e teste-a para garantir que esteja funcionando corretamente. Monitore e mantenha regularmente o sistema de BI, atualizando os dados, ajustando as transformações ETL e fazendo melhorias contínuas conforme necessário.\n",
    "\n",
    "* Treinamento e adoção: Treine os usuários finais para que possam aproveitar ao máximo a solução de BI. Forneça suporte contínuo e promova a adoção da solução, demonstrando os benefícios e as capacidades de análise.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para elaborar uma abordagem dimensional, como o modelo estrela, para os dados do conjunto de dados \"Brazilian E-Commerce\" disponível no Kaggle (https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce), podemos considerar os seguintes elementos principais:\n",
    "\n",
    "* Tabela de Fatos (Fact Table):\n",
    "A tabela de fatos é o centro do modelo dimensional e contém as métricas quantitativas ou eventos que desejamos analisar. No caso do conjunto de dados \"Brazilian E-Commerce\", uma possível tabela de fatos pode ser a \"Orders\" (Pedidos), que registra informações sobre os pedidos realizados. Alguns atributos possíveis para essa tabela podem ser: ID do pedido, data do pedido, valor total do pedido, ID do cliente, ID do produto, ID do vendedor, etc.\n",
    "\n",
    "* Tabelas de Dimensões (Dimension Tables):\n",
    "As tabelas de dimensões fornecem o contexto para as métricas na tabela de fatos e contêm atributos descritivos que ajudam a filtrar e agrupar os dados. No contexto do conjunto de dados \"Brazilian E-Commerce\", podemos ter as seguintes tabelas de dimensões:\n",
    "\n",
    "* Tabela de Dimensão \"Clientes\": contendo informações sobre os clientes, como ID do cliente, nome, endereço, cidade, estado, etc.\n",
    "* Tabela de Dimensão \"Produtos\": contendo informações sobre os produtos, como ID do produto, nome do produto, categoria, preço, etc.\n",
    "* Tabela de Dimensão \"Vendedores\": contendo informações sobre os vendedores, como ID do vendedor, nome do vendedor, região, classificação, etc.\n",
    "* Tabela de Dimensão \"Datas\": contendo informações sobre as datas, como data, dia da semana, mês, trimestre, ano, feriados, etc.\n",
    "Cada tabela de dimensão teria uma chave primária única que se relaciona com a tabela de fatos através de chaves estrangeiras.\n",
    "\n",
    "* Tabelas de Dimensões Adicionais (Optional Dimension Tables):\n",
    "Dependendo dos requisitos de análise específicos do conjunto de dados \"Brazilian E-Commerce\", outras tabelas de dimensões adicionais podem ser incluídas para fornecer mais contexto aos dados. Por exemplo, uma tabela de dimensão \"Categorias\" poderia conter informações sobre as categorias dos produtos vendidos.\n",
    "\n",
    "* Tabelas de Bridge (Bridge Tables):\n",
    "Em alguns casos, pode ser necessário representar relacionamentos muitos-para-muitos entre dimensões. Nesses casos, tabelas de bridge podem ser usadas para criar associações entre várias dimensões. Por exemplo, uma tabela de bridge \"Pedidos_Produtos\" pode ser usada para representar quais produtos foram incluídos em cada pedido."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando o pyarrow para conversão dos arquivos csv em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Processo de Transient Data (Dados Transitórios):\n",
    "Os dados transitórios são dados temporários ou de curta duração usados para processamento e transformação intermediários. Esses dados podem ser gerados durante o processamento em tempo real, pipelines de processamento de dados em lote ou durante a integração de diferentes fontes de dados. Os dados transitórios são normalmente armazenados em sistemas de processamento de fluxo de dados ou bancos de dados temporários, e seu objetivo é suportar operações intermediárias antes que os dados sejam refinados e confiáveis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os diretórios estruturais do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = !pwd\n",
    "f = pd.read_csv(str(path_folder[0]) + '/folders.csv')\n",
    "lista_folders = f['Folders'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diretório engineer já existe.\n",
      "O diretório raw já existe.\n",
      "O diretório refined já existe.\n",
      "O diretório transient já existe.\n",
      "O diretório trusted já existe.\n"
     ]
    }
   ],
   "source": [
    "for i in lista_folders:\n",
    "    diretorio = str(path_folder[0]) \n",
    "\n",
    "    if not os.path.exists(diretorio + '/' + i):\n",
    "        os.makedirs(diretorio + '/' + i)\n",
    "        print(f\"Diretório {i} criado com sucesso!\")\n",
    "    else:\n",
    "        print(f\"O diretório {i} já existe.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificar as fontes de dados: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "\n",
    "A URL em questão se refere a um conjunto de dados disponibilizado no site Kaggle, que contém informações sobre o comércio eletrônico no Brasil. O conjunto de dados é intitulado \"Brazilian E-Commerce Public Dataset by Olist\" e foi criado pela empresa Olist, que é uma plataforma de vendas on-line que conecta pequenos e médios varejistas a marketplaces.\n",
    "\n",
    "O conjunto de dados contém informações de mais de 100 mil pedidos de clientes, com dados que incluem informações do produto, preços, prazos de entrega, avaliações de clientes e informações sobre o vendedor. Além disso, o conjunto de dados contém informações sobre geolocalização dos clientes, categoria de produtos e informações sobre a própria loja virtual.\n",
    "\n",
    "Este conjunto de dados pode ser extremamente útil para análises sobre comércio eletrônico no Brasil, permitindo a análise de tendências de consumo, comportamento dos clientes, performance de vendas e muito mais. A disponibilização de dados desse tipo é importante para o desenvolvimento de modelos de negócios mais eficientes e para a tomada de decisões mais informadas no setor de e-commerce brasileiro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copia os dados do site do kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando o Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixando os arquivos csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixar & diszipar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/michel/.kaggle/kaggle.json'\n",
      "Downloading brazilian-ecommerce.zip to /home/michel/Documentos/opt/kaggle_olist/transient\n",
      "100%|██████████████████████████████████████| 42.6M/42.6M [00:10<00:00, 4.68MB/s]\n",
      "100%|██████████████████████████████████████| 42.6M/42.6M [00:10<00:00, 4.47MB/s]\n",
      "Archive:  brazilian-ecommerce.zip\n",
      "  inflating: olist_customers_dataset.csv  \n",
      "  inflating: olist_geolocation_dataset.csv  \n",
      "  inflating: olist_order_items_dataset.csv  \n",
      "  inflating: olist_order_payments_dataset.csv  \n",
      "  inflating: olist_order_reviews_dataset.csv  \n",
      "  inflating: olist_orders_dataset.csv  \n",
      "  inflating: olist_products_dataset.csv  \n",
      "  inflating: olist_sellers_dataset.csv  \n",
      "  inflating: product_category_name_translation.csv  \n",
      "Arquivo carregado e descompactado.\n"
     ]
    }
   ],
   "source": [
    "pasta = str(path_folder[0]) + '/transient'\n",
    "cont = 0\n",
    "verifica_arquivos = [os.path.splitext(arquivo)[0] for arquivo in os.listdir(pasta) if os.path.isfile(os.path.join(pasta, arquivo))]\n",
    "for i in verifica_arquivos: cont += 1\n",
    "print(cont)\n",
    "\n",
    "if cont <= 0:  \n",
    "    r = !pwd\n",
    "    PATH_FOLDER = r[0] + '/transient'\n",
    "    os.environ['PATH_FOLDER'] = PATH_FOLDER\n",
    "\n",
    "    !cd $PATH_FOLDER && kaggle datasets download -d olistbr/brazilian-ecommerce\n",
    "    !cd $PATH_FOLDER && unzip brazilian-ecommerce.zip\n",
    "    !cd $PATH_FOLDER && rm -r brazilian-ecommerce.zip\n",
    "    print('Arquivo carregado e descompactado.')\n",
    "else:\n",
    "    print('Arquivos já existem.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o arquivo 'controller.csv' na pasta enginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta = str(path_folder[0]) + '/transient'  # Substitua pelo caminho da pasta desejada\n",
    "\n",
    "# Capturar nomes dos arquivos e remover a extensão\n",
    "arquivos = [os.path.splitext(arquivo)[0] for arquivo in os.listdir(pasta) if os.path.isfile(os.path.join(pasta, arquivo))]\n",
    "# Eliminar as palavras indesejadas\n",
    "palavras_indesejadas = ['_dataset.csv', 'olist_', 'order_', '_dataset', 'product_', '_name', '_translation']\n",
    "nomes_limpos = [arquivo for arquivo in arquivos]\n",
    "for palavra in palavras_indesejadas:\n",
    "   nomes_limpos = [nome.replace(palavra, '') for nome in nomes_limpos]\n",
    "\n",
    "# Caminho do arquivo CSV a ser salvo\n",
    "caminho_arquivo = str(path_folder[0]) + '/' + 'controller.csv'\n",
    "\n",
    "# Definir os dados a serem escritos no arquivo CSV\n",
    "dados = [\n",
    "    [\"path_transient\", \"path_raw\", \"table_transient\", \"table_raw\", \"table_name\"]\n",
    "] + [\n",
    "    [str(path_folder[0]) + '/' + \"transient\", str(path_folder[0]) + '/' + \"raw\", nome, nome, nome_limp]\n",
    "    for nome, nome_limp in zip(arquivos, nomes_limpos)\n",
    "]\n",
    "\n",
    "# Salvar o arquivo CSV\n",
    "pd.DataFrame(dados).to_csv(caminho_arquivo, index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando tabela com os path necessários para manipulação dos arquivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando a formação dos path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_transient</th>\n",
       "      <th>path_raw</th>\n",
       "      <th>table_transient</th>\n",
       "      <th>table_raw</th>\n",
       "      <th>table_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>payments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>sellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>geolocation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      path_transient   \n",
       "0  /home/michel/Documentos/opt/kaggle_olist/trans...  \\\n",
       "1  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "2  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "3  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "4  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "5  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "6  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "7  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "8  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "\n",
       "                                       path_raw   \n",
       "0  /home/michel/Documentos/opt/kaggle_olist/raw  \\\n",
       "1  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "2  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "3  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "4  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "5  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "6  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "7  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "8  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "\n",
       "                     table_transient                          table_raw   \n",
       "0          olist_order_items_dataset          olist_order_items_dataset  \\\n",
       "1       olist_order_payments_dataset       olist_order_payments_dataset   \n",
       "2  product_category_name_translation  product_category_name_translation   \n",
       "3             olist_products_dataset             olist_products_dataset   \n",
       "4            olist_customers_dataset            olist_customers_dataset   \n",
       "5              olist_sellers_dataset              olist_sellers_dataset   \n",
       "6        olist_order_reviews_dataset        olist_order_reviews_dataset   \n",
       "7               olist_orders_dataset               olist_orders_dataset   \n",
       "8          olist_geolocation_dataset          olist_geolocation_dataset   \n",
       "\n",
       "    table_name  \n",
       "0        items  \n",
       "1     payments  \n",
       "2     category  \n",
       "3     products  \n",
       "4    customers  \n",
       "5      sellers  \n",
       "6      reviews  \n",
       "7       orders  \n",
       "8  geolocation  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = pd.read_csv(str(path_folder[0]) + '/' + 'controller.csv')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michel/Documentos/opt/kaggle_olist/transient/olist_order_items_dataset.csv\n",
      "/home/michel/Documentos/opt/kaggle_olist/raw/olist_order_items_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "print(path['path_transient'][0] + '/' + path['table_transient'][0] + '.csv')\n",
    "print(path['path_raw'][0] + '/' + path['table_raw'][0] + '.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertendo or arquivos CSV em pela primeira vez .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O arquivo Parquet olist_order_items_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_order_items_dataset.parquet\n",
      "O arquivo Parquet olist_order_payments_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_order_payments_dataset.parquet\n",
      "O arquivo Parquet product_category_name_translation.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/product_category_name_translation.parquet\n",
      "O arquivo Parquet olist_products_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_products_dataset.parquet\n",
      "O arquivo Parquet olist_customers_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_customers_dataset.parquet\n",
      "O arquivo Parquet olist_sellers_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_sellers_dataset.parquet\n",
      "O arquivo Parquet olist_order_reviews_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_order_reviews_dataset.parquet\n",
      "O arquivo Parquet olist_orders_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_orders_dataset.parquet\n",
      "O arquivo Parquet olist_geolocation_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_geolocation_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(len(path['path_transient'])):\n",
    "\n",
    "    diretorio = str(path['path_raw'][i])  # Diretório onde o arquivo Parquet será salvo\n",
    "    nome_arquivo = str(path['table_raw'][i]) + '.parquet'  # Nome do arquivo Parquet\n",
    "\n",
    "    caminho_arquivo = os.path.join(diretorio, nome_arquivo)  # Caminho completo do arquivo Parquet\n",
    "\n",
    "    # Verifica se o arquivo Parquet já existe\n",
    "    if not os.path.exists(caminho_arquivo):\n",
    "        # O arquivo não existe, então você pode criar\n",
    "        df = pd.read_csv(path['path_transient'][i] + '/' + path['table_transient'][i] + '.csv')\n",
    "        df = df.astype('str')\n",
    "        df.to_parquet(str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet')\n",
    "        print(f\"Arquivo Parquet {nome_arquivo} criado: {caminho_arquivo}\")\n",
    "    else:\n",
    "        # O arquivo já existe\n",
    "        print(f\"O arquivo Parquet {nome_arquivo} já existe: {caminho_arquivo}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar um código que receba um arquivo csv, transforme em um df depois receba um arquivo parquet tranforme em df, depois verifique na primeira coluna de cada df se existem dados diferente e ou acrescentados no arquivo csv com relação ao arquivo parquet, se sim, acrescentar os dados diferentes no arquivo parquete no topo do df parquet e reiniciar o index, se não retorna uma mensagem informando que não hove alterações."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Em uma segunda vez, tendo oas arquivos parquet sido criados sepres será realizada uma verificação da fonte com a raw, se houver alterações, estas serão acrescentadas dos seus respectivos arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foram encontradas diferenças na primeira coluna do CSV em relação ao Parquet. O arquivo Parquet foi atualizado.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def comparar_csv_parquet(csv_path, parquet_path):\n",
    "    # Lê o arquivo CSV e o transforma em um DataFrame\n",
    "    df_csv = pd.read_csv(csv_path)\n",
    "    df_csv = df_csv.astype(str)\n",
    "\n",
    "    # Lê o arquivo Parquet e o transforma em um DataFrame\n",
    "    df_parquet = pd.read_parquet(parquet_path)\n",
    "\n",
    "    # Verifica se há diferenças na primeira coluna\n",
    "    if not df_csv.iloc[:, 0].equals(df_parquet.iloc[:, 0]):\n",
    "        # Encontra os dados diferentes na primeira coluna do CSV\n",
    "        dados_diferentes = df_csv[~df_csv.iloc[:, 0].isin(df_parquet.iloc[:, 0])]\n",
    "\n",
    "        if not dados_diferentes.empty:\n",
    "            # Concatena os dados diferentes no topo do DataFrame Parquet\n",
    "            df_parquet = pd.concat([dados_diferentes, df_parquet], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "            # Salva o DataFrame atualizado no arquivo Parquet\n",
    "            df_parquet.to_parquet(parquet_path, index=False)\n",
    "            print(\"Foram encontradas diferenças na primeira coluna do CSV em relação ao Parquet. O arquivo Parquet foi atualizado.\")\n",
    "        else:\n",
    "            print(\"Não foram encontradas diferenças na primeira coluna do CSV em relação ao Parquet.\")\n",
    "    else:\n",
    "        print(\"Não foram encontradas diferenças na primeira coluna do CSV em relação ao Parquet.\")\n",
    "\n",
    "# Exemplo de uso\n",
    "csv_path = \"/home/michel/Documentos/opt/kaggle_olist/transient/olist_customers_dataset.csv\"\n",
    "parquet_path = \"/home/michel/Documentos/opt/kaggle_olist/raw/olist_customers_dataset.parquet\"\n",
    "comparar_csv_parquet(csv_path, parquet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for i in range(len(path['table_name'])):\n",
    "    # Ler o primeiro arquivo parquet e converter em DataFrame\n",
    "    df_1 = pd.read_parquet(str(path['path_raw'][i]) + '/' + 'parquet' + '/' + str(path['table_raw'][i]) + '.parquet')\n",
    "    df_1.drop_duplicates(subset=df_1.columns[0], inplace=True)\n",
    "    df_1.to_parquet(str(path['path_raw'][i]) + '/' + 'parquet' + '/' + str(path['table_raw'][i]) + '.parquet')\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MERGE INTO tabela_original t\\nUSING (\\n    SELECT DISTINCT coluna1, coluna2, coluna3, ...\\n    FROM tabela_original\\n) s ON (t.coluna1 = s.coluna1)\\nWHEN MATCHED THEN\\n    DELETE;\\n'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"MERGE INTO tabela_original t\n",
    "USING (\n",
    "    SELECT DISTINCT coluna1, coluna2, coluna3, ...\n",
    "    FROM tabela_original\n",
    ") s ON (t.coluna1 = s.coluna1)\n",
    "WHEN MATCHED THEN\n",
    "    DELETE;\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo os arquivos e criando o banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_transient</th>\n",
       "      <th>path_raw</th>\n",
       "      <th>table_transient</th>\n",
       "      <th>table_raw</th>\n",
       "      <th>table_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>payments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>sellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>geolocation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      path_transient   \n",
       "0  /home/michel/Documentos/opt/kaggle_olist/trans...  \\\n",
       "1  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "2  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "3  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "4  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "5  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "6  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "7  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "8  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "\n",
       "                                       path_raw   \n",
       "0  /home/michel/Documentos/opt/kaggle_olist/raw  \\\n",
       "1  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "2  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "3  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "4  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "5  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "6  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "7  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "8  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "\n",
       "                     table_transient                          table_raw   \n",
       "0          olist_order_items_dataset          olist_order_items_dataset  \\\n",
       "1       olist_order_payments_dataset       olist_order_payments_dataset   \n",
       "2  product_category_name_translation  product_category_name_translation   \n",
       "3             olist_products_dataset             olist_products_dataset   \n",
       "4            olist_customers_dataset            olist_customers_dataset   \n",
       "5              olist_sellers_dataset              olist_sellers_dataset   \n",
       "6        olist_order_reviews_dataset        olist_order_reviews_dataset   \n",
       "7               olist_orders_dataset               olist_orders_dataset   \n",
       "8          olist_geolocation_dataset          olist_geolocation_dataset   \n",
       "\n",
       "    table_name  \n",
       "0        items  \n",
       "1     payments  \n",
       "2     category  \n",
       "3     products  \n",
       "4    customers  \n",
       "5      sellers  \n",
       "6      reviews  \n",
       "7       orders  \n",
       "8  geolocation  "
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_order_items_dataset.parquet\n",
      "items\n",
      "/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_order_payments_dataset.parquet\n",
      "payments\n",
      "/home/michel/Documentos/opt/kaggle_olist/raw/parquet/product_category_name_translation.parquet\n",
      "category\n",
      "/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_products_dataset.parquet\n",
      "products\n",
      "/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_customers_dataset.parquet\n",
      "customers\n",
      "/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_sellers_dataset.parquet\n",
      "sellers\n",
      "/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_order_reviews_dataset.parquet\n",
      "reviews\n",
      "/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_orders_dataset.parquet\n",
      "orders\n",
      "/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_geolocation_dataset.parquet\n",
      "geolocation\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(path['table_name'])):\n",
    "    print(path['path_raw'][i] + '/parquet/' + path['table_raw'][i] + '.parquet')\n",
    "    print(path['table_name'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados da tabela items carregados com sucesso!\n",
      "Dados da tabela payments carregados com sucesso!\n",
      "Dados da tabela category carregados com sucesso!\n",
      "Dados da tabela products carregados com sucesso!\n",
      "Dados da tabela customers carregados com sucesso!\n",
      "Dados da tabela sellers carregados com sucesso!\n",
      "Dados da tabela reviews carregados com sucesso!\n",
      "Dados da tabela orders carregados com sucesso!\n",
      "Dados da tabela geolocation carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import io\n",
    "\n",
    "for i in range(len(path['table_name'])):\n",
    "\n",
    "    # Caminho para o arquivo Parquet\n",
    "    caminho_arquivo = path['path_raw'][i] + '/parquet/' + path['table_raw'][i] + '.parquet'\n",
    "    nome_tabela = path['table_name'][i]\n",
    "\n",
    "    # Verifica se o arquivo Parquet existe\n",
    "    if not os.path.exists(caminho_arquivo):\n",
    "        print(\"Arquivo Parquet não encontrado.\")\n",
    "        exit()\n",
    "\n",
    "    # Configurações de conexão com o banco de dados PostgreSQL\n",
    "    host = \"localhost\"\n",
    "    porta = \"5432\"\n",
    "    banco = \"airflow\"\n",
    "    usuario = \"airflow\"\n",
    "    senha = \"airflow\"\n",
    "\n",
    "    # Conexão com o banco de dados\n",
    "    conexao = psycopg2.connect(\n",
    "        host=host,\n",
    "        port=porta,\n",
    "        database=banco,\n",
    "        user=usuario,\n",
    "        password=senha\n",
    "    )\n",
    "\n",
    "    # Verifica se o banco de dados \"olistdb\" existe\n",
    "    cursor = conexao.cursor()\n",
    "    cursor.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = 'olistdb'\")\n",
    "    banco_existe = cursor.fetchone()\n",
    "\n",
    "    if not banco_existe:\n",
    "        # Criação do banco de dados \"olistdb\"\n",
    "        cursor.execute(\"CREATE DATABASE olistdb\")\n",
    "        print(\"Banco de dados 'olistdb' criado.\")\n",
    "\n",
    "    # Conexão com o banco de dados \"olistdb\"\n",
    "    conexao.close()\n",
    "    conexao = psycopg2.connect(\n",
    "        host=host,\n",
    "        port=porta,\n",
    "        database='olistdb',\n",
    "        user=usuario,\n",
    "        password=senha\n",
    "    )\n",
    "\n",
    "    # Criação da tabela no banco de dados\n",
    "    cursor = conexao.cursor()\n",
    "    cursor.execute(\"CREATE TABLE IF NOT EXISTS {0} ();\".format(nome_tabela))\n",
    "\n",
    "    # Leitura do arquivo Parquet\n",
    "    dados_parquet = pd.read_parquet(caminho_arquivo)\n",
    "\n",
    "    # Atualização da definição da tabela no banco de dados\n",
    "    for coluna in dados_parquet.columns:\n",
    "        cursor.execute(\"ALTER TABLE {0} ADD COLUMN IF NOT EXISTS {1} VARCHAR;\".format(nome_tabela, coluna))\n",
    "\n",
    "    try:\n",
    "    # Carregamento dos dados na tabela usando COPY FROM STDIN\n",
    "        buffer = io.StringIO()\n",
    "        dados_parquet = dados_parquet.replace(\"\\r\", \"\\n\")\n",
    "        dados_parquet.to_csv(buffer, index=False, header=False, sep=\"\\t\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "        buffer.seek(0)\n",
    "        cursor.copy_from(buffer, nome_tabela, sep=\"\\t\")\n",
    "    except psycopg2.errors.BadCopyFileFormat as e:\n",
    "        # Lidar com o erro ou simplesmente ignorá-lo\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Confirmação da transação e fechamento da conexão\n",
    "    conexao.commit()\n",
    "    conexao.close()\n",
    "\n",
    "    print(f\"Dados da tabela {path['table_name'][i]} carregados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport psycopg2\\nimport os\\nimport io\\n\\n# Caminho para o arquivo Parquet\\ncaminho_arquivo = \"/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_customers_dataset.parquet\"\\nnome_tabela = \\'customers\\'\\n\\n# Verifica se o arquivo Parquet existe\\nif not os.path.exists(caminho_arquivo):\\n    print(\"Arquivo Parquet não encontrado.\")\\n    exit()\\n\\n# Configurações de conexão com o banco de dados PostgreSQL\\nhost = \"localhost\"\\nporta = \"5432\"\\nbanco = \"airflow\"\\nusuario = \"airflow\"\\nsenha = \"airflow\"\\n\\n# Conexão com o banco de dados\\nconexao = psycopg2.connect(\\n    host=host,\\n    port=porta,\\n    database=banco,\\n    user=usuario,\\n    password=senha\\n)\\n\\n# Verifica se a tabela já existe\\ncursor = conexao.cursor()\\ncursor.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = %s);\", (nome_tabela,))\\ntabela_existe = cursor.fetchone()[0]\\n\\nif tabela_existe:\\n    print(\"A tabela já existe. Abortando.\")\\n    conexao.close()\\n    exit()\\n\\n# Verifica se o banco de dados \"olistdb\" existe\\ncursor.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = \\'olistdb\\'\")\\nbanco_existe = cursor.fetchone()\\n\\nif not banco_existe:\\n    # Criação do banco de dados \"olistdb\"\\n    cursor.execute(\"CREATE DATABASE olistdb\")\\n    print(\"Banco de dados \\'olistdb\\' criado.\")\\n\\n# Conexão com o banco de dados \"olistdb\"\\nconexao.close()\\nconexao = psycopg2.connect(\\n    host=host,\\n    port=porta,\\n    database=\\'olistdb\\',\\n    user=usuario,\\n    password=senha\\n)\\n\\n# Leitura do arquivo Parquet\\ndados_parquet = pd.read_parquet(caminho_arquivo)\\n\\n# Criação da tabela no banco de dados\\ncolunas_definicao = \\', \\'.join(f\"{coluna} VARCHAR\" for coluna in dados_parquet.columns)\\ncursor = conexao.cursor()\\ncursor.execute(f\"CREATE TABLE {nome_tabela} ({colunas_definicao});\")\\n\\n# Carregamento dos dados na tabela usando COPY FROM STDIN\\nbuffer = io.StringIO()\\ndados_parquet.to_csv(buffer, index=False, header=False, sep=\"\\t\")\\nbuffer.seek(0)\\ncursor.copy_from(buffer, nome_tabela, sep=\"\\t\")\\n\\n# Confirmação da transação e fechamento da conexão\\nconexao.commit()\\nconexao.close()\\n\\nprint(\"Dados carregados com sucesso!\")'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import io\n",
    "\n",
    "# Caminho para o arquivo Parquet\n",
    "caminho_arquivo = \"/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_customers_dataset.parquet\"\n",
    "nome_tabela = 'customers'\n",
    "\n",
    "# Verifica se o arquivo Parquet existe\n",
    "if not os.path.exists(caminho_arquivo):\n",
    "    print(\"Arquivo Parquet não encontrado.\")\n",
    "    exit()\n",
    "\n",
    "# Configurações de conexão com o banco de dados PostgreSQL\n",
    "host = \"localhost\"\n",
    "porta = \"5432\"\n",
    "banco = \"airflow\"\n",
    "usuario = \"airflow\"\n",
    "senha = \"airflow\"\n",
    "\n",
    "# Conexão com o banco de dados\n",
    "conexao = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=porta,\n",
    "    database=banco,\n",
    "    user=usuario,\n",
    "    password=senha\n",
    ")\n",
    "\n",
    "# Verifica se a tabela já existe\n",
    "cursor = conexao.cursor()\n",
    "cursor.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = %s);\", (nome_tabela,))\n",
    "tabela_existe = cursor.fetchone()[0]\n",
    "\n",
    "if tabela_existe:\n",
    "    print(\"A tabela já existe. Abortando.\")\n",
    "    conexao.close()\n",
    "    exit()\n",
    "\n",
    "# Verifica se o banco de dados \"olistdb\" existe\n",
    "cursor.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = 'olistdb'\")\n",
    "banco_existe = cursor.fetchone()\n",
    "\n",
    "if not banco_existe:\n",
    "    # Criação do banco de dados \"olistdb\"\n",
    "    cursor.execute(\"CREATE DATABASE olistdb\")\n",
    "    print(\"Banco de dados 'olistdb' criado.\")\n",
    "\n",
    "# Conexão com o banco de dados \"olistdb\"\n",
    "conexao.close()\n",
    "conexao = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=porta,\n",
    "    database='olistdb',\n",
    "    user=usuario,\n",
    "    password=senha\n",
    ")\n",
    "\n",
    "# Leitura do arquivo Parquet\n",
    "dados_parquet = pd.read_parquet(caminho_arquivo)\n",
    "\n",
    "# Criação da tabela no banco de dados\n",
    "colunas_definicao = ', '.join(f\"{coluna} VARCHAR\" for coluna in dados_parquet.columns)\n",
    "cursor = conexao.cursor()\n",
    "cursor.execute(f\"CREATE TABLE {nome_tabela} ({colunas_definicao});\")\n",
    "\n",
    "# Carregamento dos dados na tabela usando COPY FROM STDIN\n",
    "buffer = io.StringIO()\n",
    "dados_parquet.to_csv(buffer, index=False, header=False, sep=\"\\t\")\n",
    "buffer.seek(0)\n",
    "cursor.copy_from(buffer, nome_tabela, sep=\"\\t\")\n",
    "\n",
    "# Confirmação da transação e fechamento da conexão\n",
    "conexao.commit()\n",
    "conexao.close()\n",
    "\n",
    "print(\"Dados carregados com sucesso!\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport psycopg2\\nimport io\\n\\n# Caminho para o arquivo Parquet\\ncaminho_arquivo = \"/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_customers_dataset.parquet\"\\nnome_tabela = \\'customers\\'\\n\\n# Verifica se o arquivo Parquet existe\\nif not os.path.exists(caminho_arquivo):\\n    print(\"Arquivo Parquet não encontrado.\")\\n    exit()\\n\\n# Configurações de conexão com o banco de dados PostgreSQL\\nhost = \"localhost\"\\nporta = \"5432\"\\nbanco = \"airflow\"\\nusuario = \"airflow\"\\nsenha = \"airflow\"\\n\\n# Conexão com o banco de dados\\nconexao = psycopg2.connect(\\n    host=host,\\n    port=porta,\\n    database=banco,\\n    user=usuario,\\n    password=senha\\n)\\n\\n# Leitura do arquivo Parquet\\ndados_parquet = pd.read_parquet(caminho_arquivo)\\n\\n# Verifica se a tabela já existe\\ncursor = conexao.cursor()\\ncursor.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = %s);\", (nome_tabela,))\\ntabela_existe = cursor.fetchone()[0]\\n\\nif tabela_existe:\\n    print(\"A tabela já existe. Abortando.\")\\n    conexao.close()\\n    exit()\\n\\n# Verifica se o banco de dados \"olistdb\" existe\\ncursor.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = \\'olistdb\\'\")\\nbanco_existe = cursor.fetchone()\\n\\nif not banco_existe:\\n    # Criação do banco de dados \"olistdb\"\\n    cursor.execute(\"CREATE DATABASE olistdb\")\\n    print(\"Banco de dados \\'olistdb\\' criado.\")\\n\\n# Conexão com o banco de dados \"olistdb\"\\nconexao.close()\\nconexao = psycopg2.connect(\\n    host=host,\\n    port=porta,\\n    database=\\'olistdb\\',\\n    user=usuario,\\n    password=senha\\n)\\n\\n# Criação da tabela no banco de dados\\ncursor = conexao.cursor()\\ncursor.execute(f\"CREATE TABLE {nome_tabela} ();\")\\n\\n# Obtenção das colunas do arquivo Parquet\\ncolunas_parquet = list(dados_parquet.columns)\\n\\n# Atualização da definição da tabela no banco de dados\\nfor coluna in colunas_parquet:\\n    cursor.execute(f\"ALTER TABLE {nome_tabela} ADD COLUMN {coluna} TEXT;\")\\n\\n# Carregamento dos dados na tabela\\nbuffer = io.StringIO()\\ndados_parquet.to_csv(buffer, index=False, header=False, sep=\"\\t\")\\nbuffer.seek(0)\\ncursor.copy_from(buffer, nome_tabela, sep=\"\\t\")\\n\\n# Confirmação da transação e fechamento da conexão\\nconexao.commit()\\nconexao.close()\\n\\nprint(\"Dados carregados com sucesso!\")\\n'"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import psycopg2\n",
    "import io\n",
    "\n",
    "# Caminho para o arquivo Parquet\n",
    "caminho_arquivo = \"/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_customers_dataset.parquet\"\n",
    "nome_tabela = 'customers'\n",
    "\n",
    "# Verifica se o arquivo Parquet existe\n",
    "if not os.path.exists(caminho_arquivo):\n",
    "    print(\"Arquivo Parquet não encontrado.\")\n",
    "    exit()\n",
    "\n",
    "# Configurações de conexão com o banco de dados PostgreSQL\n",
    "host = \"localhost\"\n",
    "porta = \"5432\"\n",
    "banco = \"airflow\"\n",
    "usuario = \"airflow\"\n",
    "senha = \"airflow\"\n",
    "\n",
    "# Conexão com o banco de dados\n",
    "conexao = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=porta,\n",
    "    database=banco,\n",
    "    user=usuario,\n",
    "    password=senha\n",
    ")\n",
    "\n",
    "# Leitura do arquivo Parquet\n",
    "dados_parquet = pd.read_parquet(caminho_arquivo)\n",
    "\n",
    "# Verifica se a tabela já existe\n",
    "cursor = conexao.cursor()\n",
    "cursor.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = %s);\", (nome_tabela,))\n",
    "tabela_existe = cursor.fetchone()[0]\n",
    "\n",
    "if tabela_existe:\n",
    "    print(\"A tabela já existe. Abortando.\")\n",
    "    conexao.close()\n",
    "    exit()\n",
    "\n",
    "# Verifica se o banco de dados \"olistdb\" existe\n",
    "cursor.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = 'olistdb'\")\n",
    "banco_existe = cursor.fetchone()\n",
    "\n",
    "if not banco_existe:\n",
    "    # Criação do banco de dados \"olistdb\"\n",
    "    cursor.execute(\"CREATE DATABASE olistdb\")\n",
    "    print(\"Banco de dados 'olistdb' criado.\")\n",
    "\n",
    "# Conexão com o banco de dados \"olistdb\"\n",
    "conexao.close()\n",
    "conexao = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=porta,\n",
    "    database='olistdb',\n",
    "    user=usuario,\n",
    "    password=senha\n",
    ")\n",
    "\n",
    "# Criação da tabela no banco de dados\n",
    "cursor = conexao.cursor()\n",
    "cursor.execute(f\"CREATE TABLE {nome_tabela} ();\")\n",
    "\n",
    "# Obtenção das colunas do arquivo Parquet\n",
    "colunas_parquet = list(dados_parquet.columns)\n",
    "\n",
    "# Atualização da definição da tabela no banco de dados\n",
    "for coluna in colunas_parquet:\n",
    "    cursor.execute(f\"ALTER TABLE {nome_tabela} ADD COLUMN {coluna} TEXT;\")\n",
    "\n",
    "# Carregamento dos dados na tabela\n",
    "buffer = io.StringIO()\n",
    "dados_parquet.to_csv(buffer, index=False, header=False, sep=\"\\t\")\n",
    "buffer.seek(0)\n",
    "cursor.copy_from(buffer, nome_tabela, sep=\"\\t\")\n",
    "\n",
    "# Confirmação da transação e fechamento da conexão\n",
    "conexao.commit()\n",
    "conexao.close()\n",
    "\n",
    "print(\"Dados carregados com sucesso!\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
