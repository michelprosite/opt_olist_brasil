{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michel Souza Santana\n",
    "## Projeto Desafio Aceleras\n",
    "## Trilha 1\n",
    "> Start: 15/05/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1 - Transformação do ER proposto em um BI, realizando o ETL usando uma ferrmenta local (Talend, Apache Hop, Nifi, Airflow, SSIS, Pentaho,…)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entenda o modelo ER: Familiarize-se com o modelo ER existente, incluindo as tabelas, relacionamentos e atributos. Isso ajudará você a mapear corretamente os dados durante a transformação.\n",
    "\n",
    "* Identifique os requisitos de BI: Compreenda as necessidades e requisitos do seu projeto de BI. Identifique as informações que você precisa extrair e apresentar no ambiente de BI.\n",
    "\n",
    "* Escolha uma ferramenta ETL: Pesquise e selecione uma ferramenta ETL adequada para sua transformação de dados. Existem várias opções disponíveis, como Pentaho Data Integration, Talend, Microsoft SQL Server Integration Services (SSIS), entre outras.\n",
    "\n",
    "* Instale a ferramenta ETL: Faça o download e instale a ferramenta ETL selecionada no seu ambiente local.\n",
    "\n",
    "* Conecte-se ao banco de dados: Configure a conexão da ferramenta ETL com o banco de dados que contém os dados do modelo ER. Forneça as credenciais de acesso necessárias para estabelecer a conexão.\n",
    "\n",
    "* Extração de dados: Utilizando a ferramenta ETL, extraia os dados do banco de dados conforme necessário para o seu projeto de BI. Isso pode envolver a seleção de tabelas específicas, filtragem de dados ou até mesmo a união de várias tabelas para obter as informações desejadas.\n",
    "\n",
    "* Limpeza e transformação de dados: Aplique as transformações necessárias nos dados extraídos para adequá-los às necessidades do ambiente de BI. Isso pode incluir a remoção de dados duplicados, preenchimento de valores ausentes, conversão de formatos de data, entre outros processos de limpeza e transformação.\n",
    "\n",
    "* Mapeamento para o modelo dimensional: Se você estiver construindo um data warehouse ou uma solução de BI baseada em modelo dimensional, mapeie os dados extraídos para as dimensões e fatos do seu modelo dimensional. Isso envolve a definição de hierarquias, chaves e relacionamentos.\n",
    "\n",
    "* Desenvolva fluxos de trabalho ETL: Utilizando a ferramenta ETL, crie fluxos de trabalho que automatizem a transformação de dados. Isso pode envolver a criação de transformações, tarefas agendadas e outras operações para garantir a integridade e atualização dos dados.\n",
    "\n",
    "* Carregamento dos dados: Carregue os dados transformados no ambiente de BI, que pode incluir um data warehouse, um banco de dados ou outra solução de armazenamento de dados.\n",
    "\n",
    "* Desenvolva visualizações e relatórios: Com os dados carregados no ambiente de BI, desenvolva visualizações e relatórios interativos para fornecer insights acionáveis aos usuários finais. Isso pode ser feito usando ferramentas de visualização de dados como Tableau, Power BI, QlikView, entre outras.\n",
    "\n",
    "* Teste e valide: Realize testes para garantir a precisão e a integridade dos dados transformados. Verifique se as visualiza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os processos de engenharia de dados (data engineering) são etapas sequenciais usadas para transformar e gerenciar dados em um pipeline de processamento. Esses processos são realizados em várias etapas, começando pelos dados brutos e terminando com os dados confiáveis e prontos para análise. Aqui está uma explicação de cada processo em ordem de execução:\n",
    "\n",
    "* Processo de Transient Data (Dados Transitórios):\n",
    "Os dados transitórios são dados temporários ou de curta duração usados para processamento e transformação intermediários. Esses dados podem ser gerados durante o processamento em tempo real, pipelines de processamento de dados em lote ou durante a integração de diferentes fontes de dados. Os dados transitórios são normalmente armazenados em sistemas de processamento de fluxo de dados ou bancos de dados temporários, e seu objetivo é suportar operações intermediárias antes que os dados sejam refinados e confiáveis.\n",
    "\n",
    "* Processo de Raw Data (Dados Brutos):\n",
    "Nesta fase inicial, os dados brutos são coletados de várias fontes, como bancos de dados, arquivos CSV, APIs, feeds de streaming, entre outros. Esses dados podem estar em formatos diversos e podem conter ruído, inconsistências e falta de estrutura. O objetivo do processo de Dados Brutos é extrair e armazenar esses dados em seu estado bruto original.\n",
    "\n",
    "* Processo de Trusted Data (Dados Confiáveis):\n",
    "Nesta fase final, os dados são considerados confiáveis, prontos para uso e adequados para análise e tomada de decisão. Isso envolve garantir a qualidade dos dados, validar a precisão, consistência e integridade dos dados, aplicar regras de negócios e conformidade, bem como implementar mecanismos de controle de qualidade. O objetivo é fornecer dados confiáveis que possam ser usados com confiança para análise, geração de relatórios e outras tarefas de negócios.\n",
    "\n",
    "* Processo de Refined Data (Dados Refinados):\n",
    "Nesta fase, os dados processados são refinados ainda mais para atender a requisitos específicos de negócios e análise. Isso pode incluir agregação de dados, cálculos adicionais, enriquecimento com informações adicionais, como dados geográficos ou dados de terceiros, e transformações personalizadas para atender às necessidades específicas dos usuários finais. O objetivo é fornecer dados refinados e mais valiosos para análise e tomada de decisão.\n",
    "\n",
    "* Processo de Data Engineering (Engenharia de Dados):\n",
    "Nesta etapa, os dados brutos são processados e transformados em um formato adequado para análise e uso posterior. Isso envolve atividades como limpeza de dados, padronização de formatos, filtragem de dados inválidos ou incompletos, remoção de duplicatas e criação de estruturas de dados otimizadas para consultas e processamento eficiente. O objetivo é obter dados estruturados e refinados que possam ser usados em análises e outros processos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A transformação de um modelo Entidade-Relacionamento (ER) em uma solução de Business Intelligence (BI) envolve várias etapas. Aqui está um passo a passo geral que pode ajudá-lo nesse processo:\n",
    "\n",
    "* Entendimento dos requisitos do negócio: Comece entendendo os requisitos do negócio e as necessidades de análise. Identifique quais informações são importantes, quais métricas e indicadores devem ser acompanhados e quais perguntas de negócio devem ser respondidas.\n",
    "\n",
    "* Modelagem do data warehouse: Com base nos requisitos do negócio, projete um modelo de data warehouse que atenda às necessidades de análise. O modelo de data warehouse é uma estrutura otimizada para consultas e análises de dados. Você pode usar uma abordagem dimensional, como o modelo estrela ou floco de neve, para organizar os dados em fatos e dimensões.\n",
    "\n",
    "* Extração, Transformação e Carga (ETL): Nesta etapa, extraia os dados relevantes do modelo ER proposto e transforme-os para se adequar ao modelo de data warehouse. Isso envolve limpeza de dados, padronização, agregação, cálculos e outras transformações necessárias. Em seguida, carregue os dados transformados no data warehouse.\n",
    "\n",
    "* Projeto e desenvolvimento de cubos OLAP: Cubos OLAP são estruturas multidimensionais que permitem uma análise rápida e flexível dos dados. Projete e desenvolva cubos OLAP com base nos requisitos do negócio e no modelo de data warehouse. Defina hierarquias, dimensões, medidas e agregações adequadas aos requisitos de análise.\n",
    "\n",
    "* Desenvolvimento de relatórios e painéis: Crie relatórios e painéis interativos que apresentem as informações relevantes de forma clara e visualmente atraente. Use ferramentas de BI, como o Pentaho, para projetar e desenvolver relatórios personalizados, gráficos e visualizações que atendam às necessidades dos usuários finais.\n",
    "\n",
    "* Implantação e manutenção: Implante a solução de BI em um ambiente de produção e teste-a para garantir que esteja funcionando corretamente. Monitore e mantenha regularmente o sistema de BI, atualizando os dados, ajustando as transformações ETL e fazendo melhorias contínuas conforme necessário.\n",
    "\n",
    "* Treinamento e adoção: Treine os usuários finais para que possam aproveitar ao máximo a solução de BI. Forneça suporte contínuo e promova a adoção da solução, demonstrando os benefícios e as capacidades de análise.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para elaborar uma abordagem dimensional, como o modelo estrela, para os dados do conjunto de dados \"Brazilian E-Commerce\" disponível no Kaggle (https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce), podemos considerar os seguintes elementos principais:\n",
    "\n",
    "* Tabela de Fatos (Fact Table):\n",
    "A tabela de fatos é o centro do modelo dimensional e contém as métricas quantitativas ou eventos que desejamos analisar. No caso do conjunto de dados \"Brazilian E-Commerce\", uma possível tabela de fatos pode ser a \"Orders\" (Pedidos), que registra informações sobre os pedidos realizados. Alguns atributos possíveis para essa tabela podem ser: ID do pedido, data do pedido, valor total do pedido, ID do cliente, ID do produto, ID do vendedor, etc.\n",
    "\n",
    "* Tabelas de Dimensões (Dimension Tables):\n",
    "As tabelas de dimensões fornecem o contexto para as métricas na tabela de fatos e contêm atributos descritivos que ajudam a filtrar e agrupar os dados. No contexto do conjunto de dados \"Brazilian E-Commerce\", podemos ter as seguintes tabelas de dimensões:\n",
    "\n",
    "* Tabela de Dimensão \"Clientes\": contendo informações sobre os clientes, como ID do cliente, nome, endereço, cidade, estado, etc.\n",
    "* Tabela de Dimensão \"Produtos\": contendo informações sobre os produtos, como ID do produto, nome do produto, categoria, preço, etc.\n",
    "* Tabela de Dimensão \"Vendedores\": contendo informações sobre os vendedores, como ID do vendedor, nome do vendedor, região, classificação, etc.\n",
    "* Tabela de Dimensão \"Datas\": contendo informações sobre as datas, como data, dia da semana, mês, trimestre, ano, feriados, etc.\n",
    "Cada tabela de dimensão teria uma chave primária única que se relaciona com a tabela de fatos através de chaves estrangeiras.\n",
    "\n",
    "* Tabelas de Dimensões Adicionais (Optional Dimension Tables):\n",
    "Dependendo dos requisitos de análise específicos do conjunto de dados \"Brazilian E-Commerce\", outras tabelas de dimensões adicionais podem ser incluídas para fornecer mais contexto aos dados. Por exemplo, uma tabela de dimensão \"Categorias\" poderia conter informações sobre as categorias dos produtos vendidos.\n",
    "\n",
    "* Tabelas de Bridge (Bridge Tables):\n",
    "Em alguns casos, pode ser necessário representar relacionamentos muitos-para-muitos entre dimensões. Nesses casos, tabelas de bridge podem ser usadas para criar associações entre várias dimensões. Por exemplo, uma tabela de bridge \"Pedidos_Produtos\" pode ser usada para representar quais produtos foram incluídos em cada pedido."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando o pyarrow para conversão dos arquivos csv em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Processo de Transient Data (Dados Transitórios):\n",
    "Os dados transitórios são dados temporários ou de curta duração usados para processamento e transformação intermediários. Esses dados podem ser gerados durante o processamento em tempo real, pipelines de processamento de dados em lote ou durante a integração de diferentes fontes de dados. Os dados transitórios são normalmente armazenados em sistemas de processamento de fluxo de dados ou bancos de dados temporários, e seu objetivo é suportar operações intermediárias antes que os dados sejam refinados e confiáveis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os diretórios estruturais do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = !pwd\n",
    "f = pd.read_csv(str(path_folder[0]) + '/folders.csv')\n",
    "lista_folders = f['Folders'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diretório engineer já existe.\n",
      "O diretório raw já existe.\n",
      "O diretório refined já existe.\n",
      "O diretório transient já existe.\n",
      "O diretório trusted já existe.\n"
     ]
    }
   ],
   "source": [
    "for i in lista_folders:\n",
    "    diretorio = str(path_folder[0]) \n",
    "\n",
    "    if not os.path.exists(diretorio + '/' + i):\n",
    "        os.makedirs(diretorio + '/' + i)\n",
    "        print(f\"Diretório {i} criado com sucesso!\")\n",
    "    else:\n",
    "        print(f\"O diretório {i} já existe.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificar as fontes de dados: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "\n",
    "A URL em questão se refere a um conjunto de dados disponibilizado no site Kaggle, que contém informações sobre o comércio eletrônico no Brasil. O conjunto de dados é intitulado \"Brazilian E-Commerce Public Dataset by Olist\" e foi criado pela empresa Olist, que é uma plataforma de vendas on-line que conecta pequenos e médios varejistas a marketplaces.\n",
    "\n",
    "O conjunto de dados contém informações de mais de 100 mil pedidos de clientes, com dados que incluem informações do produto, preços, prazos de entrega, avaliações de clientes e informações sobre o vendedor. Além disso, o conjunto de dados contém informações sobre geolocalização dos clientes, categoria de produtos e informações sobre a própria loja virtual.\n",
    "\n",
    "Este conjunto de dados pode ser extremamente útil para análises sobre comércio eletrônico no Brasil, permitindo a análise de tendências de consumo, comportamento dos clientes, performance de vendas e muito mais. A disponibilização de dados desse tipo é importante para o desenvolvimento de modelos de negócios mais eficientes e para a tomada de decisões mais informadas no setor de e-commerce brasileiro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copia os dados do site do kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando o Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixando os arquivos csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixar & diszipar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Arquivos já existem.\n"
     ]
    }
   ],
   "source": [
    "pasta = str(path_folder[0]) + '/transient'\n",
    "cont = 0\n",
    "verifica_arquivos = [os.path.splitext(arquivo)[0] for arquivo in os.listdir(pasta) if os.path.isfile(os.path.join(pasta, arquivo))]\n",
    "for i in verifica_arquivos: cont += 1\n",
    "print(cont)\n",
    "\n",
    "if cont <= 0:  \n",
    "    r = !pwd\n",
    "    PATH_FOLDER = r[0] + '/transient'\n",
    "    os.environ['PATH_FOLDER'] = PATH_FOLDER\n",
    "\n",
    "    !cd $PATH_FOLDER && kaggle datasets download -d olistbr/brazilian-ecommerce\n",
    "    !cd $PATH_FOLDER && unzip brazilian-ecommerce.zip\n",
    "    !cd $PATH_FOLDER && rm -r brazilian-ecommerce.zip\n",
    "    print('Arquivo carregado e descompactado.')\n",
    "else:\n",
    "    print('Arquivos já existem.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o arquivo 'controller.csv' na pasta enginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta = str(path_folder[0]) + '/transient'  # Substitua pelo caminho da pasta desejada\n",
    "\n",
    "# Capturar nomes dos arquivos e remover a extensão\n",
    "arquivos = [os.path.splitext(arquivo)[0] for arquivo in os.listdir(pasta) if os.path.isfile(os.path.join(pasta, arquivo))]\n",
    "# Eliminar as palavras indesejadas\n",
    "palavras_indesejadas = ['_dataset.csv', 'olist_', 'order_', '_dataset', 'product_', '_name', '_translation']\n",
    "nomes_limpos = [arquivo for arquivo in arquivos]\n",
    "for palavra in palavras_indesejadas:\n",
    "   nomes_limpos = [nome.replace(palavra, '') for nome in nomes_limpos]\n",
    "\n",
    "# Caminho do arquivo CSV a ser salvo\n",
    "caminho_arquivo = str(path_folder[0]) + '/' + 'controller.csv'\n",
    "\n",
    "# Definir os dados a serem escritos no arquivo CSV\n",
    "dados = [\n",
    "    [\"path_transient\", \"path_raw\", \"path_trusted\", \"table_transient\", \"table_raw\", \"table_trusted\", \"table_name\"]\n",
    "] + [\n",
    "    [str(path_folder[0]) + '/' + \"transient\", str(path_folder[0]) + '/' + \"raw\", str(path_folder[0]) + '/' + \"trusted\", nome, nome, nome, nome_limp]\n",
    "    for nome, nome_limp in zip(arquivos, nomes_limpos)\n",
    "]\n",
    "\n",
    "# Salvar o arquivo CSV\n",
    "pd.DataFrame(dados).to_csv(caminho_arquivo, index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando tabela com os path necessários para manipulação dos arquivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando a formação dos path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_transient</th>\n",
       "      <th>path_raw</th>\n",
       "      <th>path_trusted</th>\n",
       "      <th>table_transient</th>\n",
       "      <th>table_raw</th>\n",
       "      <th>table_trusted</th>\n",
       "      <th>table_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>payments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>sellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trans...</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/raw</td>\n",
       "      <td>/home/michel/Documentos/opt/kaggle_olist/trusted</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>geolocation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      path_transient   \n",
       "0  /home/michel/Documentos/opt/kaggle_olist/trans...  \\\n",
       "1  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "2  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "3  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "4  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "5  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "6  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "7  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "8  /home/michel/Documentos/opt/kaggle_olist/trans...   \n",
       "\n",
       "                                       path_raw   \n",
       "0  /home/michel/Documentos/opt/kaggle_olist/raw  \\\n",
       "1  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "2  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "3  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "4  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "5  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "6  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "7  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "8  /home/michel/Documentos/opt/kaggle_olist/raw   \n",
       "\n",
       "                                       path_trusted   \n",
       "0  /home/michel/Documentos/opt/kaggle_olist/trusted  \\\n",
       "1  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "2  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "3  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "4  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "5  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "6  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "7  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "8  /home/michel/Documentos/opt/kaggle_olist/trusted   \n",
       "\n",
       "                     table_transient                          table_raw   \n",
       "0          olist_order_items_dataset          olist_order_items_dataset  \\\n",
       "1       olist_order_payments_dataset       olist_order_payments_dataset   \n",
       "2  product_category_name_translation  product_category_name_translation   \n",
       "3             olist_products_dataset             olist_products_dataset   \n",
       "4            olist_customers_dataset            olist_customers_dataset   \n",
       "5              olist_sellers_dataset              olist_sellers_dataset   \n",
       "6        olist_order_reviews_dataset        olist_order_reviews_dataset   \n",
       "7               olist_orders_dataset               olist_orders_dataset   \n",
       "8          olist_geolocation_dataset          olist_geolocation_dataset   \n",
       "\n",
       "                       table_trusted   table_name  \n",
       "0          olist_order_items_dataset        items  \n",
       "1       olist_order_payments_dataset     payments  \n",
       "2  product_category_name_translation     category  \n",
       "3             olist_products_dataset     products  \n",
       "4            olist_customers_dataset    customers  \n",
       "5              olist_sellers_dataset      sellers  \n",
       "6        olist_order_reviews_dataset      reviews  \n",
       "7               olist_orders_dataset       orders  \n",
       "8          olist_geolocation_dataset  geolocation  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = pd.read_csv(str(path_folder[0]) + '/' + 'controller.csv')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michel/Documentos/opt/kaggle_olist/transient/olist_order_items_dataset.csv\n",
      "/home/michel/Documentos/opt/kaggle_olist/raw/olist_order_items_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "print(path['path_transient'][0] + '/' + path['table_transient'][0] + '.csv')\n",
    "print(path['path_raw'][0] + '/' + path['table_raw'][0] + '.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Processo de Raw Data (Dados Brutos):\n",
    "Nesta fase inicial, os dados brutos são coletados de várias fontes, como bancos de dados, arquivos CSV, APIs, feeds de streaming, entre outros. Esses dados podem estar em formatos diversos e podem conter ruído, inconsistências e falta de estrutura. O objetivo do processo de Dados Brutos é extrair e armazenar esses dados em seu estado bruto original."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertendo or arquivos CSV em pela primeira vez .parquet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminando dados duplicados com base nas coludas id para evitar conflitos na transposição dos dados para o DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O arquivo Parquet olist_order_items_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_order_items_dataset.parquet\n",
      "O arquivo Parquet olist_order_payments_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_order_payments_dataset.parquet\n",
      "O arquivo Parquet product_category_name_translation.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/product_category_name_translation.parquet\n",
      "O arquivo Parquet olist_products_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_products_dataset.parquet\n",
      "O arquivo Parquet olist_customers_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_customers_dataset.parquet\n",
      "O arquivo Parquet olist_sellers_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_sellers_dataset.parquet\n",
      "O arquivo Parquet olist_order_reviews_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_order_reviews_dataset.parquet\n",
      "O arquivo Parquet olist_orders_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_orders_dataset.parquet\n",
      "O arquivo Parquet olist_geolocation_dataset.parquet já existe: /home/michel/Documentos/opt/kaggle_olist/raw/olist_geolocation_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(len(path['path_transient'])):\n",
    "\n",
    "    diretorio = str(path['path_raw'][i])  # Diretório onde o arquivo Parquet será salvo\n",
    "    nome_arquivo = str(path['table_raw'][i]) + '.parquet'  # Nome do arquivo Parquet\n",
    "\n",
    "    caminho_arquivo = os.path.join(diretorio, nome_arquivo)  # Caminho completo do arquivo Parquet\n",
    "\n",
    "    # Verifica se o arquivo Parquet já existe\n",
    "    if not os.path.exists(caminho_arquivo):\n",
    "        # O arquivo não existe, então você pode criar\n",
    "        df = pd.read_csv(path['path_transient'][i] + '/' + path['table_transient'][i] + '.csv')\n",
    "        df = df.drop_duplicates(subset=df.columns[0])\n",
    "        df.to_parquet(str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet')\n",
    "        print(f\"Arquivo Parquet {nome_arquivo} criado: {caminho_arquivo}\")\n",
    "    else:\n",
    "        # O arquivo já existe\n",
    "        print(f\"O arquivo Parquet {nome_arquivo} já existe: {caminho_arquivo}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Em uma segunda vez, tendo os arquivos parquet sido criados sempre será realizada uma verificação da fonte com a raw, se houver alterações, estas serão acrescentadas aos seus respectivos arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "(\"Expected bytes, got a 'int' object\", 'Conversion failed for column Unnamed: 0.1 with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[219], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m csv_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/michel/Documentos/opt/kaggle_olist/transient/olist_customers_dataset.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m parquet_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/michel/Documentos/opt/kaggle_olist/raw/olist_customers_dataset.parquet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 34\u001b[0m comparar_csv_parquet(csv_path, parquet_path)\n",
      "Cell \u001b[0;32mIn[219], line 24\u001b[0m, in \u001b[0;36mcomparar_csv_parquet\u001b[0;34m(csv_path, parquet_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m     df_parquet \u001b[39m=\u001b[39m df_parquet\u001b[39m.\u001b[39mdrop_duplicates(subset\u001b[39m=\u001b[39mdf_parquet\u001b[39m.\u001b[39mcolumns[\u001b[39m0\u001b[39m])\n\u001b[1;32m     23\u001b[0m     \u001b[39m# Salva o DataFrame atualizado no arquivo Parquet\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     df_parquet\u001b[39m.\u001b[39;49mto_parquet(parquet_path, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     25\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mForam encontradas diferenças na primeira coluna do CSV em relação ao Parquet. O arquivo Parquet foi atualizado.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:2889\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2885\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2886\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2889\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   2890\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2891\u001b[0m     path,\n\u001b[1;32m   2892\u001b[0m     engine,\n\u001b[1;32m   2893\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2894\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2895\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m   2896\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2897\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2898\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:411\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    409\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m--> 411\u001b[0m impl\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m    412\u001b[0m     df,\n\u001b[1;32m    413\u001b[0m     path_or_buf,\n\u001b[1;32m    414\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    415\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    416\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m    417\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    418\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    419\u001b[0m )\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:159\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     from_pandas_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpreserve_index\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m index\n\u001b[0;32m--> 159\u001b[0m table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_pandas(df, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfrom_pandas_kwargs)\n\u001b[1;32m    161\u001b[0m path_or_handle, handles, kwargs[\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    162\u001b[0m     path,\n\u001b[1;32m    163\u001b[0m     kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m     is_dir\u001b[39m=\u001b[39mpartition_cols \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    167\u001b[0m )\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m     \u001b[39misinstance\u001b[39m(path_or_handle, io\u001b[39m.\u001b[39mBufferedWriter)\n\u001b[1;32m    170\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(path_or_handle, \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_handle\u001b[39m.\u001b[39mname, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m))\n\u001b[1;32m    172\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/table.pxi:3681\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/pandas_compat.py:624\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[39mfor\u001b[39;00m i, maybe_fut \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(arrays):\n\u001b[1;32m    623\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_fut, futures\u001b[39m.\u001b[39mFuture):\n\u001b[0;32m--> 624\u001b[0m             arrays[i] \u001b[39m=\u001b[39m maybe_fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    626\u001b[0m types \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mtype \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/pandas_compat.py:598\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mexcept\u001b[39;00m (pa\u001b[39m.\u001b[39mArrowInvalid,\n\u001b[1;32m    594\u001b[0m         pa\u001b[39m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    595\u001b[0m         pa\u001b[39m.\u001b[39mArrowTypeError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    596\u001b[0m     e\u001b[39m.\u001b[39margs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mConversion failed for column \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    597\u001b[0m                \u001b[39m.\u001b[39mformat(col\u001b[39m.\u001b[39mname, col\u001b[39m.\u001b[39mdtype),)\n\u001b[0;32m--> 598\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m field_nullable \u001b[39mand\u001b[39;00m result\u001b[39m.\u001b[39mnull_count \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    600\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mField \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m was non-nullable but pandas column \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    601\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mhad \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m null values\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mstr\u001b[39m(field),\n\u001b[1;32m    602\u001b[0m                                                  result\u001b[39m.\u001b[39mnull_count))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/pandas_compat.py:592\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    589\u001b[0m     type_ \u001b[39m=\u001b[39m field\u001b[39m.\u001b[39mtype\n\u001b[1;32m    591\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m     result \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49marray(col, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49mtype_, from_pandas\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, safe\u001b[39m=\u001b[39;49msafe)\n\u001b[1;32m    593\u001b[0m \u001b[39mexcept\u001b[39;00m (pa\u001b[39m.\u001b[39mArrowInvalid,\n\u001b[1;32m    594\u001b[0m         pa\u001b[39m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    595\u001b[0m         pa\u001b[39m.\u001b[39mArrowTypeError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    596\u001b[0m     e\u001b[39m.\u001b[39margs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mConversion failed for column \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    597\u001b[0m                \u001b[39m.\u001b[39mformat(col\u001b[39m.\u001b[39mname, col\u001b[39m.\u001b[39mdtype),)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/array.pxi:323\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/array.pxi:83\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/error.pxi:123\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowTypeError\u001b[0m: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Unnamed: 0.1 with type object')"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def comparar_csv_parquet(csv_path, parquet_path):\n",
    "    # Lê o arquivo CSV e o transforma em um DataFrame\n",
    "    df_csv = pd.read_csv(csv_path)\n",
    "    df_csv = df_csv.astype(str)\n",
    "\n",
    "    # Lê o arquivo Parquet e o transforma em um DataFrame\n",
    "    df_parquet = pd.read_parquet(parquet_path)\n",
    "\n",
    "    # Verifica se há diferenças na primeira coluna\n",
    "    if not df_csv.iloc[:, 0].equals(df_parquet.iloc[:, 0]):\n",
    "        # Encontra os dados diferentes na primeira coluna do CSV\n",
    "        dados_diferentes = df_csv[~df_csv.iloc[:, 0].isin(df_parquet.iloc[:, 0])]\n",
    "\n",
    "        if not dados_diferentes.empty:\n",
    "            # Concatena os dados diferentes no topo do DataFrame Parquet\n",
    "            df_parquet = pd.concat([dados_diferentes, df_parquet], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "            # Elimina as duplicadas se ainda houver\n",
    "            df_parquet = df_parquet.drop_duplicates(subset=df_parquet.columns[0])\n",
    "\n",
    "            # Salva o DataFrame atualizado no arquivo Parquet\n",
    "            df_parquet.to_parquet(parquet_path, index=False)\n",
    "            print(\"Foram encontradas diferenças na primeira coluna do CSV em relação ao Parquet. O arquivo Parquet foi atualizado.\")\n",
    "        else:\n",
    "            print(\"Não foram encontradas diferenças na primeira coluna do CSV em relação ao Parquet.\")\n",
    "    else:\n",
    "        print(\"Não foram encontradas diferenças na primeira coluna do CSV em relação ao Parquet.\")\n",
    "\n",
    "# Exemplo de uso\n",
    "csv_path = \"/home/michel/Documentos/opt/kaggle_olist/transient/olist_customers_dataset.csv\"\n",
    "parquet_path = \"/home/michel/Documentos/opt/kaggle_olist/raw/olist_customers_dataset.parquet\"\n",
    "comparar_csv_parquet(csv_path, parquet_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Processo de Trusted Data (Dados Confiáveis):\n",
    "Nesta fase final, os dados são considerados confiáveis, prontos para uso e adequados para análise e tomada de decisão. Isso envolve garantir a qualidade dos dados, validar a precisão, consistência e integridade dos dados, aplicar regras de negócios e conformidade, bem como implementar mecanismos de controle de qualidade. O objetivo é fornecer dados confiáveis que possam ser usados com confiança para análise, geração de relatórios e outras tarefas de negócios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo os arquivos e criando o banco de dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testando os caminhos e nomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nome da tabela: items\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_order_items_dataset.parquet\n",
      "\n",
      "Nome da tabela: payments\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_order_payments_dataset.parquet\n",
      "\n",
      "Nome da tabela: category\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedproduct_category_name_translation.parquet\n",
      "\n",
      "Nome da tabela: products\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_products_dataset.parquet\n",
      "\n",
      "Nome da tabela: customers\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_customers_dataset.parquet\n",
      "\n",
      "Nome da tabela: sellers\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_sellers_dataset.parquet\n",
      "\n",
      "Nome da tabela: reviews\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_order_reviews_dataset.parquet\n",
      "\n",
      "Nome da tabela: orders\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_orders_dataset.parquet\n",
      "\n",
      "Nome da tabela: geolocation\n",
      "caminho dos dados: /home/michel/Documentos/opt/kaggle_olist/trustedolist_geolocation_dataset.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(path['table_name'])):\n",
    "    print(f\"Nome da tabela: {path['table_name'][i]}\")\n",
    "    print(f\"caminho dos dados: {path['path_trusted'][i] + path['table_trusted'][i] + '.parquet'}\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "2055: Cursor is not connected",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReferenceError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mysql/connector/cursor_cext.py:299\u001b[0m, in \u001b[0;36mCMySQLCursor.execute\u001b[0;34m(self, operation, params, multi)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cnx \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cnx\u001b[39m.\u001b[39mis_closed():\n\u001b[1;32m    300\u001b[0m         \u001b[39mraise\u001b[39;00m ProgrammingError\n",
      "\u001b[0;31mReferenceError\u001b[0m: weakly-referenced object no longer exists",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[208], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m# Verificar se as tabelas existem, se não, criar as tabelas no banco de dados\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m table_name \u001b[39min\u001b[39;00m table_names:\n\u001b[0;32m---> 40\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSHOW TABLES LIKE \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mtable_name\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     41\u001b[0m     result \u001b[39m=\u001b[39m cursor\u001b[39m.\u001b[39mfetchone()\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result:\n\u001b[1;32m     43\u001b[0m         \u001b[39m# Obter as colunas e seus tipos a partir do arquivo Parquet\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mysql/connector/cursor_cext.py:302\u001b[0m, in \u001b[0;36mCMySQLCursor.execute\u001b[0;34m(self, operation, params, multi)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[39mraise\u001b[39;00m ProgrammingError\n\u001b[1;32m    301\u001b[0m \u001b[39mexcept\u001b[39;00m (ProgrammingError, \u001b[39mReferenceError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 302\u001b[0m     \u001b[39mraise\u001b[39;00m ProgrammingError(\u001b[39m\"\u001b[39m\u001b[39mCursor is not connected\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2055\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cnx\u001b[39m.\u001b[39mhandle_unread_result()\n\u001b[1;32m    305\u001b[0m stmt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mProgrammingError\u001b[0m: 2055: Cursor is not connected"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Configurações de conexão com o banco de dados\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = '*Mss140920@'\n",
    "\n",
    "# Conectando ao servidor MySQL\n",
    "connection = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password\n",
    ")\n",
    "\n",
    "# Criando um cursor para executar comandos SQL\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Verificar se o banco 'olistdb' existe, se não, criá-lo\n",
    "cursor.execute(\"SHOW DATABASES LIKE 'olistdb'\")\n",
    "result = cursor.fetchone()\n",
    "if not result:\n",
    "    cursor.execute(\"CREATE DATABASE olistdb\")\n",
    "\n",
    "# Conectar ao banco de dados 'olistdb'\n",
    "connection = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    database='olistdb'\n",
    ")\n",
    "\n",
    "for i in range(len(path['table_name'])):\n",
    "    # Capturar os nomes das futuras tabelas em df['table_name']\n",
    "    df = pd.read_parquet(path['path_trusted'][i] + '/' + path['table_trusted'][i] + '.parquet')\n",
    "    table_names = path['table_name'][i]\n",
    "\n",
    "    # Verificar se as tabelas existem, se não, criar as tabelas no banco de dados\n",
    "    for table_name in table_names:\n",
    "        cursor.execute(f\"SHOW TABLES LIKE '{table_name}'\")\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            # Obter as colunas e seus tipos a partir do arquivo Parquet\n",
    "            table_data = path[path['table_name'] == table_name].head(1)\n",
    "            columns = table_data.columns\n",
    "            column_types = table_data.dtypes\n",
    "\n",
    "            # Criar a tabela com as colunas e tipos correspondentes\n",
    "            create_table_query = f\"CREATE TABLE {table_name} (\"\n",
    "            for col, col_type in zip(columns, column_types):\n",
    "                if col_type == 'int64':\n",
    "                    col_type = 'INT'\n",
    "                elif col_type == 'float64':\n",
    "                    col_type = 'FLOAT'\n",
    "                elif col_type == 'bool':\n",
    "                    col_type = 'BOOLEAN'\n",
    "                else:\n",
    "                    col_type = 'VARCHAR(255)'  # Tipo padrão se não for numérico\n",
    "\n",
    "                create_table_query += f\"{col} {col_type}, \"\n",
    "\n",
    "            create_table_query = create_table_query.rstrip(', ')  # Remover a última vírgula\n",
    "            create_table_query += \")\"\n",
    "            cursor.execute(create_table_query)\n",
    "\n",
    "# Fechar o cursor\n",
    "cursor.close()\n",
    "\n",
    "# Fechar a conexão com o banco de dados\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados da tabela items carregados com sucesso!\n",
      "Dados da tabela payments carregados com sucesso!\n",
      "Dados da tabela category carregados com sucesso!\n",
      "Dados da tabela products carregados com sucesso!\n",
      "Dados da tabela customers carregados com sucesso!\n",
      "Dados da tabela sellers carregados com sucesso!\n",
      "Dados da tabela reviews carregados com sucesso!\n",
      "Dados da tabela orders carregados com sucesso!\n",
      "Dados da tabela geolocation carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import io\n",
    "\n",
    "for i in range(len(path['table_name'])):\n",
    "\n",
    "    # Caminho para o arquivo Parquet\n",
    "    caminho_arquivo = path['path_trusted'][i] + '/parquet/' + path['table_trusted'][i] + '.parquet'\n",
    "    nome_tabela = path['table_name'][i]\n",
    "\n",
    "    # Verifica se o arquivo Parquet existe\n",
    "    if not os.path.exists(caminho_arquivo):\n",
    "        print(\"Arquivo Parquet não encontrado.\")\n",
    "        exit()\n",
    "\n",
    "    # Configurações de conexão com o banco de dados PostgreSQL\n",
    "    host = \"localhost\"\n",
    "    porta = \"5432\"\n",
    "    banco = \"airflow\"\n",
    "    usuario = \"airflow\"\n",
    "    senha = \"airflow\"\n",
    "\n",
    "    # Conexão com o banco de dados\n",
    "    conexao = psycopg2.connect(\n",
    "        host=host,\n",
    "        port=porta,\n",
    "        database=banco,\n",
    "        user=usuario,\n",
    "        password=senha\n",
    "    )\n",
    "\n",
    "    # Verifica se o banco de dados \"olistdb\" existe\n",
    "    cursor = conexao.cursor()\n",
    "    cursor.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = 'olistdb'\")\n",
    "    banco_existe = cursor.fetchone()\n",
    "\n",
    "    if not banco_existe:\n",
    "        # Criação do banco de dados \"olistdb\"\n",
    "        cursor.execute(\"CREATE DATABASE olistdb\")\n",
    "        print(\"Banco de dados 'olistdb' criado.\")\n",
    "\n",
    "    # Conexão com o banco de dados \"olistdb\"\n",
    "    conexao.close()\n",
    "    conexao = psycopg2.connect(\n",
    "        host=host,\n",
    "        port=porta,\n",
    "        database='olistdb',\n",
    "        user=usuario,\n",
    "        password=senha\n",
    "    )\n",
    "\n",
    "    # Criação da tabela no banco de dados\n",
    "    cursor = conexao.cursor()\n",
    "    cursor.execute(\"CREATE TABLE IF NOT EXISTS {0} ();\".format(nome_tabela))\n",
    "\n",
    "    # Leitura do arquivo Parquet\n",
    "    dados_parquet = pd.read_parquet(caminho_arquivo)\n",
    "\n",
    "    # Atualização da definição da tabela no banco de dados\n",
    "    for coluna in dados_parquet.columns:\n",
    "        cursor.execute(\"ALTER TABLE {0} ADD COLUMN IF NOT EXISTS {1} VARCHAR;\".format(nome_tabela, coluna))\n",
    "\n",
    "    try:\n",
    "    # Carregamento dos dados na tabela usando COPY FROM STDIN\n",
    "        buffer = io.StringIO()\n",
    "        dados_parquet = dados_parquet.replace(\"\\r\", \"\\n\")\n",
    "        dados_parquet.to_csv(buffer, index=False, header=False, sep=\"\\t\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "        buffer.seek(0)\n",
    "        cursor.copy_from(buffer, nome_tabela, sep=\"\\t\")\n",
    "    except psycopg2.errors.BadCopyFileFormat as e:\n",
    "        # Lidar com o erro ou simplesmente ignorá-lo\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Confirmação da transação e fechamento da conexão\n",
    "    conexao.commit()\n",
    "    conexao.close()\n",
    "\n",
    "    print(f\"Dados da tabela {path['table_name'][i]} carregados com sucesso!\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MERGE INTO tabela_original t\\nUSING (\\n    SELECT DISTINCT coluna1, coluna2, coluna3, ...\\n    FROM tabela_original\\n) s ON (t.coluna1 = s.coluna1)\\nWHEN MATCHED THEN\\n    DELETE;\\n'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"MERGE INTO tabela_original t\n",
    "USING (\n",
    "    SELECT DISTINCT coluna1, coluna2, coluna3, ...\n",
    "    FROM tabela_original\n",
    ") s ON (t.coluna1 = s.coluna1)\n",
    "WHEN MATCHED THEN\n",
    "    DELETE;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport psycopg2\\nimport os\\nimport io\\n\\n# Caminho para o arquivo Parquet\\ncaminho_arquivo = \"/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_customers_dataset.parquet\"\\nnome_tabela = \\'customers\\'\\n\\n# Verifica se o arquivo Parquet existe\\nif not os.path.exists(caminho_arquivo):\\n    print(\"Arquivo Parquet não encontrado.\")\\n    exit()\\n\\n# Configurações de conexão com o banco de dados PostgreSQL\\nhost = \"localhost\"\\nporta = \"5432\"\\nbanco = \"airflow\"\\nusuario = \"airflow\"\\nsenha = \"airflow\"\\n\\n# Conexão com o banco de dados\\nconexao = psycopg2.connect(\\n    host=host,\\n    port=porta,\\n    database=banco,\\n    user=usuario,\\n    password=senha\\n)\\n\\n# Verifica se a tabela já existe\\ncursor = conexao.cursor()\\ncursor.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = %s);\", (nome_tabela,))\\ntabela_existe = cursor.fetchone()[0]\\n\\nif tabela_existe:\\n    print(\"A tabela já existe. Abortando.\")\\n    conexao.close()\\n    exit()\\n\\n# Verifica se o banco de dados \"olistdb\" existe\\ncursor.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = \\'olistdb\\'\")\\nbanco_existe = cursor.fetchone()\\n\\nif not banco_existe:\\n    # Criação do banco de dados \"olistdb\"\\n    cursor.execute(\"CREATE DATABASE olistdb\")\\n    print(\"Banco de dados \\'olistdb\\' criado.\")\\n\\n# Conexão com o banco de dados \"olistdb\"\\nconexao.close()\\nconexao = psycopg2.connect(\\n    host=host,\\n    port=porta,\\n    database=\\'olistdb\\',\\n    user=usuario,\\n    password=senha\\n)\\n\\n# Leitura do arquivo Parquet\\ndados_parquet = pd.read_parquet(caminho_arquivo)\\n\\n# Criação da tabela no banco de dados\\ncolunas_definicao = \\', \\'.join(f\"{coluna} VARCHAR\" for coluna in dados_parquet.columns)\\ncursor = conexao.cursor()\\ncursor.execute(f\"CREATE TABLE {nome_tabela} ({colunas_definicao});\")\\n\\n# Carregamento dos dados na tabela usando COPY FROM STDIN\\nbuffer = io.StringIO()\\ndados_parquet.to_csv(buffer, index=False, header=False, sep=\"\\t\")\\nbuffer.seek(0)\\ncursor.copy_from(buffer, nome_tabela, sep=\"\\t\")\\n\\n# Confirmação da transação e fechamento da conexão\\nconexao.commit()\\nconexao.close()\\n\\nprint(\"Dados carregados com sucesso!\")'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import io\n",
    "\n",
    "# Caminho para o arquivo Parquet\n",
    "caminho_arquivo = \"/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_customers_dataset.parquet\"\n",
    "nome_tabela = 'customers'\n",
    "\n",
    "# Verifica se o arquivo Parquet existe\n",
    "if not os.path.exists(caminho_arquivo):\n",
    "    print(\"Arquivo Parquet não encontrado.\")\n",
    "    exit()\n",
    "\n",
    "# Configurações de conexão com o banco de dados PostgreSQL\n",
    "host = \"localhost\"\n",
    "porta = \"5432\"\n",
    "banco = \"airflow\"\n",
    "usuario = \"airflow\"\n",
    "senha = \"airflow\"\n",
    "\n",
    "# Conexão com o banco de dados\n",
    "conexao = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=porta,\n",
    "    database=banco,\n",
    "    user=usuario,\n",
    "    password=senha\n",
    ")\n",
    "\n",
    "# Verifica se a tabela já existe\n",
    "cursor = conexao.cursor()\n",
    "cursor.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = %s);\", (nome_tabela,))\n",
    "tabela_existe = cursor.fetchone()[0]\n",
    "\n",
    "if tabela_existe:\n",
    "    print(\"A tabela já existe. Abortando.\")\n",
    "    conexao.close()\n",
    "    exit()\n",
    "\n",
    "# Verifica se o banco de dados \"olistdb\" existe\n",
    "cursor.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = 'olistdb'\")\n",
    "banco_existe = cursor.fetchone()\n",
    "\n",
    "if not banco_existe:\n",
    "    # Criação do banco de dados \"olistdb\"\n",
    "    cursor.execute(\"CREATE DATABASE olistdb\")\n",
    "    print(\"Banco de dados 'olistdb' criado.\")\n",
    "\n",
    "# Conexão com o banco de dados \"olistdb\"\n",
    "conexao.close()\n",
    "conexao = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=porta,\n",
    "    database='olistdb',\n",
    "    user=usuario,\n",
    "    password=senha\n",
    ")\n",
    "\n",
    "# Leitura do arquivo Parquet\n",
    "dados_parquet = pd.read_parquet(caminho_arquivo)\n",
    "\n",
    "# Criação da tabela no banco de dados\n",
    "colunas_definicao = ', '.join(f\"{coluna} VARCHAR\" for coluna in dados_parquet.columns)\n",
    "cursor = conexao.cursor()\n",
    "cursor.execute(f\"CREATE TABLE {nome_tabela} ({colunas_definicao});\")\n",
    "\n",
    "# Carregamento dos dados na tabela usando COPY FROM STDIN\n",
    "buffer = io.StringIO()\n",
    "dados_parquet.to_csv(buffer, index=False, header=False, sep=\"\\t\")\n",
    "buffer.seek(0)\n",
    "cursor.copy_from(buffer, nome_tabela, sep=\"\\t\")\n",
    "\n",
    "# Confirmação da transação e fechamento da conexão\n",
    "conexao.commit()\n",
    "conexao.close()\n",
    "\n",
    "print(\"Dados carregados com sucesso!\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport psycopg2\\nimport io\\n\\n# Caminho para o arquivo Parquet\\ncaminho_arquivo = \"/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_customers_dataset.parquet\"\\nnome_tabela = \\'customers\\'\\n\\n# Verifica se o arquivo Parquet existe\\nif not os.path.exists(caminho_arquivo):\\n    print(\"Arquivo Parquet não encontrado.\")\\n    exit()\\n\\n# Configurações de conexão com o banco de dados PostgreSQL\\nhost = \"localhost\"\\nporta = \"5432\"\\nbanco = \"airflow\"\\nusuario = \"airflow\"\\nsenha = \"airflow\"\\n\\n# Conexão com o banco de dados\\nconexao = psycopg2.connect(\\n    host=host,\\n    port=porta,\\n    database=banco,\\n    user=usuario,\\n    password=senha\\n)\\n\\n# Leitura do arquivo Parquet\\ndados_parquet = pd.read_parquet(caminho_arquivo)\\n\\n# Verifica se a tabela já existe\\ncursor = conexao.cursor()\\ncursor.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = %s);\", (nome_tabela,))\\ntabela_existe = cursor.fetchone()[0]\\n\\nif tabela_existe:\\n    print(\"A tabela já existe. Abortando.\")\\n    conexao.close()\\n    exit()\\n\\n# Verifica se o banco de dados \"olistdb\" existe\\ncursor.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = \\'olistdb\\'\")\\nbanco_existe = cursor.fetchone()\\n\\nif not banco_existe:\\n    # Criação do banco de dados \"olistdb\"\\n    cursor.execute(\"CREATE DATABASE olistdb\")\\n    print(\"Banco de dados \\'olistdb\\' criado.\")\\n\\n# Conexão com o banco de dados \"olistdb\"\\nconexao.close()\\nconexao = psycopg2.connect(\\n    host=host,\\n    port=porta,\\n    database=\\'olistdb\\',\\n    user=usuario,\\n    password=senha\\n)\\n\\n# Criação da tabela no banco de dados\\ncursor = conexao.cursor()\\ncursor.execute(f\"CREATE TABLE {nome_tabela} ();\")\\n\\n# Obtenção das colunas do arquivo Parquet\\ncolunas_parquet = list(dados_parquet.columns)\\n\\n# Atualização da definição da tabela no banco de dados\\nfor coluna in colunas_parquet:\\n    cursor.execute(f\"ALTER TABLE {nome_tabela} ADD COLUMN {coluna} TEXT;\")\\n\\n# Carregamento dos dados na tabela\\nbuffer = io.StringIO()\\ndados_parquet.to_csv(buffer, index=False, header=False, sep=\"\\t\")\\nbuffer.seek(0)\\ncursor.copy_from(buffer, nome_tabela, sep=\"\\t\")\\n\\n# Confirmação da transação e fechamento da conexão\\nconexao.commit()\\nconexao.close()\\n\\nprint(\"Dados carregados com sucesso!\")\\n'"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import psycopg2\n",
    "import io\n",
    "\n",
    "# Caminho para o arquivo Parquet\n",
    "caminho_arquivo = \"/home/michel/Documentos/opt/kaggle_olist/raw/parquet/olist_customers_dataset.parquet\"\n",
    "nome_tabela = 'customers'\n",
    "\n",
    "# Verifica se o arquivo Parquet existe\n",
    "if not os.path.exists(caminho_arquivo):\n",
    "    print(\"Arquivo Parquet não encontrado.\")\n",
    "    exit()\n",
    "\n",
    "# Configurações de conexão com o banco de dados PostgreSQL\n",
    "host = \"localhost\"\n",
    "porta = \"5432\"\n",
    "banco = \"airflow\"\n",
    "usuario = \"airflow\"\n",
    "senha = \"airflow\"\n",
    "\n",
    "# Conexão com o banco de dados\n",
    "conexao = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=porta,\n",
    "    database=banco,\n",
    "    user=usuario,\n",
    "    password=senha\n",
    ")\n",
    "\n",
    "# Leitura do arquivo Parquet\n",
    "dados_parquet = pd.read_parquet(caminho_arquivo)\n",
    "\n",
    "# Verifica se a tabela já existe\n",
    "cursor = conexao.cursor()\n",
    "cursor.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = %s);\", (nome_tabela,))\n",
    "tabela_existe = cursor.fetchone()[0]\n",
    "\n",
    "if tabela_existe:\n",
    "    print(\"A tabela já existe. Abortando.\")\n",
    "    conexao.close()\n",
    "    exit()\n",
    "\n",
    "# Verifica se o banco de dados \"olistdb\" existe\n",
    "cursor.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = 'olistdb'\")\n",
    "banco_existe = cursor.fetchone()\n",
    "\n",
    "if not banco_existe:\n",
    "    # Criação do banco de dados \"olistdb\"\n",
    "    cursor.execute(\"CREATE DATABASE olistdb\")\n",
    "    print(\"Banco de dados 'olistdb' criado.\")\n",
    "\n",
    "# Conexão com o banco de dados \"olistdb\"\n",
    "conexao.close()\n",
    "conexao = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=porta,\n",
    "    database='olistdb',\n",
    "    user=usuario,\n",
    "    password=senha\n",
    ")\n",
    "\n",
    "# Criação da tabela no banco de dados\n",
    "cursor = conexao.cursor()\n",
    "cursor.execute(f\"CREATE TABLE {nome_tabela} ();\")\n",
    "\n",
    "# Obtenção das colunas do arquivo Parquet\n",
    "colunas_parquet = list(dados_parquet.columns)\n",
    "\n",
    "# Atualização da definição da tabela no banco de dados\n",
    "for coluna in colunas_parquet:\n",
    "    cursor.execute(f\"ALTER TABLE {nome_tabela} ADD COLUMN {coluna} TEXT;\")\n",
    "\n",
    "# Carregamento dos dados na tabela\n",
    "buffer = io.StringIO()\n",
    "dados_parquet.to_csv(buffer, index=False, header=False, sep=\"\\t\")\n",
    "buffer.seek(0)\n",
    "cursor.copy_from(buffer, nome_tabela, sep=\"\\t\")\n",
    "\n",
    "# Confirmação da transação e fechamento da conexão\n",
    "conexao.commit()\n",
    "conexao.close()\n",
    "\n",
    "print(\"Dados carregados com sucesso!\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
