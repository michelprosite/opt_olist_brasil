{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michel Souza Santana\n",
    "## Projeto Desafio Aceleras\n",
    "## Trilha 1\n",
    "> Start: 15/05/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1 - Transformação do ER proposto em um BI, realizando o ETL usando uma ferrmenta local (Talend, Apache Hop, Nifi, Airflow, SSIS, Pentaho,…)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entenda o modelo ER: Familiarize-se com o modelo ER existente, incluindo as tabelas, relacionamentos e atributos. Isso ajudará você a mapear corretamente os dados durante a transformação.\n",
    "\n",
    "* Identifique os requisitos de BI: Compreenda as necessidades e requisitos do seu projeto de BI. Identifique as informações que você precisa extrair e apresentar no ambiente de BI.\n",
    "\n",
    "* Escolha uma ferramenta ETL: Pesquise e selecione uma ferramenta ETL adequada para sua transformação de dados. Existem várias opções disponíveis, como Pentaho Data Integration, Talend, Microsoft SQL Server Integration Services (SSIS), entre outras.\n",
    "\n",
    "* Instale a ferramenta ETL: Faça o download e instale a ferramenta ETL selecionada no seu ambiente local.\n",
    "\n",
    "* Conecte-se ao banco de dados: Configure a conexão da ferramenta ETL com o banco de dados que contém os dados do modelo ER. Forneça as credenciais de acesso necessárias para estabelecer a conexão.\n",
    "\n",
    "* Extração de dados: Utilizando a ferramenta ETL, extraia os dados do banco de dados conforme necessário para o seu projeto de BI. Isso pode envolver a seleção de tabelas específicas, filtragem de dados ou até mesmo a união de várias tabelas para obter as informações desejadas.\n",
    "\n",
    "* Limpeza e transformação de dados: Aplique as transformações necessárias nos dados extraídos para adequá-los às necessidades do ambiente de BI. Isso pode incluir a remoção de dados duplicados, preenchimento de valores ausentes, conversão de formatos de data, entre outros processos de limpeza e transformação.\n",
    "\n",
    "* Mapeamento para o modelo dimensional: Se você estiver construindo um data warehouse ou uma solução de BI baseada em modelo dimensional, mapeie os dados extraídos para as dimensões e fatos do seu modelo dimensional. Isso envolve a definição de hierarquias, chaves e relacionamentos.\n",
    "\n",
    "* Desenvolva fluxos de trabalho ETL: Utilizando a ferramenta ETL, crie fluxos de trabalho que automatizem a transformação de dados. Isso pode envolver a criação de transformações, tarefas agendadas e outras operações para garantir a integridade e atualização dos dados.\n",
    "\n",
    "* Carregamento dos dados: Carregue os dados transformados no ambiente de BI, que pode incluir um data warehouse, um banco de dados ou outra solução de armazenamento de dados.\n",
    "\n",
    "* Desenvolva visualizações e relatórios: Com os dados carregados no ambiente de BI, desenvolva visualizações e relatórios interativos para fornecer insights acionáveis aos usuários finais. Isso pode ser feito usando ferramentas de visualização de dados como Tableau, Power BI, QlikView, entre outras.\n",
    "\n",
    "* Teste e valide: Realize testes para garantir a precisão e a integridade dos dados transformados. Verifique se as visualiza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando o pyrrow para conversão dos arquivos csv em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os diretórios estruturais do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = !pwd\n",
    "f = pd.read_csv(str(path_folder[0]) + '/folders.csv')\n",
    "lista_folders = f['Folders'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório engineer criado com sucesso!\n",
      "Diretório raw criado com sucesso!\n",
      "Diretório refined criado com sucesso!\n",
      "Diretório transient criado com sucesso!\n",
      "Diretório trusted criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "for i in lista_folders:\n",
    "    diretorio = str(path_folder[0]) \n",
    "\n",
    "    if not os.path.exists(diretorio + '/' + i):\n",
    "        os.makedirs(diretorio + '/' + i)\n",
    "        print(f\"Diretório {i} criado com sucesso!\")\n",
    "    else:\n",
    "        print(f\"O diretório {i} já existe.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o arquivo 'controller.csv' na pasta enginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho do arquivo CSV\n",
    "caminho_arquivo = str(path_folder[0]) + '/' + 'engineer/controller.csv'\n",
    "dados = [\n",
    "[\"path_transient\",\"path_raw\",\"table_transient\",\"table_raw\", \"table_name\"],\n",
    "[str(path_folder[0]) + '/' + \"transient\",\"/home/michel/opt/keggle/raw\",\"olist_customers_dataset\",\"olist_customers_dataset\",\"stg_customers\"],\n",
    "[str(path_folder[0]) + '/' + \"transient\",str(path_folder[0]) + '/' + \"raw\",\"olist_geolocation_dataset\",\"olist_geolocation_dataset\",\"stg_geolocation\"],\n",
    "[str(path_folder[0]) + '/' + \"transient\",str(path_folder[0]) + '/' + \"raw\",\"olist_order_items_dataset\",\"olist_order_items_dataset\",\"stg_items\"],\n",
    "[str(path_folder[0]) + '/' + \"transient\",str(path_folder[0]) + '/' + \"raw\",\"olist_order_payments_dataset\",\"olist_order_payments_dataset\",\"stg_payments\"],\n",
    "[str(path_folder[0]) + '/' + \"transient\",str(path_folder[0]) + '/' + \"raw\",\"olist_order_reviews_dataset\",\"olist_order_reviews_dataset\",\"stg_reviews\"],\n",
    "[str(path_folder[0]) + '/' + \"transient\",str(path_folder[0]) + '/' + \"raw\",\"olist_orders_dataset\",\"olist_orders_dataset\",\"stg_orders\"],\n",
    "[str(path_folder[0]) + '/' + \"transient\",str(path_folder[0]) + '/' + \"raw\",\"olist_products_dataset\",\"olist_products_dataset\",\"stg_products\"],\n",
    "[str(path_folder[0]) + '/' + \"transient\",str(path_folder[0]) + '/' + \"raw\",\"olist_sellers_dataset\",\"olist_sellers_dataset\",\"stg_sellers\"]\n",
    "]\n",
    "# Abrir o arquivo CSV em modo de escrita e cria\n",
    "with open(caminho_arquivo, 'w', newline='') as arquivo_csv:\n",
    "    writer = csv.writer(arquivo_csv)\n",
    "    writer.writerows(dados)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificar as fontes de dados: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "\n",
    "A URL em questão se refere a um conjunto de dados disponibilizado no site Kaggle, que contém informações sobre o comércio eletrônico no Brasil. O conjunto de dados é intitulado \"Brazilian E-Commerce Public Dataset by Olist\" e foi criado pela empresa Olist, que é uma plataforma de vendas on-line que conecta pequenos e médios varejistas a marketplaces.\n",
    "\n",
    "O conjunto de dados contém informações de mais de 100 mil pedidos de clientes, com dados que incluem informações do produto, preços, prazos de entrega, avaliações de clientes e informações sobre o vendedor. Além disso, o conjunto de dados contém informações sobre geolocalização dos clientes, categoria de produtos e informações sobre a própria loja virtual.\n",
    "\n",
    "Este conjunto de dados pode ser extremamente útil para análises sobre comércio eletrônico no Brasil, permitindo a análise de tendências de consumo, comportamento dos clientes, performance de vendas e muito mais. A disponibilização de dados desse tipo é importante para o desenvolvimento de modelos de negócios mais eficientes e para a tomada de decisões mais informadas no setor de e-commerce brasileiro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copia os dados do site do kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando o Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixando os arquivos csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixar & diszipar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michel/opt/kaggle_olist/transient\n"
     ]
    }
   ],
   "source": [
    "print(str(path_folder[0]) + '/' + 'transient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/michel/.kaggle/kaggle.json'\n",
      "Downloading brazilian-ecommerce.zip to /home/michel/opt/kaggle_olist/transient\n",
      "100%|██████████████████████████████████████| 42.6M/42.6M [00:11<00:00, 4.11MB/s]\n",
      "100%|██████████████████████████████████████| 42.6M/42.6M [00:11<00:00, 4.04MB/s]\n",
      "Archive:  brazilian-ecommerce.zip\n",
      "  inflating: olist_customers_dataset.csv  \n",
      "  inflating: olist_geolocation_dataset.csv  \n",
      "  inflating: olist_order_items_dataset.csv  \n",
      "  inflating: olist_order_payments_dataset.csv  \n",
      "  inflating: olist_order_reviews_dataset.csv  \n",
      "  inflating: olist_orders_dataset.csv  \n",
      "  inflating: olist_products_dataset.csv  \n",
      "  inflating: olist_sellers_dataset.csv  \n",
      "  inflating: product_category_name_translation.csv  \n",
      "Arquivo carregado e descompactado.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(str(path_folder[0]) + '/' + 'transient/brazilian-ecommerce.zip'):\n",
    "    \n",
    "    r = !pwd\n",
    "    PATH_FOLDER = r[0] + '/transient'\n",
    "    os.environ['PATH_FOLDER'] = PATH_FOLDER\n",
    "\n",
    "    !cd $PATH_FOLDER && kaggle datasets download -d olistbr/brazilian-ecommerce\n",
    "    !cd $PATH_FOLDER && unzip brazilian-ecommerce.zip\n",
    "    print('Arquivo carregado e descompactado.')\n",
    "else:\n",
    "    print('Arquivo já existe.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando as variáveis necessárias para manipulação dos arquivos csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_transient</th>\n",
       "      <th>path_raw</th>\n",
       "      <th>table_transient</th>\n",
       "      <th>table_raw</th>\n",
       "      <th>table_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>stg_customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>stg_geolocation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>stg_items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>stg_payments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>stg_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>stg_orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>stg_products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>stg_sellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            path_transient                           path_raw   \n",
       "0  /home/michel/opt/kaggle_olist/transient        /home/michel/opt/keggle/raw  \\\n",
       "1  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "2  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "3  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "4  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "5  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "6  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "7  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "\n",
       "                table_transient                     table_raw       table_name  \n",
       "0       olist_customers_dataset       olist_customers_dataset    stg_customers  \n",
       "1     olist_geolocation_dataset     olist_geolocation_dataset  stg_geolocation  \n",
       "2     olist_order_items_dataset     olist_order_items_dataset        stg_items  \n",
       "3  olist_order_payments_dataset  olist_order_payments_dataset     stg_payments  \n",
       "4   olist_order_reviews_dataset   olist_order_reviews_dataset      stg_reviews  \n",
       "5          olist_orders_dataset          olist_orders_dataset       stg_orders  \n",
       "6        olist_products_dataset        olist_products_dataset     stg_products  \n",
       "7         olist_sellers_dataset         olist_sellers_dataset      stg_sellers  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = pd.read_csv(str(path_folder[0]) + '/' + 'engineer/controller.csv')\n",
    "path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando a formação das rotas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michel/opt/kaggle_olist/transient/olist_customers_dataset.csv\n",
      "/home/michel/opt/keggle/raw/olist_customers_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "print(path['path_transient'][0] + '/' + path['table_transient'][0] + '.csv')\n",
    "print(path['path_raw'][0] + '/' + path['table_raw'][0] + '.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(path['path_transient']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertendo or arquivos CSV em .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(path['path_transient'])):\n",
    "    df = pd.read_csv(path['path_transient'][i] + '/' + path['table_transient'][i] + '.csv')\n",
    "    df = df.astype('str')\n",
    "    df.to_parquet(str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminando as duplicadas nos ID das tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(path['path_transient'])):\n",
    "    df_clear_duplicates = pd.read_parquet(str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet')\n",
    "    coluna = df_clear_duplicates.columns[0]\n",
    "    df_clear_duplicates.drop_duplicates(subset=coluna, inplace=True)\n",
    "    df_clear_duplicates.to_parquet(str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
