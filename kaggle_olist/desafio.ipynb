{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michel Souza Santana\n",
    "## Projeto Desafio Aceleras\n",
    "## Trilha 1\n",
    "> Start: 15/05/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1 - Transformação do ER proposto em um BI, realizando o ETL usando uma ferrmenta local (Talend, Apache Hop, Nifi, Airflow, SSIS, Pentaho,…)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entenda o modelo ER: Familiarize-se com o modelo ER existente, incluindo as tabelas, relacionamentos e atributos. Isso ajudará você a mapear corretamente os dados durante a transformação.\n",
    "\n",
    "* Identifique os requisitos de BI: Compreenda as necessidades e requisitos do seu projeto de BI. Identifique as informações que você precisa extrair e apresentar no ambiente de BI.\n",
    "\n",
    "* Escolha uma ferramenta ETL: Pesquise e selecione uma ferramenta ETL adequada para sua transformação de dados. Existem várias opções disponíveis, como Pentaho Data Integration, Talend, Microsoft SQL Server Integration Services (SSIS), entre outras.\n",
    "\n",
    "* Instale a ferramenta ETL: Faça o download e instale a ferramenta ETL selecionada no seu ambiente local.\n",
    "\n",
    "* Conecte-se ao banco de dados: Configure a conexão da ferramenta ETL com o banco de dados que contém os dados do modelo ER. Forneça as credenciais de acesso necessárias para estabelecer a conexão.\n",
    "\n",
    "* Extração de dados: Utilizando a ferramenta ETL, extraia os dados do banco de dados conforme necessário para o seu projeto de BI. Isso pode envolver a seleção de tabelas específicas, filtragem de dados ou até mesmo a união de várias tabelas para obter as informações desejadas.\n",
    "\n",
    "* Limpeza e transformação de dados: Aplique as transformações necessárias nos dados extraídos para adequá-los às necessidades do ambiente de BI. Isso pode incluir a remoção de dados duplicados, preenchimento de valores ausentes, conversão de formatos de data, entre outros processos de limpeza e transformação.\n",
    "\n",
    "* Mapeamento para o modelo dimensional: Se você estiver construindo um data warehouse ou uma solução de BI baseada em modelo dimensional, mapeie os dados extraídos para as dimensões e fatos do seu modelo dimensional. Isso envolve a definição de hierarquias, chaves e relacionamentos.\n",
    "\n",
    "* Desenvolva fluxos de trabalho ETL: Utilizando a ferramenta ETL, crie fluxos de trabalho que automatizem a transformação de dados. Isso pode envolver a criação de transformações, tarefas agendadas e outras operações para garantir a integridade e atualização dos dados.\n",
    "\n",
    "* Carregamento dos dados: Carregue os dados transformados no ambiente de BI, que pode incluir um data warehouse, um banco de dados ou outra solução de armazenamento de dados.\n",
    "\n",
    "* Desenvolva visualizações e relatórios: Com os dados carregados no ambiente de BI, desenvolva visualizações e relatórios interativos para fornecer insights acionáveis aos usuários finais. Isso pode ser feito usando ferramentas de visualização de dados como Tableau, Power BI, QlikView, entre outras.\n",
    "\n",
    "* Teste e valide: Realize testes para garantir a precisão e a integridade dos dados transformados. Verifique se as visualiza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os processos de engenharia de dados (data engineering) são etapas sequenciais usadas para transformar e gerenciar dados em um pipeline de processamento. Esses processos são realizados em várias etapas, começando pelos dados brutos e terminando com os dados confiáveis e prontos para análise. Aqui está uma explicação de cada processo em ordem de execução:\n",
    "\n",
    "* Processo de Raw Data (Dados Brutos):\n",
    "Nesta fase inicial, os dados brutos são coletados de várias fontes, como bancos de dados, arquivos CSV, APIs, feeds de streaming, entre outros. Esses dados podem estar em formatos diversos e podem conter ruído, inconsistências e falta de estrutura. O objetivo do processo de Dados Brutos é extrair e armazenar esses dados em seu estado bruto original.\n",
    "\n",
    "* Processo de Data Engineering (Engenharia de Dados):\n",
    "Nesta etapa, os dados brutos são processados e transformados em um formato adequado para análise e uso posterior. Isso envolve atividades como limpeza de dados, padronização de formatos, filtragem de dados inválidos ou incompletos, remoção de duplicatas e criação de estruturas de dados otimizadas para consultas e processamento eficiente. O objetivo é obter dados estruturados e refinados que possam ser usados em análises e outros processos.\n",
    "\n",
    "* Processo de Refined Data (Dados Refinados):\n",
    "Nesta fase, os dados processados são refinados ainda mais para atender a requisitos específicos de negócios e análise. Isso pode incluir agregação de dados, cálculos adicionais, enriquecimento com informações adicionais, como dados geográficos ou dados de terceiros, e transformações personalizadas para atender às necessidades específicas dos usuários finais. O objetivo é fornecer dados refinados e mais valiosos para análise e tomada de decisão.\n",
    "\n",
    "* Processo de Transient Data (Dados Transitórios):\n",
    "Os dados transitórios são dados temporários ou de curta duração usados para processamento e transformação intermediários. Esses dados podem ser gerados durante o processamento em tempo real, pipelines de processamento de dados em lote ou durante a integração de diferentes fontes de dados. Os dados transitórios são normalmente armazenados em sistemas de processamento de fluxo de dados ou bancos de dados temporários, e seu objetivo é suportar operações intermediárias antes que os dados sejam refinados e confiáveis.\n",
    "\n",
    "* Processo de Trusted Data (Dados Confiáveis):\n",
    "Nesta fase final, os dados são considerados confiáveis, prontos para uso e adequados para análise e tomada de decisão. Isso envolve garantir a qualidade dos dados, validar a precisão, consistência e integridade dos dados, aplicar regras de negócios e conformidade, bem como implementar mecanismos de controle de qualidade. O objetivo é fornecer dados confiáveis que possam ser usados com confiança para análise, geração de relatórios e outras tarefas de negócios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando o pyrrow para conversão dos arquivos csv em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data Raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os diretórios estruturais do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = !pwd\n",
    "f = pd.read_csv(str(path_folder[0]) + '/folders.csv')\n",
    "lista_folders = f['Folders'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório engineer criado com sucesso!\n",
      "Diretório raw criado com sucesso!\n",
      "Diretório refined criado com sucesso!\n",
      "Diretório transient criado com sucesso!\n",
      "Diretório trusted criado com sucesso!\n",
      "Pasta parquet criada.\n"
     ]
    }
   ],
   "source": [
    "for i in lista_folders:\n",
    "    diretorio = str(path_folder[0]) \n",
    "\n",
    "    if not os.path.exists(diretorio + '/' + i):\n",
    "        os.makedirs(diretorio + '/' + i)\n",
    "        print(f\"Diretório {i} criado com sucesso!\")\n",
    "    else:\n",
    "        print(f\"O diretório {i} já existe.\")\n",
    "if not os.path.exists(diretorio + '/' + '/raw' + '/parquet'):\n",
    "    os.makedirs(diretorio + '/' + '/raw' + '/parquet')\n",
    "    print('Pasta parquet criada.')\n",
    "else:\n",
    "    print('Pasta parquet já existe;')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificar as fontes de dados: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "\n",
    "A URL em questão se refere a um conjunto de dados disponibilizado no site Kaggle, que contém informações sobre o comércio eletrônico no Brasil. O conjunto de dados é intitulado \"Brazilian E-Commerce Public Dataset by Olist\" e foi criado pela empresa Olist, que é uma plataforma de vendas on-line que conecta pequenos e médios varejistas a marketplaces.\n",
    "\n",
    "O conjunto de dados contém informações de mais de 100 mil pedidos de clientes, com dados que incluem informações do produto, preços, prazos de entrega, avaliações de clientes e informações sobre o vendedor. Além disso, o conjunto de dados contém informações sobre geolocalização dos clientes, categoria de produtos e informações sobre a própria loja virtual.\n",
    "\n",
    "Este conjunto de dados pode ser extremamente útil para análises sobre comércio eletrônico no Brasil, permitindo a análise de tendências de consumo, comportamento dos clientes, performance de vendas e muito mais. A disponibilização de dados desse tipo é importante para o desenvolvimento de modelos de negócios mais eficientes e para a tomada de decisões mais informadas no setor de e-commerce brasileiro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copia os dados do site do kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando o Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixando os arquivos csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixar & diszipar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/michel/.kaggle/kaggle.json'\n",
      "Downloading brazilian-ecommerce.zip to /home/michel/opt/kaggle_olist/raw\n",
      "100%|███████████████████████████████████████| 42.6M/42.6M [00:13<00:00, 812kB/s]\n",
      "100%|██████████████████████████████████████| 42.6M/42.6M [00:13<00:00, 3.23MB/s]\n",
      "Archive:  brazilian-ecommerce.zip\n",
      "  inflating: olist_customers_dataset.csv  \n",
      "  inflating: olist_geolocation_dataset.csv  \n",
      "  inflating: olist_order_items_dataset.csv  \n",
      "  inflating: olist_order_payments_dataset.csv  \n",
      "  inflating: olist_order_reviews_dataset.csv  \n",
      "  inflating: olist_orders_dataset.csv  \n",
      "  inflating: olist_products_dataset.csv  \n",
      "  inflating: olist_sellers_dataset.csv  \n",
      "  inflating: product_category_name_translation.csv  \n",
      "Arquivo carregado e descompactado.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(str(path_folder[0]) + '/' + 'raw/brazilian-ecommerce.zip'):\n",
    "    \n",
    "    r = !pwd\n",
    "    PATH_FOLDER = r[0] + '/raw'\n",
    "    os.environ['PATH_FOLDER'] = PATH_FOLDER\n",
    "\n",
    "    !cd $PATH_FOLDER && kaggle datasets download -d olistbr/brazilian-ecommerce\n",
    "    !cd $PATH_FOLDER && unzip brazilian-ecommerce.zip\n",
    "    !cd $PATH_FOLDER && rm -r brazilian-ecommerce.zip\n",
    "    print('Arquivo carregado e descompactado.')\n",
    "else:\n",
    "    print('Arquivo já existe.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o arquivo 'controller.csv' na pasta enginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta = str(path_folder[0]) + '/raw'  # Substitua pelo caminho da pasta desejada\n",
    "\n",
    "# Capturar nomes dos arquivos e remover a extensão\n",
    "arquivos = [os.path.splitext(arquivo)[0] for arquivo in os.listdir(pasta) if os.path.isfile(os.path.join(pasta, arquivo))]\n",
    "# Eliminar as palavras indesejadas\n",
    "palavras_indesejadas = ['_dataset.csv', 'olist_', 'order_', '_dataset', 'product_', '_name', '_translation']\n",
    "nomes_limpos = [arquivo for arquivo in arquivos]\n",
    "for palavra in palavras_indesejadas:\n",
    "   nomes_limpos = [nome.replace(palavra, '') for nome in nomes_limpos]\n",
    "\n",
    "# Caminho do arquivo CSV a ser salvo\n",
    "caminho_arquivo = str(path_folder[0]) + '/' + 'controller.csv'\n",
    "\n",
    "# Definir os dados a serem escritos no arquivo CSV\n",
    "dados = [\n",
    "    [\"path_transient\", \"path_raw\", \"table_transient\", \"table_raw\", \"table_name\"]\n",
    "] + [\n",
    "    [str(path_folder[0]) + '/' + \"transient\", str(path_folder[0]) + '/' + \"raw\", nome, nome, nome_limp]\n",
    "    for nome, nome_limp in zip(arquivos, nomes_limpos)\n",
    "]\n",
    "\n",
    "# Salvar o arquivo CSV\n",
    "pd.DataFrame(dados).to_csv(caminho_arquivo, index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando tabela com os path necessários para manipulação dos arquivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando a formação dos path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michel/opt/kaggle_olist/transient/olist_order_payments_dataset.csv\n",
      "/home/michel/opt/kaggle_olist/raw/olist_order_payments_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "print(path['path_transient'][0] + '/' + path['table_transient'][0] + '.csv')\n",
    "print(path['path_raw'][0] + '/' + path['table_raw'][0] + '.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_transient</th>\n",
       "      <th>path_raw</th>\n",
       "      <th>table_transient</th>\n",
       "      <th>table_raw</th>\n",
       "      <th>table_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>payments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>sellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>geolocation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/michel/opt/kaggle_olist/transient</td>\n",
       "      <td>/home/michel/opt/kaggle_olist/raw</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>orders</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            path_transient                           path_raw   \n",
       "0  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw  \\\n",
       "1  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "2  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "3  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "4  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "5  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "6  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "7  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "8  /home/michel/opt/kaggle_olist/transient  /home/michel/opt/kaggle_olist/raw   \n",
       "\n",
       "                     table_transient                          table_raw   \n",
       "0       olist_order_payments_dataset       olist_order_payments_dataset  \\\n",
       "1            olist_customers_dataset            olist_customers_dataset   \n",
       "2             olist_products_dataset             olist_products_dataset   \n",
       "3        olist_order_reviews_dataset        olist_order_reviews_dataset   \n",
       "4              olist_sellers_dataset              olist_sellers_dataset   \n",
       "5          olist_geolocation_dataset          olist_geolocation_dataset   \n",
       "6  product_category_name_translation  product_category_name_translation   \n",
       "7          olist_order_items_dataset          olist_order_items_dataset   \n",
       "8               olist_orders_dataset               olist_orders_dataset   \n",
       "\n",
       "    table_name  \n",
       "0     payments  \n",
       "1    customers  \n",
       "2     products  \n",
       "3      reviews  \n",
       "4      sellers  \n",
       "5  geolocation  \n",
       "6     category  \n",
       "7        items  \n",
       "8       orders  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = pd.read_csv(str(path_folder[0]) + '/' + 'controller.csv')\n",
    "path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertendo or arquivos CSV em .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(path['path_raw'])):\n",
    "    df = pd.read_csv(path['path_raw'][i] + '/' + path['table_raw'][i] + '.csv')\n",
    "    df = df.astype('str')\n",
    "    df.to_parquet(str(path['path_raw'][i]) + '/' + 'parquet' + '/' + str(path['table_raw'][i]) + '.parquet')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminando as duplicadas nos ID das tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFalha ao iniciar o Kernel. \n",
      "\u001b[1;31mNão é possível executar o código, a sessão foi descartada. \n",
      "\u001b[1;31mConsulte o <a href='command:jupyter.viewOutput'>log</a> do Jupyter para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "\"\"\"for i in range(len(path['path_raw'])):\n",
    "    df_clear_duplicates = pd.read_parquet(str(path['path_raw'][i]) + '/' + 'parquet' + '/' + str(path['table_raw'][i]) + '.parquet')\n",
    "    coluna = df_clear_duplicates.columns[0]\n",
    "    df_clear_duplicates.drop_duplicates(subset=coluna, inplace=True)\n",
    "    df_clear_duplicates.to_parquet(str(path['path_raw'][i]) + '/' + 'parquet' + '/' + str(path['table_raw'][i]) + '.parquet')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFalha ao iniciar o Kernel. \n",
      "\u001b[1;31mNão é possível executar o código, a sessão foi descartada. \n",
      "\u001b[1;31mConsulte o <a href='command:jupyter.viewOutput'>log</a> do Jupyter para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "\"\"\"MERGE INTO tabela_original t\n",
    "USING (\n",
    "    SELECT DISTINCT coluna1, coluna2, coluna3, ...\n",
    "    FROM tabela_original\n",
    ") s ON (t.coluna1 = s.coluna1)\n",
    "WHEN MATCHED THEN\n",
    "    DELETE;\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aqui, estarei interando sobre um banco de dado MySql para criar tabelas correspondentes aos arquivos e carrega-las como forma de Buckap para os ddados da RAW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando as tabelas no banco de dados Mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFalha ao iniciar o Kernel. \n",
      "\u001b[1;31mNão é possível executar o código, a sessão foi descartada. \n",
      "\u001b[1;31mConsulte o <a href='command:jupyter.viewOutput'>log</a> do Jupyter para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "#!pip install mysql-connector-python\n",
    "senha_db = pd.read_csv('/home/michel/senha.csv')\n",
    "import mysql.connector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conectando ao banco de dados MySQL e criando as tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFalha ao iniciar o Kernel. \n",
      "\u001b[1;31mNão é possível executar o código, a sessão foi descartada. \n",
      "\u001b[1;31mConsulte o <a href='command:jupyter.viewOutput'>log</a> do Jupyter para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "\"\"\"cnx = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password=senha_db['senha'][0],\n",
    "    database='staging_olist_db'\n",
    ")\n",
    "\n",
    "# Cursor para executar as consultas SQL\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Lista de queries SQL para criação das tabelas\n",
    "create_table_queries = [\"\"\"\n",
    "\"\"\"\n",
    "-- Criação da tabela CUSTOMER\n",
    "CREATE TABLE IF NOT EXISTS stg_customers (\n",
    "  customer_id VARCHAR(200) NOT NULL,\n",
    "  customer_unique_id VARCHAR(255),\n",
    "  customer_zip_code_prefix VARCHAR(200),\n",
    "  customer_city VARCHAR(255),\n",
    "  customer_state VARCHAR(255),\n",
    "  PRIMARY KEY (customer_id)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela GEOLOCATION\n",
    "CREATE TABLE IF NOT EXISTS stg_geolocation (\n",
    "  geolocation_zip_code_prefix VARCHAR(200) NOT NULL,\n",
    "  geolocation_lat VARCHAR(255),\n",
    "  geolocation_lng VARCHAR(255),\n",
    "  geolocation_city VARCHAR(255),\n",
    "  geolocation_state VARCHAR(255),\n",
    "  PRIMARY KEY (geolocation_zip_code_prefix, geolocation_lat, geolocation_lng)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela ITEMS\n",
    "CREATE TABLE IF NOT EXISTS stg_items (\n",
    "  order_id VARCHAR(255) NOT NULL,\n",
    "  order_item_id VARCHAR(200) NOT NULL,\n",
    "  product_id VARCHAR(255) NOT NULL,\n",
    "  seller_id VARCHAR(255) NOT NULL,\n",
    "  shipping_limit_date VARCHAR(255),\n",
    "  price VARCHAR(255),\n",
    "  freight_value VARCHAR(255),\n",
    "  PRIMARY KEY (order_id, order_item_id) -- Primary Key Composta\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela PAYMENT\n",
    "CREATE TABLE IF NOT EXISTS stg_payments (\n",
    "  order_id VARCHAR(255) NOT NULL,\n",
    "  payment_sequential VARCHAR(200) NOT NULL,\n",
    "  payment_type VARCHAR(255) NOT NULL,\n",
    "  payment_installments VARCHAR(200),\n",
    "  payment_value VARCHAR(255),\n",
    "  PRIMARY KEY (order_id, payment_sequential) -- Primary Key Composta\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela REVIEWS\n",
    "CREATE TABLE IF NOT EXISTS stg_reviews (\n",
    "  review_id VARCHAR(255) NOT NULL,\n",
    "  order_id VARCHAR(255) NOT NULL,\n",
    "  review_score VARCHAR(200),\n",
    "  review_comment_title VARCHAR(255),\n",
    "  review_comment_message TEXT,\n",
    "  review_creation_date VARCHAR(255),\n",
    "  review_answer_timestamp VARCHAR(255),\n",
    "  PRIMARY KEY (review_id)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela ORDERS\n",
    "CREATE TABLE IF NOT EXISTS stg_orders (\n",
    "  order_id VARCHAR(255) NOT NULL,\n",
    "  customer_id VARCHAR(255),\n",
    "  order_status VARCHAR(255),\n",
    "  order_purchase_timestamp VARCHAR(255),\n",
    "  order_approved_at VARCHAR(255),\n",
    "  order_delivered_carrier_date VARCHAR(255),\n",
    "  order_delivered_customer_date VARCHAR(255),\n",
    "  order_estimated_delivery_date VARCHAR(255),\n",
    "  PRIMARY KEY (order_id)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela PRODUCTS\n",
    "CREATE TABLE IF NOT EXISTS stg_products (\n",
    "  product_id VARCHAR(255) NOT NULL,\n",
    "  product_category_name VARCHAR(255),\n",
    "  product_name_lenght VARCHAR(200),\n",
    "  product_description_lenght VARCHAR(200),\n",
    "  product_photos_qty VARCHAR(200),\n",
    "  product_weight_g VARCHAR(200),\n",
    "  product_length_cm VARCHAR(200),\n",
    "  product_height_cm VARCHAR(200),\n",
    "  product_width_cm VARCHAR(200),\n",
    "  PRIMARY KEY (product_id)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela SELLERS\n",
    "CREATE TABLE IF NOT EXISTS stg_sellers (\n",
    "  seller_id VARCHAR(255) NOT NULL,\n",
    "  seller_zip_code_prefix VARCHAR(255),\n",
    "  seller_city VARCHAR(255),\n",
    "  seller_state VARCHAR(255),\n",
    "  PRIMARY KEY (seller_id)\n",
    ")\n",
    "\"\"\"\n",
    "\"\"\"]\n",
    "\n",
    "# Executar as consultas SQL para criar as tabelas\n",
    "for query in create_table_queries:\n",
    "    cursor.execute(query)\n",
    "\n",
    "# Commit das alterações\n",
    "cnx.commit()\n",
    "\n",
    "# Fechando o cursor e a conexão\n",
    "cursor.close()\n",
    "cnx.close()\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
