{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michel Souza Santana\n",
    "## Projeto Desafio Aceleras\n",
    "## Trilha 1\n",
    "> Start: 15/05/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1 - Transformação do ER proposto em um BI, realizando o ETL usando uma ferrmenta local (Talend, Apache Hop, Nifi, Airflow, SSIS, Pentaho,…)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entenda o modelo ER: Familiarize-se com o modelo ER existente, incluindo as tabelas, relacionamentos e atributos. Isso ajudará você a mapear corretamente os dados durante a transformação.\n",
    "\n",
    "* Identifique os requisitos de BI: Compreenda as necessidades e requisitos do seu projeto de BI. Identifique as informações que você precisa extrair e apresentar no ambiente de BI.\n",
    "\n",
    "* Escolha uma ferramenta ETL: Pesquise e selecione uma ferramenta ETL adequada para sua transformação de dados. Existem várias opções disponíveis, como Pentaho Data Integration, Talend, Microsoft SQL Server Integration Services (SSIS), entre outras.\n",
    "\n",
    "* Instale a ferramenta ETL: Faça o download e instale a ferramenta ETL selecionada no seu ambiente local.\n",
    "\n",
    "* Conecte-se ao banco de dados: Configure a conexão da ferramenta ETL com o banco de dados que contém os dados do modelo ER. Forneça as credenciais de acesso necessárias para estabelecer a conexão.\n",
    "\n",
    "* Extração de dados: Utilizando a ferramenta ETL, extraia os dados do banco de dados conforme necessário para o seu projeto de BI. Isso pode envolver a seleção de tabelas específicas, filtragem de dados ou até mesmo a união de várias tabelas para obter as informações desejadas.\n",
    "\n",
    "* Limpeza e transformação de dados: Aplique as transformações necessárias nos dados extraídos para adequá-los às necessidades do ambiente de BI. Isso pode incluir a remoção de dados duplicados, preenchimento de valores ausentes, conversão de formatos de data, entre outros processos de limpeza e transformação.\n",
    "\n",
    "* Mapeamento para o modelo dimensional: Se você estiver construindo um data warehouse ou uma solução de BI baseada em modelo dimensional, mapeie os dados extraídos para as dimensões e fatos do seu modelo dimensional. Isso envolve a definição de hierarquias, chaves e relacionamentos.\n",
    "\n",
    "* Desenvolva fluxos de trabalho ETL: Utilizando a ferramenta ETL, crie fluxos de trabalho que automatizem a transformação de dados. Isso pode envolver a criação de transformações, tarefas agendadas e outras operações para garantir a integridade e atualização dos dados.\n",
    "\n",
    "* Carregamento dos dados: Carregue os dados transformados no ambiente de BI, que pode incluir um data warehouse, um banco de dados ou outra solução de armazenamento de dados.\n",
    "\n",
    "* Desenvolva visualizações e relatórios: Com os dados carregados no ambiente de BI, desenvolva visualizações e relatórios interativos para fornecer insights acionáveis aos usuários finais. Isso pode ser feito usando ferramentas de visualização de dados como Tableau, Power BI, QlikView, entre outras.\n",
    "\n",
    "* Teste e valide: Realize testes para garantir a precisão e a integridade dos dados transformados. Verifique se as visualiza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os processos de engenharia de dados (data engineering) são etapas sequenciais usadas para transformar e gerenciar dados em um pipeline de processamento. Esses processos são realizados em várias etapas, começando pelos dados brutos e terminando com os dados confiáveis e prontos para análise. Aqui está uma explicação de cada processo em ordem de execução:\n",
    "\n",
    "* Processo de Raw Data (Dados Brutos):\n",
    "Nesta fase inicial, os dados brutos são coletados de várias fontes, como bancos de dados, arquivos CSV, APIs, feeds de streaming, entre outros. Esses dados podem estar em formatos diversos e podem conter ruído, inconsistências e falta de estrutura. O objetivo do processo de Dados Brutos é extrair e armazenar esses dados em seu estado bruto original.\n",
    "\n",
    "* Processo de Data Engineering (Engenharia de Dados):\n",
    "Nesta etapa, os dados brutos são processados e transformados em um formato adequado para análise e uso posterior. Isso envolve atividades como limpeza de dados, padronização de formatos, filtragem de dados inválidos ou incompletos, remoção de duplicatas e criação de estruturas de dados otimizadas para consultas e processamento eficiente. O objetivo é obter dados estruturados e refinados que possam ser usados em análises e outros processos.\n",
    "\n",
    "* Processo de Refined Data (Dados Refinados):\n",
    "Nesta fase, os dados processados são refinados ainda mais para atender a requisitos específicos de negócios e análise. Isso pode incluir agregação de dados, cálculos adicionais, enriquecimento com informações adicionais, como dados geográficos ou dados de terceiros, e transformações personalizadas para atender às necessidades específicas dos usuários finais. O objetivo é fornecer dados refinados e mais valiosos para análise e tomada de decisão.\n",
    "\n",
    "* Processo de Transient Data (Dados Transitórios):\n",
    "Os dados transitórios são dados temporários ou de curta duração usados para processamento e transformação intermediários. Esses dados podem ser gerados durante o processamento em tempo real, pipelines de processamento de dados em lote ou durante a integração de diferentes fontes de dados. Os dados transitórios são normalmente armazenados em sistemas de processamento de fluxo de dados ou bancos de dados temporários, e seu objetivo é suportar operações intermediárias antes que os dados sejam refinados e confiáveis.\n",
    "\n",
    "* Processo de Trusted Data (Dados Confiáveis):\n",
    "Nesta fase final, os dados são considerados confiáveis, prontos para uso e adequados para análise e tomada de decisão. Isso envolve garantir a qualidade dos dados, validar a precisão, consistência e integridade dos dados, aplicar regras de negócios e conformidade, bem como implementar mecanismos de controle de qualidade. O objetivo é fornecer dados confiáveis que possam ser usados com confiança para análise, geração de relatórios e outras tarefas de negócios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando o pyrrow para conversão dos arquivos csv em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os diretórios estruturais do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('/home/michel/opt/keggle/folders.csv')\n",
    "lista_folders = f['Folders'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório engineer criado com sucesso!\n",
      "Diretório raw criado com sucesso!\n",
      "Diretório refined criado com sucesso!\n",
      "Diretório transient criado com sucesso!\n",
      "Diretório trusted criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "for i in lista_folders:\n",
    "    diretorio = \"/home/michel/opt/keggle\" \n",
    "\n",
    "    if not os.path.exists(diretorio + '/' + i):\n",
    "        os.makedirs(diretorio + '/' + i)\n",
    "        print(f\"Diretório {i} criado com sucesso!\")\n",
    "    else:\n",
    "        print(f\"O diretório {i} já existe.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o arquivo 'controller.csv' na pasta enginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho do arquivo CSV\n",
    "caminho_arquivo = '/home/michel/opt/keggle/engineer/controller.csv'\n",
    "dados = [\n",
    "[\"path_transient\",\"path_raw\",\"table_transient\",\"table_raw\", \"table_name\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_customers_dataset\",\"olist_customers_dataset\",\"stg_customers\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_geolocation_dataset\",\"olist_geolocation_dataset\",\"stg_geolocation\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_order_items_dataset\",\"olist_order_items_dataset\",\"stg_items\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_order_payments_dataset\",\"olist_order_payments_dataset\",\"stg_payments\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_order_reviews_dataset\",\"olist_order_reviews_dataset\",\"stg_reviews\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_orders_dataset\",\"olist_orders_dataset\",\"stg_orders\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_products_dataset\",\"olist_products_dataset\",\"stg_products\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_sellers_dataset\",\"olist_sellers_dataset\",\"stg_sellers\"]\n",
    "]\n",
    "# Abrir o arquivo CSV em modo de escrita e cria\n",
    "with open(caminho_arquivo, 'w', newline='') as arquivo_csv:\n",
    "    writer = csv.writer(arquivo_csv)\n",
    "    writer.writerows(dados)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificar as fontes de dados: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "\n",
    "A URL em questão se refere a um conjunto de dados disponibilizado no site Kaggle, que contém informações sobre o comércio eletrônico no Brasil. O conjunto de dados é intitulado \"Brazilian E-Commerce Public Dataset by Olist\" e foi criado pela empresa Olist, que é uma plataforma de vendas on-line que conecta pequenos e médios varejistas a marketplaces.\n",
    "\n",
    "O conjunto de dados contém informações de mais de 100 mil pedidos de clientes, com dados que incluem informações do produto, preços, prazos de entrega, avaliações de clientes e informações sobre o vendedor. Além disso, o conjunto de dados contém informações sobre geolocalização dos clientes, categoria de produtos e informações sobre a própria loja virtual.\n",
    "\n",
    "Este conjunto de dados pode ser extremamente útil para análises sobre comércio eletrônico no Brasil, permitindo a análise de tendências de consumo, comportamento dos clientes, performance de vendas e muito mais. A disponibilização de dados desse tipo é importante para o desenvolvimento de modelos de negócios mais eficientes e para a tomada de decisões mais informadas no setor de e-commerce brasileiro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copia os dados do site do kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando o Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixando os arquivos csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixar & diszipar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/michel/.kaggle/kaggle.json'\n",
      "Downloading brazilian-ecommerce.zip to /home/michel/opt/keggle/transient\n",
      "100%|██████████████████████████████████████| 42.6M/42.6M [00:13<00:00, 3.06MB/s]\n",
      "100%|██████████████████████████████████████| 42.6M/42.6M [00:13<00:00, 3.42MB/s]\n",
      "Archive:  brazilian-ecommerce.zip\n",
      "  inflating: olist_customers_dataset.csv  \n",
      "  inflating: olist_geolocation_dataset.csv  \n",
      "  inflating: olist_order_items_dataset.csv  \n",
      "  inflating: olist_order_payments_dataset.csv  \n",
      "  inflating: olist_order_reviews_dataset.csv  \n",
      "  inflating: olist_orders_dataset.csv  \n",
      "  inflating: olist_products_dataset.csv  \n",
      "  inflating: olist_sellers_dataset.csv  \n",
      "  inflating: product_category_name_translation.csv  \n",
      "Arquivo carregado e descompactado.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('/home/michel/opt/keggle/transient/brazilian-ecommerce.zip'):\n",
    "    !cd /home/michel/opt/keggle/transient/ && kaggle datasets download -d olistbr/brazilian-ecommerce\n",
    "    !cd /home/michel/opt/keggle/transient/ && unzip brazilian-ecommerce.zip\n",
    "    print('Arquivo carregado e descompactado.')\n",
    "else:\n",
    "    print('Arquivo já existe.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando as variáveis necessárias para manipulação dos arquivos csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_transient</th>\n",
       "      <th>path_raw</th>\n",
       "      <th>table_transient</th>\n",
       "      <th>table_raw</th>\n",
       "      <th>table_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>stg_customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>stg_geolocation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>stg_items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>stg_payments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>stg_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>stg_orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>stg_products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>stg_sellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      path_transient                     path_raw   \n",
       "0  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw  \\\n",
       "1  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "2  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "3  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "4  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "5  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "6  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "7  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "\n",
       "                table_transient                     table_raw       table_name  \n",
       "0       olist_customers_dataset       olist_customers_dataset    stg_customers  \n",
       "1     olist_geolocation_dataset     olist_geolocation_dataset  stg_geolocation  \n",
       "2     olist_order_items_dataset     olist_order_items_dataset        stg_items  \n",
       "3  olist_order_payments_dataset  olist_order_payments_dataset     stg_payments  \n",
       "4   olist_order_reviews_dataset   olist_order_reviews_dataset      stg_reviews  \n",
       "5          olist_orders_dataset          olist_orders_dataset       stg_orders  \n",
       "6        olist_products_dataset        olist_products_dataset     stg_products  \n",
       "7         olist_sellers_dataset         olist_sellers_dataset      stg_sellers  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = pd.read_csv('/home/michel/opt/keggle/engineer/controller.csv')\n",
    "path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando a formação das rotas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michel/opt/keggle/transient/olist_customers_dataset.csv\n",
      "/home/michel/opt/keggle/raw/olist_customers_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "print(path['path_transient'][0] + '/' + path['table_transient'][0] + '.csv')\n",
    "print(path['path_raw'][0] + '/' + path['table_raw'][0] + '.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(path['path_transient']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo os arquivos CSV em .partquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(path['path_transient'])):\n",
    "    df = pd.read_csv(path['path_transient'][i] + '/' + path['table_transient'][i] + '.csv')\n",
    "    df = df.astype('str')\n",
    "    df.to_parquet(str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando os arquivos parquet e eliminando as duplicadas do id da primeira coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(path['path_transient'])):\n",
    "    df_clear_duplicates = pd.read_parquet(str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet')\n",
    "    coluna = df_clear_duplicates.columns[0]\n",
    "    df_clear_duplicates.drop_duplicates(subset=coluna, inplace=True)\n",
    "    df_clear_duplicates.to_parquet(str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando as tabelas no banco de dados Mysql"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instalando mysql connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "senha_db = pd.read_csv('/home/michel/senha.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Conectando ao banco de dados MySQL\n",
    "cnx = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password=senha_db['senha'][0],\n",
    "    database='staging_olist_db'\n",
    ")\n",
    "\n",
    "# Cursor para executar as consultas SQL\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Lista de queries SQL para criação das tabelas\n",
    "create_table_queries = [\n",
    "\"\"\"\n",
    "-- Criação da tabela CUSTOMER\n",
    "CREATE TABLE IF NOT EXISTS stg_customers (\n",
    "  customer_id VARCHAR(200) NOT NULL,\n",
    "  customer_unique_id VARCHAR(255),\n",
    "  customer_zip_code_prefix VARCHAR(200),\n",
    "  customer_city VARCHAR(255),\n",
    "  customer_state VARCHAR(255),\n",
    "  PRIMARY KEY (customer_id)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela GEOLOCATION\n",
    "CREATE TABLE IF NOT EXISTS stg_geolocation (\n",
    "  geolocation_zip_code_prefix VARCHAR(200) NOT NULL,\n",
    "  geolocation_lat VARCHAR(255),\n",
    "  geolocation_lng VARCHAR(255),\n",
    "  geolocation_city VARCHAR(255),\n",
    "  geolocation_state VARCHAR(255),\n",
    "  PRIMARY KEY (geolocation_zip_code_prefix)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela ITEMS\n",
    "CREATE TABLE IF NOT EXISTS stg_items (\n",
    "  order_id VARCHAR(255) NOT NULL,\n",
    "  order_item_id VARCHAR(200) NOT NULL,\n",
    "  product_id VARCHAR(255) NOT NULL,\n",
    "  seller_id VARCHAR(255) NOT NULL,\n",
    "  shipping_limit_date VARCHAR(255),\n",
    "  price VARCHAR(255),\n",
    "  freight_value VARCHAR(255),\n",
    "  PRIMARY KEY (order_id, order_item_id) -- Primary Key Composta\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela PAYMENT\n",
    "CREATE TABLE IF NOT EXISTS stg_payments (\n",
    "  order_id VARCHAR(255) NOT NULL,\n",
    "  payment_sequential VARCHAR(200) NOT NULL,\n",
    "  payment_type VARCHAR(255) NOT NULL,\n",
    "  payment_installments VARCHAR(200),\n",
    "  payment_value VARCHAR(255),\n",
    "  PRIMARY KEY (order_id, payment_sequential) -- Primary Key Composta\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela REVIEWS\n",
    "CREATE TABLE IF NOT EXISTS stg_reviews (\n",
    "  review_id VARCHAR(255) NOT NULL,\n",
    "  order_id VARCHAR(255) NOT NULL,\n",
    "  review_score VARCHAR(200),\n",
    "  review_comment_title VARCHAR(255),\n",
    "  review_comment_message TEXT,\n",
    "  review_creation_date VARCHAR(255),\n",
    "  review_answer_timestamp VARCHAR(255),\n",
    "  PRIMARY KEY (review_id)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela ORDERS\n",
    "CREATE TABLE IF NOT EXISTS stg_orders (\n",
    "  order_id VARCHAR(255) NOT NULL,\n",
    "  customer_id VARCHAR(255),\n",
    "  order_status VARCHAR(255),\n",
    "  order_purchase_timestamp VARCHAR(255),\n",
    "  order_approved_at VARCHAR(255),\n",
    "  order_delivered_carrier_date VARCHAR(255),\n",
    "  order_delivered_customer_date VARCHAR(255),\n",
    "  order_estimated_delivery_date VARCHAR(255),\n",
    "  PRIMARY KEY (order_id)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela PRODUCTS\n",
    "CREATE TABLE IF NOT EXISTS stg_products (\n",
    "  product_id VARCHAR(255) NOT NULL,\n",
    "  product_category_name VARCHAR(255),\n",
    "  product_name_lenght VARCHAR(200),\n",
    "  product_description_lenght VARCHAR(200),\n",
    "  product_photos_qty VARCHAR(200),\n",
    "  product_weight_g VARCHAR(200),\n",
    "  product_length_cm VARCHAR(200),\n",
    "  product_height_cm VARCHAR(200),\n",
    "  product_width_cm VARCHAR(200),\n",
    "  PRIMARY KEY (product_id)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela SELLERS\n",
    "CREATE TABLE IF NOT EXISTS stg_sellers (\n",
    "  seller_id VARCHAR(255) NOT NULL,\n",
    "  seller_zip_code_prefix VARCHAR(255),\n",
    "  seller_city VARCHAR(255),\n",
    "  seller_state VARCHAR(255),\n",
    "  PRIMARY KEY (seller_id)\n",
    ")\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "# Executar as consultas SQL para criar as tabelas\n",
    "for query in create_table_queries:\n",
    "    cursor.execute(query)\n",
    "\n",
    "# Commit das alterações\n",
    "cnx.commit()\n",
    "\n",
    "# Fechando o cursor e a conexão\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import mysql.connector\\n\\n# Conectando ao banco de dados MySQL\\ncnx = mysql.connector.connect(\\n    host=\\'localhost\\',\\n    user=\\'root\\',\\n    password=senha_db[\\'senha\\'][0],\\n    database=\\'staging_olist_db\\'\\n)\\n\\n# Criar um cursor\\ncursor = cnx.cursor()\\n\\nlista = []\\nfor i in range(len(lista_tabelas)):\\n   lista.append(lista_tabelas[i][1])\\nlista\\n\\n# Loop para dropar as tabelas\\nfor tabela in lista:\\n    cursor.execute(f\"DROP TABLE IF EXISTS {tabela}\")\\n\\n# Confirmar as alterações\\ncnx.commit()\\n\\n# Fechar a conexão\\ncursor.close()\\ncnx.close()'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import mysql.connector\n",
    "\n",
    "# Conectando ao banco de dados MySQL\n",
    "cnx = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password=senha_db['senha'][0],\n",
    "    database='staging_olist_db'\n",
    ")\n",
    "\n",
    "# Criar um cursor\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "lista = []\n",
    "for i in range(len(lista_tabelas)):\n",
    "   lista.append(lista_tabelas[i][1])\n",
    "lista\n",
    "\n",
    "# Loop para dropar as tabelas\n",
    "for tabela in lista:\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {tabela}\")\n",
    "\n",
    "# Confirmar as alterações\n",
    "cnx.commit()\n",
    "\n",
    "# Fechar a conexão\n",
    "cursor.close()\n",
    "cnx.close()\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimprimindo path para consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_transient</th>\n",
       "      <th>path_raw</th>\n",
       "      <th>table_transient</th>\n",
       "      <th>table_raw</th>\n",
       "      <th>table_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>stg_customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>stg_geolocation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>stg_items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>stg_payments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>stg_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>stg_orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>stg_products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/michel/opt/keggle/transient</td>\n",
       "      <td>/home/michel/opt/keggle/raw</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>stg_sellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      path_transient                     path_raw   \n",
       "0  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw  \\\n",
       "1  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "2  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "3  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "4  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "5  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "6  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "7  /home/michel/opt/keggle/transient  /home/michel/opt/keggle/raw   \n",
       "\n",
       "                table_transient                     table_raw       table_name  \n",
       "0       olist_customers_dataset       olist_customers_dataset    stg_customers  \n",
       "1     olist_geolocation_dataset     olist_geolocation_dataset  stg_geolocation  \n",
       "2     olist_order_items_dataset     olist_order_items_dataset        stg_items  \n",
       "3  olist_order_payments_dataset  olist_order_payments_dataset     stg_payments  \n",
       "4   olist_order_reviews_dataset   olist_order_reviews_dataset      stg_reviews  \n",
       "5          olist_orders_dataset          olist_orders_dataset       stg_orders  \n",
       "6        olist_products_dataset        olist_products_dataset     stg_products  \n",
       "7         olist_sellers_dataset         olist_sellers_dataset      stg_sellers  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = pd.read_csv('/home/michel/opt/keggle/engineer/controller.csv')\n",
    "path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alimentando o banco de dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para alimentar o banco de dados se faz necessário modificar a variável 'posicao' a qual chama no path a tabela correspondente.\n",
    "* Em certo momento foi automatizada essa alimentação com um lço for, todavia não se mostrou produtivo tal procedimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "posicao = 0\n",
    "numero_tabela = posicao\n",
    "nome_tabela = path['table_name'][posicao]\n",
    "\n",
    "# Conectando ao banco de dados MySQL\n",
    "cnx = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password=senha_db['senha'][0],\n",
    "    database='staging_olist_db',\n",
    "    charset='utf8',\n",
    "    use_unicode=True,\n",
    "    connection_timeout=600  # Aumentar para 600 segundos (10 minutos)\n",
    ")\n",
    "\n",
    "# Ler os dados do arquivo Parquet usando o pandas\n",
    "dados_parquet = pd.read_parquet(path['path_raw'][numero_tabela] + '/' + path['table_raw'][numero_tabela] + '.parquet')\n",
    "\n",
    "# Nome da tabela no banco de dados\n",
    "nome_tabela = nome_tabela\n",
    "\n",
    "# Colunas da tabela no banco de dados (devem corresponder às colunas do Parquet)\n",
    "colunas_tabela = list(pd.read_parquet(path['path_raw'][numero_tabela] + '/' + path['table_raw'][numero_tabela] + '.parquet').columns)\n",
    "\n",
    "# Cursor para executar as consultas SQL\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Inserir os dados na tabela\n",
    "for _, linha in dados_parquet.iterrows():\n",
    "    valores = [linha[coluna] for coluna in colunas_tabela]\n",
    "    query = f\"INSERT INTO {nome_tabela} ({', '.join(colunas_tabela)}) VALUES ({', '.join(['%s']*len(colunas_tabela))})\"\n",
    "    cursor.execute(query, valores)\n",
    "\n",
    "# Confirmar as alterações no banco de dados\n",
    "cnx.commit()\n",
    "\n",
    "# Fechar a conexão com o banco de dados\n",
    "cursor.close()\n",
    "cnx.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2 - Enviar esse BI para a Nuvem (Azure / GCP), pensando em uma cloud e desenvolvendo um pipeline fim a fim, entregando o DW em uma camada final para visualização."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Defina a estratégia de nuvem: Avalie as necessidades do seu projeto e decida qual plataforma em nuvem (Azure ou GCP) atende melhor aos seus requisitos. Considere fatores como escalabilidade, disponibilidade de serviços e integração com suas ferramentas e tecnologias existentes.\n",
    "\n",
    "* Provisione recursos na nuvem: Crie uma conta na plataforma escolhida (Azure ou GCP) e provisione os recursos necessários. Isso pode incluir máquinas virtuais, armazenamento, bancos de dados, serviços de data warehouse, entre outros.\n",
    "\n",
    "* Migre o data warehouse para a nuvem: Realize a migração dos dados do seu data warehouse local para o ambiente em nuvem. Isso pode ser feito através de exportação/importação de dados ou replicação contínua, dependendo da sua estratégia e tamanho do conjunto de dados.\n",
    "\n",
    "* Configure o ambiente de data warehouse na nuvem: Configure o ambiente de data warehouse na plataforma em nuvem, criando esquemas, tabelas e índices conforme necessário. Garanta que a estrutura do data warehouse na nuvem esteja alinhada com o modelo dimensional que você definiu.\n",
    "\n",
    "* Desenvolva o pipeline de dados: Utilize as ferramentas disponíveis na plataforma em nuvem para criar um pipeline de dados automatizado. Isso pode incluir serviços como Azure Data Factory (Azure) ou Cloud Dataflow (GCP). Defina as etapas de extração, transformação e carga (ETL) dos dados do seu data warehouse local para o data warehouse na nuvem.\n",
    "\n",
    "* Agende o pipeline de dados: Configure agendamentos regulares para o pipeline de dados, garantindo que as atualizações do data warehouse local sejam refletidas no ambiente em nuvem. Considere a frequência necessária para manter os dados atualizados e sincronizados.\n",
    "\n",
    "* Implemente segurança e conformidade: Aplique as práticas recomendadas de segurança e conformidade na nuvem, como controle de acesso, criptografia e monitoramento de auditoria. Certifique-se de que os dados e o ambiente de BI estejam protegidos contra ameaças e em conformidade com as regulamentações aplicáveis.\n",
    "\n",
    "* Escolha uma ferramenta de visualização de dados: Selecione uma ferramenta de visualização de dados adequada à plataforma em nuvem escolhida, como Power BI (Azure) ou Data Studio (GCP). Essas ferramentas permitem criar painéis interativos, relatórios e gráficos com base nos dados armazenados no data warehouse na nuvem.\n",
    "\n",
    "* Desenvolva visualizações e relatórios: Utilize a ferramenta de visualização de dados escolhida para criar visualizações e relatórios interativos. Explore recursos como filtros, drill-downs e painéis personalizados para fornecer insights acionáveis aos usuários finais.\n",
    "\n",
    "* Publique o BI na nuvem: Faça o deploy do seu ambiente de BI na nuvem para que os usuários possam acessá-lo. Isso pode ser feito através de compartilhamento de links, integração com portais corporativos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3 - Apontar alternativas viáveis de evolução da solução proposta e apresentar um desenho da arquitetura da proposta final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Avalie os requisitos atuais e futuros: Entenda os requisitos atuais da solução proposta e identifique possíveis necessidades futuras. Considere o crescimento esperado do volume de dados, a demanda por recursos computacionais e quaisquer outros requisitos específicos.\n",
    "\n",
    "* Pesquise as tecnologias disponíveis: Realize uma pesquisa das tecnologias e soluções disponíveis no mercado que possam atender às necessidades identificadas. Verifique as tendências de mercado e as melhores práticas para obter insights sobre as opções mais recentes.\n",
    "\n",
    "* Identifique as alternativas viáveis: Com base na pesquisa realizada, identifique as alternativas viáveis de evolução da solução proposta. Isso pode incluir a adoção de novas ferramentas de ETL, o uso de serviços de data lake, a implementação de data streaming, a integração de ferramentas de inteligência artificial ou aprendizado de máquina, entre outras opções.\n",
    "\n",
    "* Analise os prós e contras de cada alternativa: Avalie os prós e contras de cada alternativa identificada. Considere aspectos como escalabilidade, desempenho, custo, complexidade de implementação, recursos disponíveis na plataforma em nuvem escolhida, compatibilidade com a solução atual e habilidades da equipe.\n",
    "\n",
    "* Escolha a alternativa mais adequada: Com base na análise realizada, escolha a alternativa que seja mais adequada para atender aos requisitos atuais e futuros da solução proposta. Leve em consideração fatores como custo-benefício, viabilidade técnica e alinhamento estratégico com a organização.\n",
    "\n",
    "* Desenhe a arquitetura proposta: Com a alternativa selecionada, elabore um desenho da arquitetura da proposta final. Isso envolve identificar os componentes principais, as integrações entre eles e o fluxo de dados. Considere a separação de camadas (por exemplo, ingestão, processamento, armazenamento, visualização) e a escalabilidade horizontal ou vertical.\n",
    "\n",
    "* Especifique os componentes da arquitetura: Para cada componente da arquitetura proposta, especifique as tecnologias e serviços específicos a serem utilizados. Por exemplo, se você optar por um data lake, especifique a ferramenta de armazenamento, como Azure Data Lake Storage ou Google Cloud Storage. Se escolher um serviço de ETL, identifique a ferramenta específica.\n",
    "\n",
    "* Considere a segurança e a governança: Certifique-se de incluir aspectos de segurança e governança na arquitetura proposta. Isso pode envolver a definição de políticas de acesso aos dados, a implementação de criptografia, a garantia da conformidade com regulamentações de privacidade de dados e a criação de um framework de governança para a solução.\n",
    "\n",
    "* Documente a arquitetura proposta: Documente todos os detalhes da arquitetura proposta, incluindo os componentes, integrações, fluxo de dados, tecnologias e serviços utilizados. Isso ajudará na comunicação com as partes interessadas e no entendimento da"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
