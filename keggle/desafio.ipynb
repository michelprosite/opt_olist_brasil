{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michel Souza Santana\n",
    "## Projeto Desafio Aceleras\n",
    "## Trilha 1\n",
    "> Start: 15/05/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1 - Transformação do ER proposto em um BI, realizando o ETL usando uma ferrmenta local (Talend, Apache Hop, Nifi, Airflow, SSIS, Pentaho,…)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entenda o modelo ER: Familiarize-se com o modelo ER existente, incluindo as tabelas, relacionamentos e atributos. Isso ajudará você a mapear corretamente os dados durante a transformação.\n",
    "\n",
    "* Identifique os requisitos de BI: Compreenda as necessidades e requisitos do seu projeto de BI. Identifique as informações que você precisa extrair e apresentar no ambiente de BI.\n",
    "\n",
    "* Escolha uma ferramenta ETL: Pesquise e selecione uma ferramenta ETL adequada para sua transformação de dados. Existem várias opções disponíveis, como Pentaho Data Integration, Talend, Microsoft SQL Server Integration Services (SSIS), entre outras.\n",
    "\n",
    "* Instale a ferramenta ETL: Faça o download e instale a ferramenta ETL selecionada no seu ambiente local.\n",
    "\n",
    "* Conecte-se ao banco de dados: Configure a conexão da ferramenta ETL com o banco de dados que contém os dados do modelo ER. Forneça as credenciais de acesso necessárias para estabelecer a conexão.\n",
    "\n",
    "* Extração de dados: Utilizando a ferramenta ETL, extraia os dados do banco de dados conforme necessário para o seu projeto de BI. Isso pode envolver a seleção de tabelas específicas, filtragem de dados ou até mesmo a união de várias tabelas para obter as informações desejadas.\n",
    "\n",
    "* Limpeza e transformação de dados: Aplique as transformações necessárias nos dados extraídos para adequá-los às necessidades do ambiente de BI. Isso pode incluir a remoção de dados duplicados, preenchimento de valores ausentes, conversão de formatos de data, entre outros processos de limpeza e transformação.\n",
    "\n",
    "* Mapeamento para o modelo dimensional: Se você estiver construindo um data warehouse ou uma solução de BI baseada em modelo dimensional, mapeie os dados extraídos para as dimensões e fatos do seu modelo dimensional. Isso envolve a definição de hierarquias, chaves e relacionamentos.\n",
    "\n",
    "* Desenvolva fluxos de trabalho ETL: Utilizando a ferramenta ETL, crie fluxos de trabalho que automatizem a transformação de dados. Isso pode envolver a criação de transformações, tarefas agendadas e outras operações para garantir a integridade e atualização dos dados.\n",
    "\n",
    "* Carregamento dos dados: Carregue os dados transformados no ambiente de BI, que pode incluir um data warehouse, um banco de dados ou outra solução de armazenamento de dados.\n",
    "\n",
    "* Desenvolva visualizações e relatórios: Com os dados carregados no ambiente de BI, desenvolva visualizações e relatórios interativos para fornecer insights acionáveis aos usuários finais. Isso pode ser feito usando ferramentas de visualização de dados como Tableau, Power BI, QlikView, entre outras.\n",
    "\n",
    "* Teste e valide: Realize testes para garantir a precisão e a integridade dos dados transformados. Verifique se as visualiza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagrama Entidade Relacionamento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entidades:\n",
    "- Cliente (Customer)\n",
    "- Pedido (Order)\n",
    "- Item do Pedido (Order Item)\n",
    "- Produto (Product)\n",
    "- Vendedor (Seller)\n",
    "- Categoria de Produto (Product Category)\n",
    "- Avaliação (Review)\n",
    "- Localização Geográfica (Geolocation)\n",
    "\n",
    "Relacionamentos:\n",
    "- O Cliente (Customer) pode fazer vários Pedidos (Order)\n",
    "- Um Pedido (Order) pertence a um único Cliente (Customer)\n",
    "- Um Pedido (Order) possui vários Itens do Pedido (Order Item)\n",
    "- Cada Item do Pedido (Order Item) está associado a um único Produto (Product)\n",
    "- Um Produto (Product) pertence a uma Categoria de Produto (Product Category)\n",
    "- Um Produto (Product) é vendido por um Vendedor (Seller)\n",
    "- Um Pedido (Order) pode ter várias Avaliações (Review)\n",
    "- Cada Avaliação (Review) está associada a um único Pedido (Order)\n",
    "- Cada Vendedor (Seller) está associado a uma Localização Geográfica (Geolocation)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PAYMENT:\n",
    "    * pk_payment_order_id (chave primária)\n",
    "    * fk_order_id (chave estrangeira referenciando ORDERS)\n",
    "    * payment_sequential\n",
    "    * payment_type\n",
    "    * payment_installments\n",
    "    * payment_value\n",
    "\n",
    "****\n",
    "\n",
    "* PRODUCT:\n",
    "    * pk_product_id (chave primária)\n",
    "    * product_category\n",
    "    * product_name\n",
    "    \n",
    "****\n",
    "\n",
    "* REVIEW:\n",
    "    * pk_review_id (chave primária)\n",
    "    * fk_order_id (chave estrangeira referenciando ORDERS)\n",
    "    * review_score\n",
    "    * review_comment_title\n",
    "    * review_comment_message\n",
    "    * review_creation_date\n",
    "    * review_answer_timestamp\n",
    "    \n",
    "****\n",
    "\n",
    "* ORDERS:\n",
    "    * pk_order_id (chave primária)\n",
    "    * fk_payment_order_id (chave estrangeira referenciando PAYMENT)\n",
    "    * fk_customer_id (chave estrangeira referenciando CUSTOMER)\n",
    "    * order_status\n",
    "    * order_purchase_timestamp\n",
    "    * order_approved_at\n",
    "    * order_delivered_carrier_date\n",
    "    * order_delivered_customer_date\n",
    "    * order_estimated_delivery_date\n",
    "    \n",
    "****\n",
    "\n",
    "* ORDER_ITEM:\n",
    "    * pk_order_item_id (chave primária)\n",
    "    * fk_order_id (chave estrangeira referenciando ORDERS)\n",
    "    * fk_product_id (chave estrangeira referenciando PRODUCT)\n",
    "    * fk_seller_id (chave estrangeira referenciando SELLERS)\n",
    "    * shipping_limit_date\n",
    "    * price\n",
    "    * freight_value\n",
    "    \n",
    "****\n",
    "\n",
    "* SELLERS:\n",
    "    * pk_seller_id (chave primária)\n",
    "    * fk_geolocation_zip_code_prefix (chave estrangeira referenciando GEOLOCATION)\n",
    "    * seller_city\n",
    "    * seller_state\n",
    "    \n",
    "****\n",
    "\n",
    "* CUSTOMER:\n",
    "    * pk_customer_id (chave primária)\n",
    "    * fk_zip_code_prefix (chave estrangeira referenciando GEOLOCATION)\n",
    "    * customer_unique_id\n",
    "    * customer_city\n",
    "    * customer_state\n",
    "    \n",
    "****\n",
    "\n",
    "* GEOLOCATION:\n",
    "    * pk_geolocation_zip_code_prefix (chave primária)\n",
    "    * geolocation_lat\n",
    "    * geolocation_lng\n",
    "    * geolocation_city\n",
    "    * geolocation_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando o pyrrow para conversão dos arquivos csv em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os diretórios estruturais do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('/home/michel/opt/keggle/folders.csv')\n",
    "lista_folders = f['Folders'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lista_folders:\n",
    "    diretorio = \"/home/michel/opt/keggle\" \n",
    "\n",
    "    if not os.path.exists(diretorio + '/' + i):\n",
    "        os.makedirs(diretorio + '/' + i)\n",
    "        print(f\"Diretório {i} criado com sucesso!\")\n",
    "    else:\n",
    "        print(f\"O diretório {i} já existe.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o arquivo 'controller.csv' na pasta enginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho do arquivo CSV\n",
    "caminho_arquivo = '/home/michel/opt/keggle/engineer/controller.csv'\n",
    "dados = [\n",
    "[\"path_transient\",\"path_raw\",\"table_transient\",\"table_raw\", \"table_name\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_customers_dataset\",\"olist_customers_dataset\",\"stg_customers\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_geolocation_dataset\",\"olist_geolocation_dataset\",\"stg_geolocation\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_order_items_dataset\",\"olist_order_items_dataset\",\"stg_items\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_order_payments_dataset\",\"olist_order_payments_dataset\",\"stg_payments\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_order_reviews_dataset\",\"olist_order_reviews_dataset\",\"stg_reviews\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_orders_dataset\",\"olist_orders_dataset\",\"stg_orders\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_products_dataset\",\"olist_products_dataset\",\"stg_products\"],\n",
    "[\"/home/michel/opt/keggle/transient\",\"/home/michel/opt/keggle/raw\",\"olist_sellers_dataset\",\"olist_sellers_dataset\",\"stg_sellers\"]\n",
    "]\n",
    "# Abrir o arquivo CSV em modo de escrita e cria\n",
    "with open(caminho_arquivo, 'w', newline='') as arquivo_csv:\n",
    "    writer = csv.writer(arquivo_csv)\n",
    "    writer.writerows(dados)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificar as fontes de dados: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "\n",
    "A URL em questão se refere a um conjunto de dados disponibilizado no site Kaggle, que contém informações sobre o comércio eletrônico no Brasil. O conjunto de dados é intitulado \"Brazilian E-Commerce Public Dataset by Olist\" e foi criado pela empresa Olist, que é uma plataforma de vendas on-line que conecta pequenos e médios varejistas a marketplaces.\n",
    "\n",
    "O conjunto de dados contém informações de mais de 100 mil pedidos de clientes, com dados que incluem informações do produto, preços, prazos de entrega, avaliações de clientes e informações sobre o vendedor. Além disso, o conjunto de dados contém informações sobre geolocalização dos clientes, categoria de produtos e informações sobre a própria loja virtual.\n",
    "\n",
    "Este conjunto de dados pode ser extremamente útil para análises sobre comércio eletrônico no Brasil, permitindo a análise de tendências de consumo, comportamento dos clientes, performance de vendas e muito mais. A disponibilização de dados desse tipo é importante para o desenvolvimento de modelos de negócios mais eficientes e para a tomada de decisões mais informadas no setor de e-commerce brasileiro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copia os dados do site do kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando o Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixando os arquivos csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixar & diszipar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('/home/michel/opt/keggle/transient/brazilian-ecommerce.zip'):\n",
    "    !cd /home/michel/opt/keggle/transient/ && kaggle datasets download -d olistbr/brazilian-ecommerce\n",
    "    !cd /home/michel/opt/keggle/transient/ && unzip brazilian-ecommerce.zip\n",
    "    print('Arquivo carregado e descompactado.')\n",
    "else:\n",
    "    print('Arquivo já existe.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando as variáveis necessárias para manipulação dos arquivos csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pd.read_csv('/home/michel/opt/keggle/engineer/controller.csv')\n",
    "path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando a formação das rotas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path['path_transient'][0] + '/' + path['table_transient'][0] + '.csv')\n",
    "print(path['path_raw'][0] + '/' + path['table_raw'][0] + '.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(path['path_transient']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(path['path_transient'])):\n",
    "    df = pd.read_csv(path['path_transient'][i] + '/' + path['table_transient'][i] + '.csv')\n",
    "    df = df.astype('str')\n",
    "    df.to_parquet(str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(path['path_transient'])):\n",
    "    df_clear_duplicates = pd.read_parquet(str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet')\n",
    "    coluna = df_clear_duplicates.columns[0]\n",
    "    df_clear_duplicates.drop_duplicates(subset=coluna, inplace=True)\n",
    "    df_clear_duplicates.to_parquet(str(path['path_raw'][i]) + '/' + str(path['table_raw'][i]) + '.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando as tabelas no banco de dados Mysql"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instalando mysql connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senha_db = pd.read_csv('/home/michel/senha.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Conectando ao banco de dados MySQL\n",
    "cnx = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password=senha_db['senha'][0],\n",
    "    database='staging_olist_db'\n",
    ")\n",
    "\n",
    "# Cursor para executar as consultas SQL\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Lista de queries SQL para criação das tabelas\n",
    "create_table_queries = [\n",
    "\"\"\"\n",
    "-- Criação da tabela CUSTOMER\n",
    "CREATE TABLE IF NOT EXISTS stg_customers (\n",
    "  customer_id VARCHAR(200) NOT NULL,\n",
    "  customer_unique_id VARCHAR(255),\n",
    "  customer_zip_code_prefix VARCHAR(200),\n",
    "  customer_city VARCHAR(255),\n",
    "  customer_state VARCHAR(255),\n",
    "  PRIMARY KEY (customer_id)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela GEOLOCATION\n",
    "CREATE TABLE IF NOT EXISTS stg_geolocation (\n",
    "  geolocation_zip_code_prefix VARCHAR(200) NOT NULL,\n",
    "  geolocation_lat VARCHAR(255),\n",
    "  geolocation_lng VARCHAR(255),\n",
    "  geolocation_city VARCHAR(255),\n",
    "  geolocation_state VARCHAR(255),\n",
    "  PRIMARY KEY (geolocation_zip_code_prefix)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela ITEMS\n",
    "CREATE TABLE IF NOT EXISTS stg_items (\n",
    "  order_id VARCHAR(255) NOT NULL,\n",
    "  order_item_id VARCHAR(200) NOT NULL,\n",
    "  product_id VARCHAR(255) NOT NULL,\n",
    "  seller_id VARCHAR(255) NOT NULL,\n",
    "  shipping_limit_date VARCHAR(255),\n",
    "  price VARCHAR(255),\n",
    "  freight_value VARCHAR(255),\n",
    "  PRIMARY KEY (order_id, order_item_id) -- Primary Key Composta\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela PAYMENT\n",
    "CREATE TABLE IF NOT EXISTS stg_payments (\n",
    "  order_id VARCHAR(255) NOT NULL,\n",
    "  payment_sequential VARCHAR(200) NOT NULL,\n",
    "  payment_type VARCHAR(255) NOT NULL,\n",
    "  payment_installments VARCHAR(200),\n",
    "  payment_value VARCHAR(255),\n",
    "  PRIMARY KEY (order_id, payment_sequential) -- Primary Key Composta\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela REVIEWS\n",
    "CREATE TABLE IF NOT EXISTS stg_reviews (\n",
    "  review_id VARCHAR(255) NOT NULL,\n",
    "  order_id VARCHAR(255) NOT NULL,\n",
    "  review_score VARCHAR(200),\n",
    "  review_comment_title VARCHAR(255),\n",
    "  review_comment_message TEXT,\n",
    "  review_creation_date VARCHAR(255),\n",
    "  review_answer_timestamp VARCHAR(255),\n",
    "  PRIMARY KEY (review_id)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela ORDERS\n",
    "CREATE TABLE IF NOT EXISTS stg_orders (\n",
    "  order_id VARCHAR(255) NOT NULL,\n",
    "  customer_id VARCHAR(255),\n",
    "  order_status VARCHAR(255),\n",
    "  order_purchase_timestamp VARCHAR(255),\n",
    "  order_approved_at VARCHAR(255),\n",
    "  order_delivered_carrier_date VARCHAR(255),\n",
    "  order_delivered_customer_date VARCHAR(255),\n",
    "  order_estimated_delivery_date VARCHAR(255),\n",
    "  PRIMARY KEY (order_id)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela PRODUCTS\n",
    "CREATE TABLE IF NOT EXISTS stg_products (\n",
    "  product_id VARCHAR(255) NOT NULL,\n",
    "  product_category_name VARCHAR(255),\n",
    "  product_name_lenght VARCHAR(200),\n",
    "  product_description_lenght VARCHAR(200),\n",
    "  product_photos_qty VARCHAR(200),\n",
    "  product_weight_g VARCHAR(200),\n",
    "  product_length_cm VARCHAR(200),\n",
    "  product_height_cm VARCHAR(200),\n",
    "  product_width_cm VARCHAR(200),\n",
    "  PRIMARY KEY (product_id)\n",
    ")\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "-- Criação da tabela SELLERS\n",
    "CREATE TABLE IF NOT EXISTS stg_sellers (\n",
    "  seller_id VARCHAR(255) NOT NULL,\n",
    "  seller_zip_code_prefix VARCHAR(255),\n",
    "  seller_city VARCHAR(255),\n",
    "  seller_state VARCHAR(255),\n",
    "  PRIMARY KEY (seller_id)\n",
    ")\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "# Executar as consultas SQL para criar as tabelas\n",
    "for query in create_table_queries:\n",
    "    cursor.execute(query)\n",
    "\n",
    "# Commit das alterações\n",
    "cnx.commit()\n",
    "\n",
    "# Fechando o cursor e a conexão\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import mysql.connector\n",
    "\n",
    "# Conectando ao banco de dados MySQL\n",
    "cnx = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password=senha_db['senha'][0],\n",
    "    database='staging_olist_db'\n",
    ")\n",
    "\n",
    "# Criar um cursor\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "lista = []\n",
    "for i in range(len(lista_tabelas)):\n",
    "   lista.append(lista_tabelas[i][1])\n",
    "lista\n",
    "\n",
    "# Loop para dropar as tabelas\n",
    "for tabela in lista:\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {tabela}\")\n",
    "\n",
    "# Confirmar as alterações\n",
    "cnx.commit()\n",
    "\n",
    "# Fechar a conexão\n",
    "cursor.close()\n",
    "cnx.close()\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimprimindo path para consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pd.read_csv('/home/michel/opt/keggle/engineer/controller.csv')\n",
    "path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alimentando o banco de dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para alimentar o banco de dados se faz necessário modificar a variável 'posicao' a qual chama no path a tabela correspondente.\n",
    "* Em certo momento foi automatizada essa alimentação com um lço for, todavia não se mostrou produtivo tal procedimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "posicao = 0\n",
    "numero_tabela = posicao\n",
    "nome_tabela = path['table_name'][posicao]\n",
    "\n",
    "# Conectando ao banco de dados MySQL\n",
    "cnx = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password=senha_db['senha'][0],\n",
    "    database='staging_olist_db',\n",
    "    charset='utf8',\n",
    "    use_unicode=True,\n",
    "    connection_timeout=600  # Aumentar para 600 segundos (10 minutos)\n",
    ")\n",
    "\n",
    "# Ler os dados do arquivo Parquet usando o pandas\n",
    "dados_parquet = pd.read_parquet(path['path_raw'][numero_tabela] + '/' + path['table_raw'][numero_tabela] + '.parquet')\n",
    "\n",
    "# Nome da tabela no banco de dados\n",
    "nome_tabela = nome_tabela\n",
    "\n",
    "# Colunas da tabela no banco de dados (devem corresponder às colunas do Parquet)\n",
    "colunas_tabela = list(pd.read_parquet(path['path_raw'][numero_tabela] + '/' + path['table_raw'][numero_tabela] + '.parquet').columns)\n",
    "\n",
    "# Cursor para executar as consultas SQL\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Inserir os dados na tabela\n",
    "for _, linha in dados_parquet.iterrows():\n",
    "    valores = [linha[coluna] for coluna in colunas_tabela]\n",
    "    query = f\"INSERT INTO {nome_tabela} ({', '.join(colunas_tabela)}) VALUES ({', '.join(['%s']*len(colunas_tabela))})\"\n",
    "    cursor.execute(query, valores)\n",
    "\n",
    "# Confirmar as alterações no banco de dados\n",
    "cnx.commit()\n",
    "\n",
    "# Fechar a conexão com o banco de dados\n",
    "cursor.close()\n",
    "cnx.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2 - Enviar esse BI para a Nuvem (Azure / GCP), pensando em uma cloud e desenvolvendo um pipeline fim a fim, entregando o DW em uma camada final para visualização."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Defina a estratégia de nuvem: Avalie as necessidades do seu projeto e decida qual plataforma em nuvem (Azure ou GCP) atende melhor aos seus requisitos. Considere fatores como escalabilidade, disponibilidade de serviços e integração com suas ferramentas e tecnologias existentes.\n",
    "\n",
    "* Provisione recursos na nuvem: Crie uma conta na plataforma escolhida (Azure ou GCP) e provisione os recursos necessários. Isso pode incluir máquinas virtuais, armazenamento, bancos de dados, serviços de data warehouse, entre outros.\n",
    "\n",
    "* Migre o data warehouse para a nuvem: Realize a migração dos dados do seu data warehouse local para o ambiente em nuvem. Isso pode ser feito através de exportação/importação de dados ou replicação contínua, dependendo da sua estratégia e tamanho do conjunto de dados.\n",
    "\n",
    "* Configure o ambiente de data warehouse na nuvem: Configure o ambiente de data warehouse na plataforma em nuvem, criando esquemas, tabelas e índices conforme necessário. Garanta que a estrutura do data warehouse na nuvem esteja alinhada com o modelo dimensional que você definiu.\n",
    "\n",
    "* Desenvolva o pipeline de dados: Utilize as ferramentas disponíveis na plataforma em nuvem para criar um pipeline de dados automatizado. Isso pode incluir serviços como Azure Data Factory (Azure) ou Cloud Dataflow (GCP). Defina as etapas de extração, transformação e carga (ETL) dos dados do seu data warehouse local para o data warehouse na nuvem.\n",
    "\n",
    "* Agende o pipeline de dados: Configure agendamentos regulares para o pipeline de dados, garantindo que as atualizações do data warehouse local sejam refletidas no ambiente em nuvem. Considere a frequência necessária para manter os dados atualizados e sincronizados.\n",
    "\n",
    "* Implemente segurança e conformidade: Aplique as práticas recomendadas de segurança e conformidade na nuvem, como controle de acesso, criptografia e monitoramento de auditoria. Certifique-se de que os dados e o ambiente de BI estejam protegidos contra ameaças e em conformidade com as regulamentações aplicáveis.\n",
    "\n",
    "* Escolha uma ferramenta de visualização de dados: Selecione uma ferramenta de visualização de dados adequada à plataforma em nuvem escolhida, como Power BI (Azure) ou Data Studio (GCP). Essas ferramentas permitem criar painéis interativos, relatórios e gráficos com base nos dados armazenados no data warehouse na nuvem.\n",
    "\n",
    "* Desenvolva visualizações e relatórios: Utilize a ferramenta de visualização de dados escolhida para criar visualizações e relatórios interativos. Explore recursos como filtros, drill-downs e painéis personalizados para fornecer insights acionáveis aos usuários finais.\n",
    "\n",
    "* Publique o BI na nuvem: Faça o deploy do seu ambiente de BI na nuvem para que os usuários possam acessá-lo. Isso pode ser feito através de compartilhamento de links, integração com portais corporativos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3 - Apontar alternativas viáveis de evolução da solução proposta e apresentar um desenho da arquitetura da proposta final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Avalie os requisitos atuais e futuros: Entenda os requisitos atuais da solução proposta e identifique possíveis necessidades futuras. Considere o crescimento esperado do volume de dados, a demanda por recursos computacionais e quaisquer outros requisitos específicos.\n",
    "\n",
    "* Pesquise as tecnologias disponíveis: Realize uma pesquisa das tecnologias e soluções disponíveis no mercado que possam atender às necessidades identificadas. Verifique as tendências de mercado e as melhores práticas para obter insights sobre as opções mais recentes.\n",
    "\n",
    "* Identifique as alternativas viáveis: Com base na pesquisa realizada, identifique as alternativas viáveis de evolução da solução proposta. Isso pode incluir a adoção de novas ferramentas de ETL, o uso de serviços de data lake, a implementação de data streaming, a integração de ferramentas de inteligência artificial ou aprendizado de máquina, entre outras opções.\n",
    "\n",
    "* Analise os prós e contras de cada alternativa: Avalie os prós e contras de cada alternativa identificada. Considere aspectos como escalabilidade, desempenho, custo, complexidade de implementação, recursos disponíveis na plataforma em nuvem escolhida, compatibilidade com a solução atual e habilidades da equipe.\n",
    "\n",
    "* Escolha a alternativa mais adequada: Com base na análise realizada, escolha a alternativa que seja mais adequada para atender aos requisitos atuais e futuros da solução proposta. Leve em consideração fatores como custo-benefício, viabilidade técnica e alinhamento estratégico com a organização.\n",
    "\n",
    "* Desenhe a arquitetura proposta: Com a alternativa selecionada, elabore um desenho da arquitetura da proposta final. Isso envolve identificar os componentes principais, as integrações entre eles e o fluxo de dados. Considere a separação de camadas (por exemplo, ingestão, processamento, armazenamento, visualização) e a escalabilidade horizontal ou vertical.\n",
    "\n",
    "* Especifique os componentes da arquitetura: Para cada componente da arquitetura proposta, especifique as tecnologias e serviços específicos a serem utilizados. Por exemplo, se você optar por um data lake, especifique a ferramenta de armazenamento, como Azure Data Lake Storage ou Google Cloud Storage. Se escolher um serviço de ETL, identifique a ferramenta específica.\n",
    "\n",
    "* Considere a segurança e a governança: Certifique-se de incluir aspectos de segurança e governança na arquitetura proposta. Isso pode envolver a definição de políticas de acesso aos dados, a implementação de criptografia, a garantia da conformidade com regulamentações de privacidade de dados e a criação de um framework de governança para a solução.\n",
    "\n",
    "* Documente a arquitetura proposta: Documente todos os detalhes da arquitetura proposta, incluindo os componentes, integrações, fluxo de dados, tecnologias e serviços utilizados. Isso ajudará na comunicação com as partes interessadas e no entendimento da"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
